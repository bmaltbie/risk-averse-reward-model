{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk-Averse Reward Model Training\n",
    "\n",
    "This notebook implements the complete risk aversion experiment for training **Qwen3-8B** to prefer risk-averse choices over risk-neutral ones.\n",
    "\n",
    "## Features\n",
    "- **CSV Data Loading**: Loads scenarios from `11_7_low_stakes_training_set.csv`\n",
    "- **CARA vs LINEAR Utility**: Trains on CARA (risk-averse) vs LINEAR (risk-neutral) best options\n",
    "\n",
    "\n",
    "**Memory Optimized for 8B Model:** Uses gradient checkpointing, smaller batch size, and fp16 for memory efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate torch pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    get_linear_schedule_with_warmup\n)\nfrom sklearn.model_selection import train_test_split\nimport json\nfrom typing import List, Dict, Tuple\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nwarnings.filterwarnings('ignore')\n\n# Set plotting style\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Reproducibility function (will be called after config is loaded)\ndef set_seed(seed):\n    \"\"\"Set all seeds for reproducibility\"\"\"\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nprint(\"Libraries imported successfully!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Configuration\n\nAll configurable parameters for the experiment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CONFIGURATION - All configurable parameters for the experiment\n# =============================================================================\n\n# Model settings\nMODEL_NAME = \"Qwen/Qwen3-8B\"          # Base model to use\nMAX_LENGTH = 256                       # Maximum sequence length for tokenization\n\n# Training hyperparameters\nBATCH_SIZE = 2                         # Batch size per forward pass\nGRADIENT_ACCUMULATION_STEPS = 4        # Effective batch size = BATCH_SIZE * this\nLEARNING_RATE = 5e-4                   # Learning rate for reward head\nWEIGHT_DECAY = 0.01                    # L2 regularization on reward head\nNUM_EPOCHS = 5                         # Maximum number of training epochs\n\n# Stability settings\nREWARD_CLIP_VALUE = 10.0               # Clip r+ - r- to avoid extreme logits\nEARLY_STOPPING_PATIENCE = 2            # Stop if no improvement for N epochs\n\n# Data settings\nDATA_FILE = \"11_7_low_stakes_training_set.csv\"  # Training data file\nTEST_SPLIT = 0.2                       # Fraction of data for validation\nRANDOM_SEED = 42                       # Random seed for reproducibility\n\n# =============================================================================\n# Set seed for reproducibility\nset_seed(RANDOM_SEED)\n\nprint(\"Configuration loaded:\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\nprint(f\"  Random seed: {RANDOM_SEED}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Data Loading Class\n\nLoads risk scenarios from CSV file with proper prompt modification and grouping by situation_id."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class RiskAversionDataLoader:\n    \"\"\"Load and process data from CSV file for risk aversion training\"\"\"\n    \n    def __init__(self, csv_file_path=\"11_7_low_stakes_training_set.csv\"):\n        self.csv_file_path = csv_file_path\n        \n    def load_and_process_data(self) -> pd.DataFrame:\n        \"\"\"Load CSV data and process it for training\n        \n        CSV format (11_7_low_stakes_training_set.csv):\n        - Multiple rows per situation (one per option)\n        - is_best_cara = True marks risk-averse option\n        - is_best_linear = True marks risk-neutral option\n        - correct_label/incorrect_label columns contain the actual labels\n        \"\"\"\n        # Check if CSV file exists\n        if not os.path.exists(self.csv_file_path):\n            raise FileNotFoundError(\n                f\"Required data file '{self.csv_file_path}' not found. \"\n                f\"Please ensure the CSV file is uploaded to Colab.\"\n            )\n        \n        # Load the CSV file\n        df = pd.read_csv(self.csv_file_path)\n        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n        \n        # Check required columns exist\n        required_columns = ['situation_id', 'prompt_text', 'option_index', 'is_best_cara', 'is_best_linear']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(\n                f\"Missing required columns in CSV: {missing_columns}. \"\n                f\"Available columns: {list(df.columns)}\"\n            )\n        \n        # Check if pre-computed labels exist\n        has_precomputed_labels = 'correct_label' in df.columns and 'incorrect_label' in df.columns\n        \n        # Group by situation_id to get unique situations\n        situations = []\n        situations_skipped = 0\n        \n        for situation_id, group in df.groupby('situation_id'):\n            # Find risk-averse option (CARA best)\n            cara_rows = group[group['is_best_cara'] == True]\n            if len(cara_rows) == 0:\n                situations_skipped += 1\n                continue\n            cara_option = cara_rows.iloc[0]\n            \n            # Find risk-neutral option (LINEAR best)\n            linear_rows = group[group['is_best_linear'] == True]\n            if len(linear_rows) == 0:\n                situations_skipped += 1\n                continue\n            linear_option = linear_rows.iloc[0]\n            \n            # Get prompt text from first row (same for all options)\n            prompt_text = group.iloc[0]['prompt_text']\n            \n            # Use pre-computed labels if available, otherwise compute from option_index\n            if has_precomputed_labels and pd.notna(cara_option['correct_label']):\n                correct_label = str(cara_option['correct_label'])\n                incorrect_label = str(cara_option['incorrect_label'])\n            else:\n                # Fallback: convert 0-indexed option_index to 1-indexed option numbers\n                correct_label = str(cara_option['option_index'] + 1)\n                incorrect_label = str(linear_option['option_index'] + 1)\n            \n            situations.append({\n                'situation_id': situation_id,\n                'prompt_text': prompt_text,\n                'correct_label': correct_label,  # Risk-averse option\n                'incorrect_label': incorrect_label,  # Risk-neutral option\n                'num_options': len(group)\n            })\n        \n        if situations_skipped > 0:\n            print(f\"Warning: Skipped {situations_skipped} situations missing CARA or LINEAR best options\")\n        \n        result_df = pd.DataFrame(situations)\n        print(f\"Processed into {len(result_df)} unique situations\")\n        \n        # Display sample data\n        if len(result_df) > 0:\n            sample = result_df.iloc[0]\n            print(f\"\\nSample situation:\")\n            print(f\"Prompt: {sample['prompt_text'][:200]}...\")\n            print(f\"Risk-averse choice (CARA best): Option {sample['correct_label']}\")\n            print(f\"Risk-neutral choice (LINEAR best): Option {sample['incorrect_label']}\")\n            print(f\"Number of options in this situation: {sample['num_options']}\")\n        \n        return result_df\n\nprint(\"RiskAversionDataLoader defined\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Load and Validate Data\n\nLoad the training data and split into training and validation sets.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load data using config\nloader = RiskAversionDataLoader(DATA_FILE)\nfull_dataset = loader.load_and_process_data()\n\n# Split into train and validation sets\ntrain_df, val_df = train_test_split(\n    full_dataset, \n    test_size=TEST_SPLIT, \n    random_state=RANDOM_SEED\n)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Dataset Split:\")\nprint(f\"  Total situations: {len(full_dataset)}\")\nprint(f\"  Training situations: {len(train_df)} ({100*(1-TEST_SPLIT):.0f}%)\")\nprint(f\"  Validation situations: {len(val_df)} ({100*TEST_SPLIT:.0f}%)\")\nprint(f\"{'='*60}\")\n\n# Validate data format\nassert 'prompt_text' in train_df.columns, \"Missing prompt_text column\"\nassert 'correct_label' in train_df.columns, \"Missing correct_label column\"\nassert 'incorrect_label' in train_df.columns, \"Missing incorrect_label column\"\n\nprint(\"\\nData validation passed!\")\nprint(\"Train/validation split complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Pairwise Dataset for Reward Modeling\n\nDataset class that provides pairs of (preferred, rejected) options for Bradley-Terry loss.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PairwiseRewardDataset(Dataset):\n    \"\"\"Dataset for pairwise reward model training with Bradley-Terry loss\"\"\"\n    \n    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length: int = 256):\n        \"\"\"\n        Args:\n            dataframe: DataFrame with columns: prompt_text, correct_label, incorrect_label\n            tokenizer: Tokenizer for encoding text\n            max_length: Maximum sequence length\n        \"\"\"\n        self.data = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # Format: prompt + chosen option\n        preferred_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['correct_label']}\"\n        rejected_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['incorrect_label']}\"\n        \n        # Tokenize both options\n        preferred_encoding = self.tokenizer(\n            preferred_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        rejected_encoding = self.tokenizer(\n            rejected_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'preferred_input_ids': preferred_encoding['input_ids'].squeeze(0),\n            'preferred_attention_mask': preferred_encoding['attention_mask'].squeeze(0),\n            'rejected_input_ids': rejected_encoding['input_ids'].squeeze(0),\n            'rejected_attention_mask': rejected_encoding['attention_mask'].squeeze(0),\n        }\n\nprint(\"✓ PairwiseRewardDataset defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Reward Model Architecture\n\nQwen3-8B base model with frozen weights + trainable scalar reward head.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoModel\n\nclass RewardModel(nn.Module):\n    \"\"\"Reward model with frozen backbone and trainable scalar reward head\"\"\"\n    \n    def __init__(self, model_name: str = \"Qwen/Qwen3-8B\"):\n        super().__init__()\n        \n        # Load base model\n        print(f\"Loading base model: {model_name}\")\n        self.backbone = AutoModel.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        \n        # Freeze all backbone parameters\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n        \n        print(f\"Backbone loaded and frozen ({sum(p.numel() for p in self.backbone.parameters())/1e9:.2f}B params)\")\n        \n        # Trainable scalar reward head - replaces LM head\n        # Takes final hidden state h_T and computes: reward = W^T * h_T + b\n        hidden_size = self.backbone.config.hidden_size\n        self.reward_head = nn.Linear(hidden_size, 1, bias=True)\n        \n        # Xavier initialization for better gradient flow\n        nn.init.xavier_uniform_(self.reward_head.weight)\n        nn.init.zeros_(self.reward_head.bias)\n        \n        print(f\"Reward head initialized ({sum(p.numel() for p in self.reward_head.parameters())} trainable params)\")\n        print(f\"  Architecture: Linear({hidden_size} -> 1) with bias\")\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Forward pass to compute scalar reward from final hidden state\n        \n        Args:\n            input_ids: Input token IDs [batch_size, seq_len]\n            attention_mask: Attention mask [batch_size, seq_len]\n            \n        Returns:\n            rewards: Scalar reward scores [batch_size]\n        \"\"\"\n        # Get hidden states from frozen backbone (in fp16)\n        with torch.no_grad():\n            outputs = self.backbone(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n        \n        # Extract final hidden state h_T (last non-padding token per sequence)\n        # For each sequence, find the position of the last non-padding token\n        sequence_lengths = attention_mask.sum(dim=1) - 1  # 0-indexed position\n        batch_size = input_ids.shape[0]\n        \n        # Gather the hidden state at the last token position for each sequence\n        # Shape: [batch_size, hidden_size]\n        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n        last_hidden_states = hidden_states[\n            torch.arange(batch_size, device=hidden_states.device),\n            sequence_lengths\n        ]\n        \n        # Detach and convert to fp32 for stable training\n        last_hidden_states = last_hidden_states.detach().float()\n        \n        # Compute scalar reward: r = W^T * h_T + b\n        # Shape: [batch_size, 1] -> [batch_size]\n        rewards = self.reward_head(last_hidden_states).squeeze(-1)\n        \n        return rewards\n\nprint(\"RewardModel class defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Training Setup\n\nInitialize model, tokenizer, datasets, loss function, and optimizer.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load tokenizer\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n# Left padding is standard for decoder-only models when batching\ntokenizer.padding_side = 'left'\n\n# Create datasets\nprint(\"Creating datasets...\")\ntrain_dataset = PairwiseRewardDataset(train_df, tokenizer, max_length=MAX_LENGTH)\nval_dataset = PairwiseRewardDataset(val_df, tokenizer, max_length=MAX_LENGTH)\n\nprint(f\"  Training examples: {len(train_dataset)}\")\nprint(f\"  Validation examples: {len(val_dataset)}\")\n\n# Initialize model\nprint(\"\\nInitializing reward model...\")\nmodel = RewardModel(MODEL_NAME)\n\n# Move reward head to correct device (keep in fp32 for stability)\ndevice = next(model.backbone.parameters()).device\nmodel.reward_head = model.reward_head.to(device)\n\nprint(f\"  Device: {device}\")\nprint(f\"  Backbone dtype: {next(model.backbone.parameters()).dtype}\")\nprint(f\"  Reward head dtype: {next(model.reward_head.parameters()).dtype}\")\n\n# Bradley-Terry loss function with clipping for stability\ndef bradley_terry_loss(preferred_rewards, rejected_rewards, clip_value=REWARD_CLIP_VALUE):\n    \"\"\"\n    Bradley-Terry pairwise ranking loss with margin clipping\n    Loss = -log(sigmoid(r_preferred - r_rejected))\n    \n    Clipping prevents extreme logits that can cause numerical instability.\n    Encourages: r_preferred > r_rejected\n    \"\"\"\n    margin = preferred_rewards - rejected_rewards\n    margin = torch.clamp(margin, -clip_value, clip_value)  # Clip for stability\n    return -F.logsigmoid(margin).mean()  # More numerically stable than log(sigmoid())\n\n# Optimizer with L2 weight decay on reward head only\noptimizer = torch.optim.AdamW(\n    model.reward_head.parameters(),\n    lr=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY\n)\n\n# Learning rate scheduler with warmup\ntrain_dataloader_for_scheduler = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntotal_steps = (len(train_dataloader_for_scheduler) // GRADIENT_ACCUMULATION_STEPS) * NUM_EPOCHS\nwarmup_steps = total_steps // 10\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\nprint(f\"\\n{'='*60}\")\nprint(\"Training Configuration:\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\nprint(f\"  Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Weight decay (L2): {WEIGHT_DECAY}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Max sequence length: {MAX_LENGTH}\")\nprint(f\"  Reward margin clipping: +/- {REWARD_CLIP_VALUE}\")\nprint(f\"  LR scheduler: linear warmup ({warmup_steps} steps) + decay\")\nprint(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE} epochs\")\nprint(f\"  Total training steps: {total_steps}\")\nprint(f\"{'='*60}\")\nprint(\"\\nTraining setup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Evaluation Function\n\nCompute pairwise accuracy: percentage of pairs where preferred option scores higher.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def evaluate_model(model, dataset, batch_size=None):\n    \"\"\"\n    Evaluate model on pairwise accuracy\n    \n    Args:\n        model: RewardModel to evaluate\n        dataset: PairwiseRewardDataset\n        batch_size: Batch size for evaluation (defaults to BATCH_SIZE * 2)\n        \n    Returns:\n        accuracy: Float, percentage of pairs where preferred scores higher\n        avg_loss: Float, average Bradley-Terry loss\n        preferred_scores: List of reward scores for preferred options\n        rejected_scores: List of reward scores for rejected options\n    \"\"\"\n    if batch_size is None:\n        batch_size = BATCH_SIZE * 2  # Can use larger batch for eval (no gradients)\n    \n    model.eval()\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    correct = 0\n    total = 0\n    total_loss = 0.0\n    preferred_scores_list = []\n    rejected_scores_list = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            # Get rewards for preferred options\n            preferred_rewards = model(\n                input_ids=batch['preferred_input_ids'].to(device),\n                attention_mask=batch['preferred_attention_mask'].to(device)\n            )\n            \n            # Get rewards for rejected options\n            rejected_rewards = model(\n                input_ids=batch['rejected_input_ids'].to(device),\n                attention_mask=batch['rejected_attention_mask'].to(device)\n            )\n            \n            # Compute loss\n            loss = bradley_terry_loss(preferred_rewards, rejected_rewards)\n            total_loss += loss.item() * len(preferred_rewards)\n            \n            # Compute accuracy: count pairs where preferred > rejected\n            correct += (preferred_rewards > rejected_rewards).sum().item()\n            total += len(preferred_rewards)\n            \n            # Store scores for analysis\n            preferred_scores_list.extend(preferred_rewards.cpu().float().numpy())\n            rejected_scores_list.extend(rejected_rewards.cpu().float().numpy())\n    \n    accuracy = correct / total if total > 0 else 0.0\n    avg_loss = total_loss / total if total > 0 else 0.0\n    \n    return accuracy, avg_loss, preferred_scores_list, rejected_scores_list\n\nprint(\"Evaluation function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Training Loop\n\nTrain the model with logging, validation, and checkpointing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create output directory for checkpoints\nos.makedirs(\"outputs\", exist_ok=True)\n\n# Training history\nhistory = {\n    'train_loss': [],\n    'train_steps': [],\n    'val_accuracy': [],\n    'val_loss': [],\n    'epochs': [],\n    'reward_margins': [],\n    'preferred_rewards': [],\n    'rejected_rewards': [],\n    'learning_rates': [],\n}\n\n# Create dataloader\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True,\n    pin_memory=True if torch.cuda.is_available() else False,\n)\n\nprint(\"Starting training...\")\nprint(f\"Total batches per epoch: {len(train_dataloader)}\")\nprint(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Optimizer steps per epoch: {len(train_dataloader) // GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Total optimizer steps: {total_steps}\\n\")\n\nbest_val_accuracy = 0.0\nglobal_step = 0\noptimizer_step = 0\nepochs_without_improvement = 0\n\n# Store initial weights for comparison\ninitial_weight = model.reward_head.weight.clone().detach()\ninitial_bias = model.reward_head.bias.clone().detach()\n\n# Zero gradients at start\noptimizer.zero_grad()\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"{'='*60}\")\n    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n    print(f\"{'='*60}\")\n    \n    model.train()\n    epoch_loss = 0.0\n    epoch_preferred_rewards = []\n    epoch_rejected_rewards = []\n    epoch_margins = []\n    accumulated_loss = 0.0\n    \n    for step, batch in enumerate(train_dataloader):\n        # Forward pass for preferred options\n        preferred_rewards = model(\n            input_ids=batch['preferred_input_ids'].to(device),\n            attention_mask=batch['preferred_attention_mask'].to(device)\n        )\n        \n        # Forward pass for rejected options\n        rejected_rewards = model(\n            input_ids=batch['rejected_input_ids'].to(device),\n            attention_mask=batch['rejected_attention_mask'].to(device)\n        )\n        \n        # Track reward statistics\n        epoch_preferred_rewards.extend(preferred_rewards.detach().cpu().tolist())\n        epoch_rejected_rewards.extend(rejected_rewards.detach().cpu().tolist())\n        reward_margin = (preferred_rewards - rejected_rewards).detach()\n        epoch_margins.extend(reward_margin.cpu().tolist())\n        \n        # Compute Bradley-Terry loss (scaled for gradient accumulation)\n        loss = bradley_terry_loss(preferred_rewards, rejected_rewards) / GRADIENT_ACCUMULATION_STEPS\n        \n        # Check for NaN loss\n        if torch.isnan(loss):\n            print(f\"  WARNING: NaN loss detected at step {step + 1}\")\n            print(f\"    Preferred rewards: {preferred_rewards}\")\n            print(f\"    Rejected rewards: {rejected_rewards}\")\n            optimizer.zero_grad()\n            continue\n        \n        # Backward pass (accumulate gradients)\n        loss.backward()\n        accumulated_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n        epoch_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n        \n        # Detailed diagnostics on first few optimizer steps\n        if optimizer_step < 3 and (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            print(f\"\\n  === Diagnostics for Optimizer Step {optimizer_step + 1} ===\")\n            \n            # Reward statistics\n            print(f\"  Rewards:\")\n            print(f\"    Preferred: mean={preferred_rewards.mean().item():.4f}, std={preferred_rewards.std().item():.4f}\")\n            print(f\"    Rejected:  mean={rejected_rewards.mean().item():.4f}, std={rejected_rewards.std().item():.4f}\")\n            print(f\"    Margin:    mean={reward_margin.mean().item():.4f}, std={reward_margin.std().item():.4f}\")\n            print(f\"    Loss:      {accumulated_loss:.4f}\")\n            \n            # Gradient statistics\n            print(f\"  Gradients:\")\n            for name, param in model.reward_head.named_parameters():\n                if param.grad is not None:\n                    grad_norm = param.grad.norm().item()\n                    grad_mean = param.grad.mean().item()\n                    grad_max = param.grad.abs().max().item()\n                    print(f\"    {name}: norm={grad_norm:.6f}, mean={grad_mean:.6f}, max={grad_max:.6f}\")\n                else:\n                    print(f\"    {name}: NO GRADIENT!\")\n            \n            # Parameter statistics\n            print(f\"  Parameters:\")\n            weight_change = (model.reward_head.weight - initial_weight).abs().max().item()\n            bias_change = (model.reward_head.bias - initial_bias).abs().max().item()\n            print(f\"    Weight max change: {weight_change:.6f}\")\n            print(f\"    Bias max change: {bias_change:.6f}\")\n            print(f\"    Current LR: {scheduler.get_last_lr()[0]:.6f}\")\n            print()\n        \n        # Optimizer step after accumulation\n        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            # Gradient clipping\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.reward_head.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n            optimizer_step += 1\n            global_step += 1\n            \n            # Log every 50 optimizer steps\n            if optimizer_step % 50 == 0:\n                avg_loss = epoch_loss / (step + 1)\n                weight_change = (model.reward_head.weight - initial_weight).abs().max().item()\n                recent_margin = np.mean(epoch_margins[-100:]) if len(epoch_margins) >= 100 else np.mean(epoch_margins)\n                current_lr = scheduler.get_last_lr()[0]\n                \n                print(f\"  Step {optimizer_step} | \"\n                      f\"Loss: {avg_loss:.4f} | \"\n                      f\"Margin: {recent_margin:+.4f} | \"\n                      f\"LR: {current_lr:.2e} | \"\n                      f\"Weight: {weight_change:.6f}\")\n                \n                history['train_loss'].append(avg_loss)\n                history['train_steps'].append(global_step)\n                history['learning_rates'].append(current_lr)\n            \n            accumulated_loss = 0.0\n    \n    # Handle remaining gradients if dataset doesn't divide evenly\n    remaining_steps = len(train_dataloader) % GRADIENT_ACCUMULATION_STEPS\n    if remaining_steps > 0:\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.reward_head.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        optimizer_step += 1\n    \n    # End of epoch statistics\n    avg_train_loss = epoch_loss / len(train_dataloader)\n    \n    # Weight changes\n    total_weight_change = (model.reward_head.weight - initial_weight).abs().mean().item()\n    total_bias_change = (model.reward_head.bias - initial_bias).abs().mean().item()\n    \n    # Reward statistics\n    mean_preferred = np.mean(epoch_preferred_rewards)\n    mean_rejected = np.mean(epoch_rejected_rewards)\n    mean_margin = np.mean(epoch_margins)\n    std_margin = np.std(epoch_margins)\n    percent_correct = np.mean([m > 0 for m in epoch_margins]) * 100\n    \n    print(f\"\\n  Epoch {epoch + 1} Summary:\")\n    print(f\"    Loss: {avg_train_loss:.4f}\")\n    print(f\"    Rewards: preferred={mean_preferred:+.4f}, rejected={mean_rejected:+.4f}\")\n    print(f\"    Margin: mean={mean_margin:+.4f}, std={std_margin:.4f}\")\n    print(f\"    Correct ranking: {percent_correct:.1f}% (preferred > rejected)\")\n    print(f\"    Weight changes: weight={total_weight_change:.6f}, bias={total_bias_change:.6f}\")\n    print(f\"    Learning rate: {scheduler.get_last_lr()[0]:.2e}\")\n    \n    # Store for history\n    history['reward_margins'].append(mean_margin)\n    history['preferred_rewards'].append(mean_preferred)\n    history['rejected_rewards'].append(mean_rejected)\n    \n    # Validation\n    print(f\"\\n  Running validation...\")\n    val_accuracy, val_loss, val_pref_scores, val_rej_scores = evaluate_model(model, val_dataset)\n    \n    val_margin = np.mean(val_pref_scores) - np.mean(val_rej_scores)\n    \n    print(f\"    Validation accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n    print(f\"    Validation loss: {val_loss:.4f}\")\n    print(f\"    Validation margin: {val_margin:+.4f}\")\n    \n    # Save to history\n    history['val_accuracy'].append(val_accuracy)\n    history['val_loss'].append(val_loss)\n    history['epochs'].append(epoch + 1)\n    \n    # Save checkpoint if best model\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        epochs_without_improvement = 0\n        checkpoint_path = f\"outputs/best_model_epoch{epoch+1}.pt\"\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.reward_head.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'val_accuracy': val_accuracy,\n            'val_loss': val_loss,\n        }, checkpoint_path)\n        print(f\"    New best! Saved checkpoint: {checkpoint_path}\")\n    else:\n        epochs_without_improvement += 1\n        print(f\"    No improvement for {epochs_without_improvement} epoch(s)\")\n        \n        # Early stopping check\n        if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n            print(f\"\\n  Early stopping triggered: no improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n            break\n    \n    # Clear GPU cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    print()\n\nprint(f\"\\n{'='*60}\")\nprint(\"Training Complete!\")\nprint(f\"{'='*60}\")\nprint(f\"Best validation accuracy: {best_val_accuracy:.4f} ({best_val_accuracy*100:.2f}%)\")\nprint(f\"Total optimizer steps: {optimizer_step}\")\nprint(f\"Final model saved to: outputs/best_model_epoch*.pt\")\n\n# Save training history\nhistory_path = \"outputs/training_history.json\"\nwith open(history_path, 'w') as f:\n    json.dump(history, f, indent=2)\nprint(f\"Training history saved to: {history_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Visualization and Results\n\nPlot training curves and analyze model performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Evaluate final model on validation set\nprint(\"Evaluating final model on validation set...\")\nfinal_accuracy, final_loss, preferred_scores, rejected_scores = evaluate_model(\n    model, val_dataset\n)\n\nprint(f\"\\nFinal Validation Results:\")\nprint(f\"  Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\nprint(f\"  Loss: {final_loss:.4f}\")\nprint(f\"  Mean preferred score: {np.mean(preferred_scores):.4f}\")\nprint(f\"  Mean rejected score: {np.mean(rejected_scores):.4f}\")\nprint(f\"  Score difference: {np.mean(preferred_scores) - np.mean(rejected_scores):.4f}\")\n\n# Calculate reward margins\nreward_margins = np.array(preferred_scores) - np.array(rejected_scores)\n\n# Create comprehensive visualizations\nfig = plt.figure(figsize=(18, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\nfig.suptitle('Reward Model Training Results - Risk Aversion Analysis', \n             fontsize=16, fontweight='bold', y=0.995)\n\n# Plot 1: Training Loss Over Time\nax1 = fig.add_subplot(gs[0, 0])\nif len(history['train_steps']) > 0:\n    ax1.plot(history['train_steps'], history['train_loss'], 'b-', linewidth=2, label='Training Loss')\n    ax1.set_xlabel('Training Steps')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training Loss Over Time')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n# Plot 2: Validation Accuracy Over Epochs\nax2 = fig.add_subplot(gs[0, 1])\nif len(history['epochs']) > 0:\n    ax2.plot(history['epochs'], history['val_accuracy'], 'g-o', linewidth=2, markersize=8)\n    ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Validation Pairwise Accuracy')\n    ax2.set_ylim([0, 1])\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n# Plot 3: Reward Margin Progression\nax3 = fig.add_subplot(gs[0, 2])\nif len(history['epochs']) > 0 and len(history['reward_margins']) > 0:\n    ax3.plot(history['epochs'], history['reward_margins'], 'purple', linewidth=2, marker='s', markersize=8)\n    ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='No Preference')\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('Mean Reward Margin')\n    ax3.set_title('Risk-Averse Preference Strength\\n(Preferred - Rejected)')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n\n# Plot 4: Score Distribution Comparison (Histogram)\nax4 = fig.add_subplot(gs[1, 0])\nbins = np.linspace(min(min(preferred_scores), min(rejected_scores)),\n                  max(max(preferred_scores), max(rejected_scores)), 30)\nax4.hist(preferred_scores, bins=bins, alpha=0.6, label='Preferred (Risk-Averse)', \n         color='green', density=True, edgecolor='black', linewidth=0.5)\nax4.hist(rejected_scores, bins=bins, alpha=0.6, label='Rejected (Risk-Neutral)', \n         color='red', density=True, edgecolor='black', linewidth=0.5)\nax4.set_xlabel('Reward Score')\nax4.set_ylabel('Density')\nax4.set_title('Score Distribution Comparison')\nax4.legend()\nax4.grid(True, alpha=0.3, axis='y')\n\n# Plot 5: Scatter Plot - Preferred vs Rejected Scores\nax5 = fig.add_subplot(gs[1, 1])\nax5.scatter(rejected_scores, preferred_scores, alpha=0.5, s=30, c=reward_margins, \n            cmap='RdYlGn', edgecolors='black', linewidth=0.5)\nmin_val = min(min(rejected_scores), min(preferred_scores))\nmax_val = max(max(rejected_scores), max(preferred_scores))\nax5.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, linewidth=2, label='Equal Scores')\nax5.fill_between([min_val, max_val], [min_val, max_val], [max_val, max_val],\n                alpha=0.15, color='green', label='Risk-Averse Preferred')\nax5.fill_between([min_val, max_val], [min_val, min_val], [min_val, max_val],\n                alpha=0.15, color='red', label='Risk-Neutral Preferred')\nax5.set_xlabel('Risk-Neutral Score')\nax5.set_ylabel('Risk-Averse Score')\nax5.set_title('Pairwise Score Comparison')\nax5.legend(fontsize=8)\nax5.grid(True, alpha=0.3)\nax5.axis('equal')\n\n# Plot 6: Reward Margin Distribution\nax6 = fig.add_subplot(gs[1, 2])\nax6.hist(reward_margins, bins=40, alpha=0.7, color='purple', edgecolor='black', linewidth=0.5)\nax6.axvline(x=0, color='r', linestyle='--', linewidth=2, label='No Preference')\nax6.axvline(x=np.mean(reward_margins), color='g', linestyle='-', linewidth=2, \n            label=f'Mean: {np.mean(reward_margins):.3f}')\nax6.set_xlabel('Reward Margin (Preferred - Rejected)')\nax6.set_ylabel('Count')\nax6.set_title('Distribution of Reward Margins')\nax6.legend()\nax6.grid(True, alpha=0.3, axis='y')\n\n# Plot 7: Ranking Performance Breakdown\nax7 = fig.add_subplot(gs[2, 0])\ncorrect_rankings = np.sum(reward_margins > 0)\nincorrect_rankings = np.sum(reward_margins < 0)\ntied_rankings = np.sum(reward_margins == 0)\ncategories = ['Correct\\n(Pref > Rej)', 'Incorrect\\n(Pref < Rej)', 'Tied\\n(Pref = Rej)']\ncounts = [correct_rankings, incorrect_rankings, tied_rankings]\ncolors = ['green', 'red', 'gray']\nbars = ax7.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\nfor bar, count in zip(bars, counts):\n    height = bar.get_height()\n    pct = 100 * count / len(reward_margins)\n    ax7.text(bar.get_x() + bar.get_width()/2., height + 1,\n             f'{count}\\n({pct:.1f}%)', ha='center', va='bottom', fontweight='bold')\nax7.set_ylabel('Number of Pairs')\nax7.set_title('Ranking Performance Breakdown')\nax7.grid(True, alpha=0.3, axis='y')\n\n# Plot 8: Score Evolution Over Training\nax8 = fig.add_subplot(gs[2, 1])\nif len(history['epochs']) > 0 and len(history['preferred_rewards']) > 0:\n    ax8.plot(history['epochs'], history['preferred_rewards'], 'g-o', \n             linewidth=2, markersize=8, label='Preferred (Risk-Averse)')\n    ax8.plot(history['epochs'], history['rejected_rewards'], 'r-s', \n             linewidth=2, markersize=8, label='Rejected (Risk-Neutral)')\n    ax8.set_xlabel('Epoch')\n    ax8.set_ylabel('Mean Reward Score')\n    ax8.set_title('Score Evolution During Training')\n    ax8.legend()\n    ax8.grid(True, alpha=0.3)\n\n# Plot 9: Cumulative Distribution Functions\nax9 = fig.add_subplot(gs[2, 2])\nsorted_pref = np.sort(preferred_scores)\nsorted_rej = np.sort(rejected_scores)\ncdf_pref = np.arange(1, len(sorted_pref) + 1) / len(sorted_pref)\ncdf_rej = np.arange(1, len(sorted_rej) + 1) / len(sorted_rej)\nax9.plot(sorted_pref, cdf_pref, 'g-', linewidth=2, label='Preferred (Risk-Averse)')\nax9.plot(sorted_rej, cdf_rej, 'r-', linewidth=2, label='Rejected (Risk-Neutral)')\nax9.set_xlabel('Reward Score')\nax9.set_ylabel('Cumulative Probability')\nax9.set_title('Cumulative Distribution Comparison')\nax9.legend()\nax9.grid(True, alpha=0.3)\n\nplt.tight_layout()\n\n# Save plot\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nplot_path = f\"outputs/training_results_{timestamp}.png\"\nplt.savefig(plot_path, dpi=300, bbox_inches='tight')\nprint(f\"\\nComprehensive plots saved to: {plot_path}\")\n\nplt.show()\n\n# Save results to JSON\nresults = {\n    'model_name': MODEL_NAME,\n    'final_validation_accuracy': float(final_accuracy),\n    'final_validation_loss': float(final_loss),\n    'best_validation_accuracy': float(best_val_accuracy),\n    'mean_preferred_score': float(np.mean(preferred_scores)),\n    'mean_rejected_score': float(np.mean(rejected_scores)),\n    'score_difference': float(np.mean(preferred_scores) - np.mean(rejected_scores)),\n    'margin_mean': float(np.mean(reward_margins)),\n    'margin_std': float(np.std(reward_margins)),\n    'correct_rankings': int(np.sum(reward_margins > 0)),\n    'incorrect_rankings': int(np.sum(reward_margins < 0)),\n    'tied_rankings': int(np.sum(reward_margins == 0)),\n    'num_epochs': NUM_EPOCHS,\n    'epochs_trained': len(history['epochs']),\n    'batch_size': BATCH_SIZE,\n    'effective_batch_size': BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS,\n    'learning_rate': LEARNING_RATE,\n    'weight_decay': WEIGHT_DECAY,\n    'reward_clip_value': REWARD_CLIP_VALUE,\n    'training_samples': len(train_dataset),\n    'validation_samples': len(val_dataset),\n    'timestamp': timestamp\n}\n\nresults_path = f\"outputs/results_{timestamp}.json\"\nwith open(results_path, 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"Results saved to: {results_path}\")\n\n# Save training history with timestamp\nhistory_path = f\"outputs/training_history_{timestamp}.json\"\nwith open(history_path, 'w') as f:\n    json.dump(history, f, indent=2)\nprint(f\"Training history saved to: {history_path}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Experiment Complete!\")\nprint(\"=\"*60)\n\n# Find checkpoints before try block to avoid undefined variable\nimport glob\ncheckpoints = glob.glob(\"outputs/best_model_epoch*.pt\")\n\n# Automatic downloads (for Google Colab)\nprint(\"\\n\" + \"=\"*60)\nprint(\"Downloading Results...\")\nprint(\"=\"*60)\n\ntry:\n    from google.colab import files\n    \n    # Download plots\n    print(f\"Downloading: {plot_path}\")\n    files.download(plot_path)\n    \n    # Download results JSON\n    print(f\"Downloading: {results_path}\")\n    files.download(results_path)\n    \n    # Download training history\n    print(f\"Downloading: {history_path}\")\n    files.download(history_path)\n    \n    # Download best model checkpoint (if exists)\n    if checkpoints:\n        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n        print(f\"Downloading: {latest_checkpoint}\")\n        files.download(latest_checkpoint)\n    \n    print(\"\\nAll files downloaded successfully!\")\n    \nexcept ImportError:\n    print(\"\\nNOTE: Not running in Google Colab - files saved to outputs/ directory\")\n    print(\"Files available:\")\n    print(f\"  - {plot_path}\")\n    print(f\"  - {results_path}\")\n    print(f\"  - {history_path}\")\n    if checkpoints:\n        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n        print(f\"  - {latest_checkpoint}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}