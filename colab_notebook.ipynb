{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Risk-Averse Reward Model Training\n\nThis notebook implements the complete risk aversion experiment for training **Qwen3-8B** to prefer risk-averse choices over risk-neutral ones.\n\n## Features\n- **Separate Train/Val Data**: Uses dedicated training and validation CSV files with different label types\n- **CARA-based Training**: Trains on CARA (risk-averse) labels with smart incorrect label selection\n- **Cooperation-based Validation**: Validates on cooperate labels to test generalization\n- **Per-Epoch Re-randomization**: Training data with \"both\" bucket_label gets re-randomized each epoch\n\n## Data Files\n- **Training**: `data/2025_12_5_training_set_low_stakes_balanced.csv` (CARA labels)\n- **Validation**: `data/2025_12_5_val_set_medium_stakes_balanced.csv` (cooperate labels)\n\n**Memory Optimized for 8B Model:** Uses gradient checkpointing, smaller batch size, and fp16 for memory efficiency"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q transformers datasets accelerate torch pandas numpy scikit-learn matplotlib seaborn hf_transfer peft\n\nprint(\"âœ“ Dependencies installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom sklearn.model_selection import train_test_split\nimport json\nfrom typing import List, Dict, Tuple\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nwarnings.filterwarnings('ignore')\n\n# Set plotting style\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Reproducibility function (will be called after config is loaded)\ndef set_seed(seed):\n    \"\"\"Set all seeds for reproducibility\"\"\"\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nprint(\"Libraries imported successfully!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Configuration\n\nAll configurable parameters for the experiment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CONFIGURATION - All configurable parameters for the experiment\n# =============================================================================\n\n# Model settings\nMODEL_NAME = \"Qwen/Qwen3-8B\"          # Base model to use\nMAX_LENGTH = 256                       # Maximum sequence length for tokenization\n\n# Training hyperparameters\nBATCH_SIZE = 2                         # Batch size per forward pass\nLEARNING_RATE = 2e-4                   # Learning rate for LoRA layers\nWEIGHT_DECAY = 0.01                    # L2 regularization\nNUM_EPOCHS = 10                        # Number of training epochs\n\n# LoRA configuration\nLORA_R = 8                             # LoRA rank (low for small dataset)\nLORA_ALPHA = 16                        # LoRA alpha (scaling = alpha/r = 2.0)\nLORA_DROPOUT = 0.05                    # LoRA dropout for regularization\nLORA_TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # Query and Value attention projections\n\n# Data settings - separate training and validation files\nTRAIN_DATA_FILE = \"data/2025_12_5_training_set_low_stakes_balanced.csv\"\nVAL_DATA_FILE = \"data/2025_12_5_val_set_medium_stakes_balanced.csv\"\nRANDOM_SEED = 42                       # Random seed for reproducibility\n\n# In-distribution validation split\nIN_DIST_VAL_SPLIT = 0.10               # Hold out 10% of training data for in-distribution validation\n\n# =============================================================================\n# Set seed for reproducibility\nset_seed(RANDOM_SEED)\n\nprint(\"Configuration loaded:\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Random seed: {RANDOM_SEED}\")\nprint(f\"  In-dist val split: {IN_DIST_VAL_SPLIT*100:.0f}%\")\nprint(f\"\\nLoRA Configuration:\")\nprint(f\"  Rank (r): {LORA_R}\")\nprint(f\"  Alpha: {LORA_ALPHA}\")\nprint(f\"  Dropout: {LORA_DROPOUT}\")\nprint(f\"  Target modules: {LORA_TARGET_MODULES}\")\nprint(f\"\\nData Files:\")\nprint(f\"  Training: {TRAIN_DATA_FILE}\")\nprint(f\"  Validation (out-of-dist): {VAL_DATA_FILE}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Data Loading Classes\n\nTwo specialized loaders for training and validation data:\n- **TrainingDataLoader**: Loads CARA-based labels with `low_bucket_label` logic for incorrect label selection\n- **ValidationDataLoader**: Loads cooperate-based labels for generalization testing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_train_val_situation_split(csv_file_path: str, val_fraction: float = 0.10, random_seed: int = 42):\n    \"\"\"Split situation IDs into train and validation sets, stratified by low_bucket_label.\n    \n    Args:\n        csv_file_path: Path to training CSV file\n        val_fraction: Fraction of situations to hold out for validation (default 0.10)\n        random_seed: Random seed for reproducibility\n        \n    Returns:\n        Tuple of (train_situation_ids, val_situation_ids) as lists\n    \"\"\"\n    df = pd.read_csv(csv_file_path)\n    \n    # Get unique situations with their low_bucket_label for stratification\n    situations = df.groupby('situation_id').first().reset_index()\n    situation_ids = situations['situation_id'].tolist()\n    \n    # Get stratification labels\n    if 'low_bucket_label' in situations.columns:\n        strat_labels = situations['low_bucket_label'].apply(lambda x: x.strip('\"') if isinstance(x, str) else x).tolist()\n        try:\n            train_ids, val_ids = train_test_split(\n                situation_ids,\n                test_size=val_fraction,\n                random_state=random_seed,\n                stratify=strat_labels\n            )\n            print(f\"Stratified split by low_bucket_label: {len(train_ids)} train, {len(val_ids)} val\")\n        except ValueError:\n            # Fall back to non-stratified if stratification fails\n            train_ids, val_ids = train_test_split(\n                situation_ids,\n                test_size=val_fraction,\n                random_state=random_seed\n            )\n            print(f\"Non-stratified split (stratification failed): {len(train_ids)} train, {len(val_ids)} val\")\n    else:\n        train_ids, val_ids = train_test_split(\n            situation_ids,\n            test_size=val_fraction,\n            random_state=random_seed\n        )\n        print(f\"Non-stratified split: {len(train_ids)} train, {len(val_ids)} val\")\n    \n    return train_ids, val_ids\n\n\nclass TrainingDataLoader:\n    \"\"\"Load and process training data with CARA-based labels and low_bucket_label logic.\n    \n    This loader handles the special logic for selecting incorrect labels based on\n    the low_bucket_label field:\n    - \"010_only\": use CARA_alpha_0_10_best_labels as incorrect (avoid over-risk-aversion)\n    - \"lin_only\": use linear_best_labels as incorrect (avoid being linear/risk-neutral)\n    - \"both\": randomly choose between the two (re-randomizes each epoch)\n    \"\"\"\n    \n    def __init__(self, csv_file_path: str, epoch: int = 0, random_seed: int = 42, \n                 situation_ids: List = None):\n        \"\"\"\n        Args:\n            csv_file_path: Path to training CSV file\n            epoch: Current epoch number (used for reproducible per-epoch randomization)\n            random_seed: Base random seed for reproducibility\n            situation_ids: Optional list of situation IDs to include (for train/val split)\n        \"\"\"\n        self.csv_file_path = csv_file_path\n        self.epoch = epoch\n        self.rng = np.random.default_rng(random_seed + epoch)  # Per-epoch randomization\n        self.situation_ids = situation_ids\n        \n    def load_and_process_data(self) -> pd.DataFrame:\n        \"\"\"Load CSV data and process it for training.\n        \n        Returns:\n            DataFrame with columns: situation_id, prompt_text, correct_label, incorrect_label, low_bucket_label\n        \"\"\"\n        # Check if CSV file exists\n        if not os.path.exists(self.csv_file_path):\n            raise FileNotFoundError(\n                f\"Required training data file '{self.csv_file_path}' not found. \"\n                f\"Please ensure the CSV file exists.\"\n            )\n        \n        # Load the CSV file\n        df = pd.read_csv(self.csv_file_path)\n        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n        \n        # Check required columns exist\n        required_columns = ['situation_id', 'prompt_text', 'CARA_correct_labels', 'low_bucket_label']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(\n                f\"Missing required columns in training CSV: {missing_columns}. \"\n                f\"Available columns: {list(df.columns)}\"\n            )\n        \n        # Group by situation_id, take first row of each group (all rows have same labels)\n        situations = df.groupby('situation_id').first().reset_index()\n        \n        # Filter to specified situation IDs if provided\n        if self.situation_ids is not None:\n            situations = situations[situations['situation_id'].isin(self.situation_ids)]\n            print(f\"Filtered to {len(situations)} situations (from {len(self.situation_ids)} specified IDs)\")\n        else:\n            print(f\"Found {len(situations)} unique situations\")\n        \n        processed = []\n        skipped = 0\n        \n        for _, row in situations.iterrows():\n            try:\n                prompt_text = row['prompt_text']\n                \n                # Parse JSON array for correct labels\n                correct_labels = json.loads(row['CARA_correct_labels'])\n                if not correct_labels:\n                    skipped += 1\n                    continue\n                \n                # Get low_bucket_label and determine incorrect labels\n                low_bucket = row['low_bucket_label'].strip('\"')  # Remove surrounding quotes\n                \n                if low_bucket == '010_only':\n                    # Use CARA_alpha_0_10_best_labels as incorrect\n                    incorrect_labels = json.loads(row['CARA_alpha_0_10_best_labels'])\n                elif low_bucket == 'lin_only':\n                    # Use linear_best_labels as incorrect\n                    incorrect_labels = json.loads(row['linear_best_labels'])\n                elif low_bucket == 'both':\n                    # Randomly choose between linear and alpha_0_10 (re-randomizes each epoch)\n                    if self.rng.random() < 0.5:\n                        incorrect_labels = json.loads(row['linear_best_labels'])\n                    else:\n                        incorrect_labels = json.loads(row['CARA_alpha_0_10_best_labels'])\n                else:\n                    # Fallback: use CARA_incorrect_labels if available\n                    incorrect_labels = json.loads(row.get('CARA_incorrect_labels', '[]'))\n                \n                if not incorrect_labels:\n                    skipped += 1\n                    continue\n                \n                # Randomly select one label from each array\n                correct_label = str(self.rng.choice(correct_labels))\n                incorrect_label = str(self.rng.choice(incorrect_labels))\n                \n                processed.append({\n                    'situation_id': row['situation_id'],\n                    'prompt_text': prompt_text,\n                    'correct_label': correct_label,\n                    'incorrect_label': incorrect_label,\n                    'low_bucket_label': low_bucket,\n                })\n                \n            except (json.JSONDecodeError, KeyError) as e:\n                print(f\"Warning: Error processing situation {row['situation_id']}: {e}\")\n                skipped += 1\n                continue\n        \n        if skipped > 0:\n            print(f\"Warning: Skipped {skipped} situations due to missing/empty labels\")\n        \n        result_df = pd.DataFrame(processed)\n        print(f\"Processed into {len(result_df)} training examples (epoch {self.epoch})\")\n        \n        # Display low_bucket_label distribution\n        if 'low_bucket_label' in result_df.columns and len(result_df) > 0:\n            print(f\"\\nlow_bucket_label distribution:\")\n            for label, count in result_df['low_bucket_label'].value_counts().items():\n                print(f\"  {label}: {count} ({100*count/len(result_df):.1f}%)\")\n        \n        return result_df\n\n\ndef _is_true(value) -> bool:\n    \"\"\"Check if a value is TRUE (handles NaN as FALSE).\n    \n    Boolean columns in the CSV only contain TRUE or NaN, where NaN means FALSE.\n    \"\"\"\n    if pd.isna(value):\n        return False\n    return str(value).upper() == 'TRUE'\n\n\nclass InDistributionValidationDataLoader:\n    \"\"\"Load and process in-distribution validation data with CARA-based labels.\n    \n    Uses the same label logic as training but with fixed randomization (no per-epoch changes).\n    Includes error_type categorization for breakdown analysis.\n    \"\"\"\n    \n    def __init__(self, csv_file_path: str, random_seed: int = 42, situation_ids: List = None):\n        \"\"\"\n        Args:\n            csv_file_path: Path to training CSV file\n            random_seed: Random seed for reproducibility (fixed, no per-epoch changes)\n            situation_ids: List of situation IDs to include (typically the held-out 10%)\n        \"\"\"\n        self.csv_file_path = csv_file_path\n        self.rng = np.random.default_rng(random_seed)\n        self.situation_ids = situation_ids\n    \n    def load_and_process_data(self) -> pd.DataFrame:\n        \"\"\"Load CSV data and process it for in-distribution validation.\n        \n        Returns:\n            DataFrame with columns: situation_id, prompt_text, correct_label, incorrect_label, \n                                   error_type, low_bucket_label\n        \"\"\"\n        if not os.path.exists(self.csv_file_path):\n            raise FileNotFoundError(\n                f\"Required data file '{self.csv_file_path}' not found.\"\n            )\n        \n        df = pd.read_csv(self.csv_file_path)\n        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n        \n        # Group by situation_id, take first row of each group\n        situations = df.groupby('situation_id').first().reset_index()\n        \n        # Filter to specified situation IDs\n        if self.situation_ids is not None:\n            situations = situations[situations['situation_id'].isin(self.situation_ids)]\n            print(f\"Filtered to {len(situations)} in-distribution validation situations\")\n        else:\n            print(f\"Found {len(situations)} unique situations\")\n        \n        processed = []\n        skipped = 0\n        error_type_counts = {'too_risky': 0, 'too_risk_averse': 0, 'other': 0}\n        \n        for _, row in situations.iterrows():\n            try:\n                prompt_text = row['prompt_text']\n                \n                # Parse JSON array for correct labels (CARA labels)\n                correct_labels = json.loads(row['CARA_correct_labels'])\n                if not correct_labels:\n                    skipped += 1\n                    continue\n                \n                # Get low_bucket_label and determine incorrect labels + error type\n                low_bucket = row['low_bucket_label'].strip('\"')\n                \n                # For validation: use fixed selection based on low_bucket_label\n                # This determines both the incorrect label AND the error type\n                if low_bucket == '010_only':\n                    incorrect_labels = json.loads(row['CARA_alpha_0_10_best_labels'])\n                    error_type = 'too_risk_averse'\n                elif low_bucket == 'lin_only':\n                    incorrect_labels = json.loads(row['linear_best_labels'])\n                    error_type = 'too_risky'\n                elif low_bucket == 'both':\n                    # For 'both', randomly choose one but stay consistent (fixed seed)\n                    if self.rng.random() < 0.5:\n                        incorrect_labels = json.loads(row['linear_best_labels'])\n                        error_type = 'too_risky'\n                    else:\n                        incorrect_labels = json.loads(row['CARA_alpha_0_10_best_labels'])\n                        error_type = 'too_risk_averse'\n                else:\n                    incorrect_labels = json.loads(row.get('CARA_incorrect_labels', '[]'))\n                    error_type = 'other'\n                \n                if not incorrect_labels:\n                    skipped += 1\n                    continue\n                \n                # Randomly select one label from each array\n                correct_label = str(self.rng.choice(correct_labels))\n                incorrect_label = str(self.rng.choice(incorrect_labels))\n                \n                error_type_counts[error_type] += 1\n                \n                processed.append({\n                    'situation_id': row['situation_id'],\n                    'prompt_text': prompt_text,\n                    'correct_label': correct_label,\n                    'incorrect_label': incorrect_label,\n                    'error_type': error_type,\n                    'low_bucket_label': low_bucket,\n                })\n                \n            except (json.JSONDecodeError, KeyError) as e:\n                print(f\"Warning: Error processing situation {row['situation_id']}: {e}\")\n                skipped += 1\n                continue\n        \n        if skipped > 0:\n            print(f\"Warning: Skipped {skipped} situations due to missing/empty labels\")\n        \n        result_df = pd.DataFrame(processed)\n        print(f\"Processed into {len(result_df)} in-distribution validation examples\")\n        \n        # Display error type distribution\n        print(f\"\\nError type distribution (if model incorrectly prefers non-CARA option):\")\n        for error_type, count in error_type_counts.items():\n            pct = 100 * count / len(result_df) if len(result_df) > 0 else 0\n            print(f\"  {error_type}: {count} ({pct:.1f}%)\")\n        \n        return result_df\n\n\nclass ValidationDataLoader:\n    \"\"\"Load and process validation data with cooperate-based labels and error categorization.\n    \n    For validation, we use cooperate_correct_labels and cooperate_incorrect_labels.\n    Each pair is categorized by error type:\n    - \"too_risky\": incorrect option is linear_best (risk-seeking error)\n    - \"too_risk_averse\": incorrect option is in CARA_alpha_0_10_best_labels (overly cautious)\n    - \"other\": neither of the above\n    \"\"\"\n    \n    def __init__(self, csv_file_path: str, random_seed: int = 42):\n        \"\"\"\n        Args:\n            csv_file_path: Path to validation CSV file\n            random_seed: Random seed for reproducibility\n        \"\"\"\n        self.csv_file_path = csv_file_path\n        self.rng = np.random.default_rng(random_seed)\n    \n    def load_and_process_data(self) -> pd.DataFrame:\n        \"\"\"Load CSV data and process it for validation.\n        \n        Returns:\n            DataFrame with columns: situation_id, prompt_text, correct_label, incorrect_label, error_type\n        \"\"\"\n        # Check if CSV file exists\n        if not os.path.exists(self.csv_file_path):\n            raise FileNotFoundError(\n                f\"Required validation data file '{self.csv_file_path}' not found. \"\n                f\"Please ensure the CSV file exists.\"\n            )\n        \n        # Load the CSV file\n        df = pd.read_csv(self.csv_file_path)\n        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n        \n        # Drop completely empty rows (CSV artifact - all columns are NaN)\n        original_len = len(df)\n        df = df.dropna(how='all')\n        if len(df) < original_len:\n            print(f\"Dropped {original_len - len(df)} empty rows (CSV artifact)\")\n        \n        # Check required columns exist\n        required_columns = ['situation_id', 'prompt_text', 'cooperate_correct_labels', 'cooperate_incorrect_labels']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(\n                f\"Missing required columns in validation CSV: {missing_columns}. \"\n                f\"Available columns: {list(df.columns)}\"\n            )\n        \n        # Build a lookup for each option's properties (linear_best, option_type, etc.)\n        # Key: (situation_id, label) -> properties dict\n        option_properties = {}\n        for _, row in df.iterrows():\n            sit_id = row['situation_id']\n            opt_idx = int(row['option_index'])\n            \n            # Determine the label letter (a, b, c, ... or 1, 2, 3, ... based on data)\n            # First check if cooperate_correct_labels uses letters or numbers\n            correct_labels_str = row['cooperate_correct_labels']\n            if correct_labels_str and pd.notna(correct_labels_str):\n                try:\n                    sample_labels = json.loads(correct_labels_str)\n                    if sample_labels and str(sample_labels[0]).isdigit():\n                        # Uses numeric labels (1, 2, 3, ...)\n                        label = str(opt_idx + 1)\n                    else:\n                        # Uses letter labels (a, b, c, ...)\n                        label = chr(ord('a') + opt_idx)\n                except json.JSONDecodeError:\n                    label = chr(ord('a') + opt_idx)\n            else:\n                label = chr(ord('a') + opt_idx)\n            \n            # Boolean columns: TRUE or NaN (NaN means FALSE)\n            is_linear_best = _is_true(row.get('is_best_linear_display'))\n            is_cara_best = _is_true(row.get('is_best_cara_display'))\n            is_rebel_fosd_all_coops = _is_true(row.get('option_is_rebel_fosd_all_coops'))\n            is_coop_fosd_all_rebels = _is_true(row.get('option_is_coop_fosd_all_rebels'))\n            is_rebel_best_cara = _is_true(row.get('option_is_rebel_best_cara'))\n            is_coop_best_linear = _is_true(row.get('option_is_coop_best_linear'))\n            \n            option_type = row.get('option_type', '')\n            \n            # Get CARA_alpha_0_10_best_labels for this situation\n            alpha_010_str = row.get('CARA_alpha_0_10_best_labels', '')\n            alpha_010_labels = []\n            if alpha_010_str and pd.notna(alpha_010_str) and str(alpha_010_str).strip():\n                try:\n                    alpha_010_labels = json.loads(alpha_010_str)\n                except json.JSONDecodeError:\n                    alpha_010_labels = []\n            \n            option_properties[(sit_id, label)] = {\n                'is_linear_best': is_linear_best,\n                'is_cara_best': is_cara_best,\n                'is_rebel_fosd_all_coops': is_rebel_fosd_all_coops,\n                'is_coop_fosd_all_rebels': is_coop_fosd_all_rebels,\n                'is_rebel_best_cara': is_rebel_best_cara,\n                'is_coop_best_linear': is_coop_best_linear,\n                'option_type': option_type,\n                'alpha_010_labels': alpha_010_labels,\n            }\n        \n        # Group by situation_id, take first row of each group\n        situations = df.groupby('situation_id').first().reset_index()\n        print(f\"Found {len(situations)} unique situations\")\n        \n        processed = []\n        skipped = 0\n        error_type_counts = {'too_risky': 0, 'too_risk_averse': 0, 'other': 0}\n        \n        for _, row in situations.iterrows():\n            try:\n                sit_id = row['situation_id']\n                \n                # Skip if cooperate labels are missing\n                if pd.isna(row['cooperate_correct_labels']) or pd.isna(row['cooperate_incorrect_labels']):\n                    skipped += 1\n                    continue\n                \n                # Parse JSON arrays\n                correct_labels = json.loads(row['cooperate_correct_labels'])\n                incorrect_labels = json.loads(row['cooperate_incorrect_labels'])\n                \n                # Skip situations with empty labels\n                if not correct_labels or not incorrect_labels:\n                    skipped += 1\n                    continue\n                \n                # Randomly select one label from each array\n                correct_label = str(self.rng.choice(correct_labels))\n                incorrect_label = str(self.rng.choice(incorrect_labels))\n                \n                # Determine error type based on the selected incorrect label's properties\n                props = option_properties.get((sit_id, incorrect_label), {})\n                is_linear_best = props.get('is_linear_best', False)\n                alpha_010_labels = props.get('alpha_010_labels', [])\n                option_type = props.get('option_type', '')\n                \n                # Categorize the error type\n                if is_linear_best:\n                    error_type = 'too_risky'\n                elif incorrect_label in alpha_010_labels:\n                    error_type = 'too_risk_averse'\n                else:\n                    error_type = 'other'\n                \n                error_type_counts[error_type] += 1\n                \n                processed.append({\n                    'situation_id': sit_id,\n                    'prompt_text': row['prompt_text'],\n                    'correct_label': correct_label,\n                    'incorrect_label': incorrect_label,\n                    'error_type': error_type,\n                    'incorrect_option_type': option_type,\n                })\n                \n            except (json.JSONDecodeError, KeyError) as e:\n                print(f\"Warning: Error processing situation {row['situation_id']}: {e}\")\n                skipped += 1\n                continue\n        \n        if skipped > 0:\n            print(f\"Warning: Skipped {skipped} situations due to missing/empty labels\")\n        \n        result_df = pd.DataFrame(processed)\n        print(f\"Processed into {len(result_df)} validation examples\")\n        \n        # Display error type distribution\n        print(f\"\\nError type distribution (if model incorrectly prefers non-cooperate):\")\n        for error_type, count in error_type_counts.items():\n            pct = 100 * count / len(result_df) if len(result_df) > 0 else 0\n            print(f\"  {error_type}: {count} ({pct:.1f}%)\")\n        \n        # Display option type distribution for incorrect options\n        if 'incorrect_option_type' in result_df.columns and len(result_df) > 0:\n            print(f\"\\nIncorrect option types:\")\n            for opt_type, count in result_df['incorrect_option_type'].value_counts().items():\n                print(f\"  {opt_type}: {count} ({100*count/len(result_df):.1f}%)\")\n        \n        return result_df\n\n\nprint(\"Data loaders defined: TrainingDataLoader, InDistributionValidationDataLoader, ValidationDataLoader\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Load and Validate Data\n\nLoad the separate training and validation data files.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# SPLIT TRAINING DATA INTO TRAIN AND IN-DISTRIBUTION VALIDATION\n# =============================================================================\nprint(\"Splitting training data into train and in-distribution validation sets...\")\ntrain_situation_ids, in_dist_val_situation_ids = get_train_val_situation_split(\n    TRAIN_DATA_FILE,\n    val_fraction=IN_DIST_VAL_SPLIT,\n    random_seed=RANDOM_SEED\n)\n\n# =============================================================================\n# LOAD TRAINING DATA (90% of low stakes)\n# =============================================================================\nprint(f\"\\n{'='*60}\")\nprint(\"Loading Training Data (90% of low stakes)\")\nprint(f\"{'='*60}\")\ntrain_loader = TrainingDataLoader(\n    TRAIN_DATA_FILE, \n    epoch=0, \n    random_seed=RANDOM_SEED,\n    situation_ids=train_situation_ids\n)\ntrain_df = train_loader.load_and_process_data()\n\n# =============================================================================\n# LOAD IN-DISTRIBUTION VALIDATION DATA (10% of low stakes)\n# =============================================================================\nprint(f\"\\n{'='*60}\")\nprint(\"Loading In-Distribution Validation Data (10% of low stakes)\")\nprint(f\"{'='*60}\")\nin_dist_val_loader = InDistributionValidationDataLoader(\n    TRAIN_DATA_FILE,\n    random_seed=RANDOM_SEED,\n    situation_ids=in_dist_val_situation_ids\n)\nin_dist_val_df = in_dist_val_loader.load_and_process_data()\n\n# =============================================================================\n# LOAD OUT-OF-DISTRIBUTION VALIDATION DATA (medium stakes, cooperate labels)\n# =============================================================================\nprint(f\"\\n{'='*60}\")\nprint(\"Loading Out-of-Distribution Validation Data (medium stakes)\")\nprint(f\"{'='*60}\")\nout_dist_val_loader = ValidationDataLoader(VAL_DATA_FILE, random_seed=RANDOM_SEED)\nout_dist_val_df = out_dist_val_loader.load_and_process_data()\n\n# =============================================================================\n# DATASET SUMMARY\n# =============================================================================\nprint(f\"\\n{'='*60}\")\nprint(\"Dataset Summary\")\nprint(f\"{'='*60}\")\nprint(f\"  Training file: {TRAIN_DATA_FILE}\")\nprint(f\"    Training situations: {len(train_df)} (90%)\")\nprint(f\"    In-dist validation situations: {len(in_dist_val_df)} (10%)\")\nprint(f\"  Out-of-dist validation file: {VAL_DATA_FILE}\")\nprint(f\"    Out-of-dist validation situations: {len(out_dist_val_df)}\")\nprint(f\"\\n  Label types:\")\nprint(f\"    Training: CARA (risk-aversion)\")\nprint(f\"    In-dist validation: CARA (risk-aversion)\")\nprint(f\"    Out-of-dist validation: Cooperate (generalization test)\")\nprint(f\"{'='*60}\")\n\n# Validate data format\nfor df_name, df in [('train_df', train_df), ('in_dist_val_df', in_dist_val_df), ('out_dist_val_df', out_dist_val_df)]:\n    assert 'prompt_text' in df.columns, f\"Missing prompt_text column in {df_name}\"\n    assert 'correct_label' in df.columns, f\"Missing correct_label column in {df_name}\"\n    assert 'incorrect_label' in df.columns, f\"Missing incorrect_label column in {df_name}\"\n\nprint(\"\\nData validation passed!\")\nprint(\"Data loading complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Pairwise Dataset for Reward Modeling\n\nDataset class that provides pairs of (preferred, rejected) options for Bradley-Terry loss.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PairwiseRewardDataset(Dataset):\n    \"\"\"Dataset for pairwise reward model training with Bradley-Terry loss.\n    \n    Supports optional error_type metadata for validation analysis.\n    \"\"\"\n    \n    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length: int = 256):\n        \"\"\"\n        Args:\n            dataframe: DataFrame with columns: prompt_text, correct_label, incorrect_label\n                       Optional: error_type, incorrect_option_type (for validation breakdown)\n            tokenizer: Tokenizer for encoding text\n            max_length: Maximum sequence length\n        \"\"\"\n        self.data = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        # Check if error_type column exists (for validation data)\n        self.has_error_types = 'error_type' in self.data.columns\n        if self.has_error_types:\n            self.error_types = self.data['error_type'].tolist()\n        else:\n            self.error_types = None\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # Format: prompt + chosen option\n        preferred_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['correct_label']}\"\n        rejected_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['incorrect_label']}\"\n        \n        # Tokenize both options\n        preferred_encoding = self.tokenizer(\n            preferred_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        rejected_encoding = self.tokenizer(\n            rejected_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        result = {\n            'preferred_input_ids': preferred_encoding['input_ids'].squeeze(0),\n            'preferred_attention_mask': preferred_encoding['attention_mask'].squeeze(0),\n            'rejected_input_ids': rejected_encoding['input_ids'].squeeze(0),\n            'rejected_attention_mask': rejected_encoding['attention_mask'].squeeze(0),\n        }\n        \n        # Include error_type if available (for validation analysis)\n        if self.has_error_types:\n            result['error_type'] = self.error_types[idx]\n        \n        return result\n    \n    def get_error_type_indices(self) -> Dict[str, List[int]]:\n        \"\"\"Get indices grouped by error type for validation analysis.\n        \n        Returns:\n            Dictionary mapping error_type to list of example indices\n        \"\"\"\n        if not self.has_error_types:\n            return {}\n        \n        indices = {'too_risky': [], 'too_risk_averse': [], 'other': []}\n        for idx, error_type in enumerate(self.error_types):\n            if error_type in indices:\n                indices[error_type].append(idx)\n        return indices\n\nprint(\"PairwiseRewardDataset defined (with error_type support)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Reward Model Architecture\n\nQwen3-8B base model with LoRA adapters (q_proj, v_proj) + trainable scalar reward head.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class RewardModel(nn.Module):\n    \"\"\"Reward model with LoRA-adapted backbone and trainable scalar reward head\"\"\"\n    \n    def __init__(\n        self, \n        model_name: str = \"Qwen/Qwen3-8B\",\n        lora_r: int = 8,\n        lora_alpha: int = 16,\n        lora_dropout: float = 0.05,\n        lora_target_modules: list = None,\n    ):\n        super().__init__()\n        \n        # Load base model in fp16\n        print(f\"Loading base model: {model_name}\")\n        self.backbone = AutoModel.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        \n        hidden_size = self.backbone.config.hidden_size\n        \n        # Add reward head BEFORE applying LoRA\n        self.reward_head = nn.Linear(hidden_size, 1, bias=True)\n        \n        # Initialize reward head with small weights\n        nn.init.normal_(self.reward_head.weight, mean=0.0, std=0.01)\n        nn.init.zeros_(self.reward_head.bias)\n        \n        # Configure and apply LoRA\n        if lora_target_modules is None:\n            lora_target_modules = [\"q_proj\", \"v_proj\"]\n        \n        lora_config = LoraConfig(\n            r=lora_r,\n            lora_alpha=lora_alpha,\n            lora_dropout=lora_dropout,\n            target_modules=lora_target_modules,\n            bias=\"none\",\n            task_type=TaskType.FEATURE_EXTRACTION,\n        )\n        \n        self.backbone = get_peft_model(self.backbone, lora_config)\n        \n        # Print trainable parameter info\n        print(f\"\\nLoRA Configuration:\")\n        print(f\"  Rank: {lora_r}\")\n        print(f\"  Alpha: {lora_alpha}\")\n        print(f\"  Dropout: {lora_dropout}\")\n        print(f\"  Target modules: {lora_target_modules}\")\n        \n        self.backbone.print_trainable_parameters()\n        \n        print(f\"\\nReward head: Linear({hidden_size} -> 1) with bias\")\n        \n        # Count total trainable parameters\n        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        total_params = sum(p.numel() for p in self.parameters())\n        print(f\"Total trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.4f}%)\")\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Forward pass to compute scalar reward from final hidden state\n        \n        Args:\n            input_ids: Input token IDs [batch_size, seq_len]\n            attention_mask: Attention mask [batch_size, seq_len]\n            \n        Returns:\n            rewards: Scalar reward scores [batch_size]\n        \"\"\"\n        # Get hidden states - gradients flow through LoRA layers\n        outputs = self.backbone(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n        )\n        \n        # Extract final hidden state h_T (last non-padding token per sequence)\n        sequence_lengths = attention_mask.sum(dim=1) - 1  # 0-indexed position\n        batch_size = input_ids.shape[0]\n        \n        # Gather the hidden state at the last token position for each sequence\n        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n        last_hidden_states = hidden_states[\n            torch.arange(batch_size, device=hidden_states.device),\n            sequence_lengths\n        ]\n        \n        # Convert to fp32 for numerical stability in reward head\n        last_hidden_states = last_hidden_states.float()\n        \n        # Compute scalar reward: r = W^T * h_T + b\n        rewards = self.reward_head(last_hidden_states).squeeze(-1)\n        \n        return rewards\n\nprint(\"RewardModel class defined (with LoRA support)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Training Setup\n\nInitialize model, tokenizer, datasets, loss function, and optimizer.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load tokenizer\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Create datasets\nprint(\"Creating datasets...\")\ntrain_dataset = PairwiseRewardDataset(train_df, tokenizer, max_length=MAX_LENGTH)\nin_dist_val_dataset = PairwiseRewardDataset(in_dist_val_df, tokenizer, max_length=MAX_LENGTH)\nout_dist_val_dataset = PairwiseRewardDataset(out_dist_val_df, tokenizer, max_length=MAX_LENGTH)\n\nprint(f\"  Training examples: {len(train_dataset)}\")\nprint(f\"  In-distribution validation examples: {len(in_dist_val_dataset)}\")\nprint(f\"  Out-of-distribution validation examples: {len(out_dist_val_dataset)}\")\n\n\n# Helper function for per-epoch re-randomization of training data\ndef recreate_training_dataset(epoch: int):\n    \"\"\"Recreate training dataset with new randomization for 'both' bucket cases.\n    \n    This function creates a fresh training dataset where situations with\n    low_bucket_label='both' get a new random choice between linear_best_labels\n    and CARA_alpha_0_10_best_labels.\n    \n    Args:\n        epoch: Current epoch number (used for reproducible randomization)\n        \n    Returns:\n        PairwiseRewardDataset with re-randomized label selections\n    \"\"\"\n    loader = TrainingDataLoader(\n        TRAIN_DATA_FILE, \n        epoch=epoch, \n        random_seed=RANDOM_SEED,\n        situation_ids=train_situation_ids  # Only use training situations\n    )\n    new_train_df = loader.load_and_process_data()\n    return PairwiseRewardDataset(new_train_df, tokenizer, max_length=MAX_LENGTH)\n\n\n# Initialize model with LoRA\nprint(\"\\nInitializing reward model with LoRA...\")\nmodel = RewardModel(\n    model_name=MODEL_NAME,\n    lora_r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    lora_target_modules=LORA_TARGET_MODULES,\n)\n\n# Get device from backbone\ndevice = next(model.backbone.parameters()).device\n\n# Move reward head to same device and ensure fp32\nmodel.reward_head = model.reward_head.to(device).float()\n\nprint(f\"\\n  Device: {device}\")\nprint(f\"  Backbone dtype: {next(model.backbone.parameters()).dtype}\")\nprint(f\"  Reward head dtype: {next(model.reward_head.parameters()).dtype}\")\n\n# Bradley-Terry loss function\ndef bradley_terry_loss(preferred_rewards, rejected_rewards):\n    \"\"\"\n    Bradley-Terry pairwise ranking loss\n    Loss = -log(sigmoid(r_preferred - r_rejected))\n\n    Encourages: r_preferred > r_rejected\n    \"\"\"\n    return -torch.log(torch.sigmoid(preferred_rewards - rejected_rewards)).mean()\n\n# Optimizer - train LoRA parameters AND reward head with different learning rates\noptimizer = torch.optim.AdamW([\n    {'params': model.backbone.parameters(), 'lr': LEARNING_RATE},\n    {'params': model.reward_head.parameters(), 'lr': LEARNING_RATE * 2.5},  # Higher LR for reward head\n], weight_decay=WEIGHT_DECAY)\n\nprint(f\"\\n{'='*60}\")\nprint(\"Training Configuration:\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  LoRA parameters LR: {LEARNING_RATE}\")\nprint(f\"  Reward head LR: {LEARNING_RATE * 2.5}\")\nprint(f\"  Weight decay (L2): {WEIGHT_DECAY}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Max sequence length: {MAX_LENGTH}\")\nprint(f\"  Per-epoch re-randomization: Enabled for 'both' bucket cases\")\nprint(f\"\\nDataset sizes:\")\nprint(f\"  Training: {len(train_dataset)}\")\nprint(f\"  In-dist validation: {len(in_dist_val_dataset)}\")\nprint(f\"  Out-of-dist validation: {len(out_dist_val_dataset)}\")\nprint(f\"{'='*60}\")\nprint(\"\\nTraining setup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Evaluation Function\n\nCompute pairwise accuracy: percentage of pairs where preferred option scores higher.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def evaluate_model(model, dataset, batch_size=None):\n    \"\"\"\n    Evaluate model on pairwise accuracy with optional error type breakdown.\n    \n    Args:\n        model: RewardModel to evaluate\n        dataset: PairwiseRewardDataset (may include error_type metadata)\n        batch_size: Batch size for evaluation (defaults to BATCH_SIZE * 2)\n        \n    Returns:\n        dict with keys:\n            - accuracy: Float, percentage of pairs where preferred scores higher\n            - avg_loss: Float, average Bradley-Terry loss\n            - preferred_scores: List of reward scores for preferred options\n            - rejected_scores: List of reward scores for rejected options\n            - error_type_breakdown: Dict with accuracy by error category (if available)\n    \"\"\"\n    if batch_size is None:\n        batch_size = BATCH_SIZE * 2  # Can use larger batch for eval (no gradients)\n    \n    model.eval()\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    correct = 0\n    total = 0\n    total_loss = 0.0\n    preferred_scores_list = []\n    rejected_scores_list = []\n    error_types_list = []\n    correct_by_type = {'too_risky': 0, 'too_risk_averse': 0, 'other': 0}\n    total_by_type = {'too_risky': 0, 'too_risk_averse': 0, 'other': 0}\n    \n    # Track per-example results for breakdown\n    per_example_results = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            # Get rewards for preferred options\n            preferred_rewards = model(\n                input_ids=batch['preferred_input_ids'].to(device),\n                attention_mask=batch['preferred_attention_mask'].to(device)\n            )\n            \n            # Get rewards for rejected options\n            rejected_rewards = model(\n                input_ids=batch['rejected_input_ids'].to(device),\n                attention_mask=batch['rejected_attention_mask'].to(device)\n            )\n            \n            # Compute loss\n            loss = bradley_terry_loss(preferred_rewards, rejected_rewards)\n            total_loss += loss.item() * len(preferred_rewards)\n            \n            # Compute accuracy: count pairs where preferred > rejected\n            is_correct = (preferred_rewards > rejected_rewards)\n            correct += is_correct.sum().item()\n            total += len(preferred_rewards)\n            \n            # Store scores for analysis\n            preferred_scores_list.extend(preferred_rewards.cpu().float().numpy())\n            rejected_scores_list.extend(rejected_rewards.cpu().float().numpy())\n            \n            # Track by error type if available\n            if 'error_type' in batch:\n                batch_error_types = batch['error_type']\n                error_types_list.extend(batch_error_types)\n                \n                for i, error_type in enumerate(batch_error_types):\n                    if error_type in total_by_type:\n                        total_by_type[error_type] += 1\n                        if is_correct[i].item():\n                            correct_by_type[error_type] += 1\n                \n                # Store per-example results\n                for i in range(len(preferred_rewards)):\n                    per_example_results.append({\n                        'preferred_score': preferred_rewards[i].item(),\n                        'rejected_score': rejected_rewards[i].item(),\n                        'margin': (preferred_rewards[i] - rejected_rewards[i]).item(),\n                        'is_correct': is_correct[i].item(),\n                        'error_type': batch_error_types[i],\n                    })\n    \n    accuracy = correct / total if total > 0 else 0.0\n    avg_loss = total_loss / total if total > 0 else 0.0\n    \n    # Compute accuracy by error type\n    error_type_breakdown = {}\n    has_error_types = len(error_types_list) > 0\n    \n    if has_error_types:\n        for error_type in ['too_risky', 'too_risk_averse', 'other']:\n            if total_by_type[error_type] > 0:\n                error_type_breakdown[error_type] = {\n                    'accuracy': correct_by_type[error_type] / total_by_type[error_type],\n                    'correct': correct_by_type[error_type],\n                    'total': total_by_type[error_type],\n                }\n            else:\n                error_type_breakdown[error_type] = {\n                    'accuracy': 0.0,\n                    'correct': 0,\n                    'total': 0,\n                }\n    \n    return {\n        'accuracy': accuracy,\n        'avg_loss': avg_loss,\n        'preferred_scores': preferred_scores_list,\n        'rejected_scores': rejected_scores_list,\n        'error_type_breakdown': error_type_breakdown,\n        'per_example_results': per_example_results if has_error_types else [],\n        'error_types': error_types_list,\n    }\n\n\ndef print_error_type_breakdown(breakdown: Dict, title: str = \"Error Type Breakdown\"):\n    \"\"\"Pretty print the error type breakdown.\"\"\"\n    if not breakdown:\n        return\n    \n    print(f\"\\n  {title}:\")\n    print(f\"  {'Category':<20} {'Accuracy':>10} {'Correct':>10} {'Total':>10}\")\n    print(f\"  {'-'*52}\")\n    \n    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n        if error_type in breakdown:\n            stats = breakdown[error_type]\n            acc_pct = stats['accuracy'] * 100\n            print(f\"  {error_type:<20} {acc_pct:>9.1f}% {stats['correct']:>10} {stats['total']:>10}\")\n\n\nprint(\"Evaluation function defined (with error type breakdown)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Baseline Evaluation (Before Training)\n\nEvaluate the randomly initialized reward model to establish a baseline for comparison.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# BASELINE EVALUATION - Before any training\n# =============================================================================\n# This establishes how well the randomly initialized reward head performs\n# Expected: ~50% accuracy (random guessing)\n\nprint(\"=\"*60)\nprint(\"BASELINE EVALUATION (Before Training)\")\nprint(\"=\"*60)\n\n# =============================================================================\n# IN-DISTRIBUTION BASELINE (CARA labels, same as training)\n# =============================================================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"In-Distribution Validation (CARA labels)\")\nprint(\"-\"*60)\n\nbaseline_in_dist_eval = evaluate_model(model, in_dist_val_dataset)\n\nbaseline_in_dist_accuracy = baseline_in_dist_eval['accuracy']\nbaseline_in_dist_loss = baseline_in_dist_eval['avg_loss']\nbaseline_in_dist_pref_scores = baseline_in_dist_eval['preferred_scores']\nbaseline_in_dist_rej_scores = baseline_in_dist_eval['rejected_scores']\nbaseline_in_dist_error_breakdown = baseline_in_dist_eval['error_type_breakdown']\n\nbaseline_in_dist_margins = np.array(baseline_in_dist_pref_scores) - np.array(baseline_in_dist_rej_scores)\n\nprint(f\"\\nIn-Dist Baseline Results (Untrained Model):\")\nprint(f\"  Accuracy: {baseline_in_dist_accuracy:.4f} ({baseline_in_dist_accuracy*100:.2f}%)\")\nprint(f\"  Loss: {baseline_in_dist_loss:.4f}\")\nprint(f\"  Mean margin: {np.mean(baseline_in_dist_margins):.4f}\")\n\nif baseline_in_dist_error_breakdown:\n    print_error_type_breakdown(baseline_in_dist_error_breakdown, \"In-Dist Baseline Error Type Breakdown\")\n\n# Store in-dist baseline for later comparison\nbaseline_in_dist_results = {\n    'accuracy': baseline_in_dist_accuracy,\n    'loss': baseline_in_dist_loss,\n    'preferred_scores': baseline_in_dist_pref_scores,\n    'rejected_scores': baseline_in_dist_rej_scores,\n    'margins': baseline_in_dist_margins.tolist(),\n    'mean_margin': float(np.mean(baseline_in_dist_margins)),\n    'std_margin': float(np.std(baseline_in_dist_margins)),\n    'error_type_breakdown': baseline_in_dist_error_breakdown,\n}\n\n# =============================================================================\n# OUT-OF-DISTRIBUTION BASELINE (Cooperate labels, generalization test)\n# =============================================================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"Out-of-Distribution Validation (Cooperate labels)\")\nprint(\"-\"*60)\n\nbaseline_out_dist_eval = evaluate_model(model, out_dist_val_dataset)\n\nbaseline_out_dist_accuracy = baseline_out_dist_eval['accuracy']\nbaseline_out_dist_loss = baseline_out_dist_eval['avg_loss']\nbaseline_out_dist_pref_scores = baseline_out_dist_eval['preferred_scores']\nbaseline_out_dist_rej_scores = baseline_out_dist_eval['rejected_scores']\nbaseline_out_dist_error_breakdown = baseline_out_dist_eval['error_type_breakdown']\n\nbaseline_out_dist_margins = np.array(baseline_out_dist_pref_scores) - np.array(baseline_out_dist_rej_scores)\n\nprint(f\"\\nOut-Dist Baseline Results (Untrained Model):\")\nprint(f\"  Accuracy: {baseline_out_dist_accuracy:.4f} ({baseline_out_dist_accuracy*100:.2f}%)\")\nprint(f\"  Loss: {baseline_out_dist_loss:.4f}\")\nprint(f\"  Mean margin: {np.mean(baseline_out_dist_margins):.4f}\")\n\nif baseline_out_dist_error_breakdown:\n    print_error_type_breakdown(baseline_out_dist_error_breakdown, \"Out-Dist Baseline Error Type Breakdown\")\n\n# Store out-dist baseline for later comparison\nbaseline_out_dist_results = {\n    'accuracy': baseline_out_dist_accuracy,\n    'loss': baseline_out_dist_loss,\n    'preferred_scores': baseline_out_dist_pref_scores,\n    'rejected_scores': baseline_out_dist_rej_scores,\n    'margins': baseline_out_dist_margins.tolist(),\n    'mean_margin': float(np.mean(baseline_out_dist_margins)),\n    'std_margin': float(np.std(baseline_out_dist_margins)),\n    'error_type_breakdown': baseline_out_dist_error_breakdown,\n}\n\n# Legacy alias for backward compatibility\nbaseline_results = baseline_out_dist_results\n\n# =============================================================================\n# BASELINE SUMMARY\n# =============================================================================\nprint(f\"\\n{'='*60}\")\nprint(\"BASELINE SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"  In-Distribution (CARA):        {baseline_in_dist_accuracy*100:.1f}%\")\nprint(f\"  Out-of-Distribution (Cooperate): {baseline_out_dist_accuracy*100:.1f}%\")\nprint(f\"\\nExpected baseline: ~50% (random initialization)\")\nif abs(baseline_in_dist_accuracy - 0.5) < 0.1 and abs(baseline_out_dist_accuracy - 0.5) < 0.1:\n    print(\"Both baselines are near random as expected.\")\nelse:\n    print(\"NOTE: Baseline deviates from 50% - may indicate bias in initialization or data.\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Training Loop\n\nTrain the model with logging, validation, and checkpointing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create output directory for checkpoints\nos.makedirs(\"outputs\", exist_ok=True)\n\n# Training history - now tracks both in-dist and out-of-dist validation\nhistory = {\n    'train_loss': [],\n    'train_steps': [],\n    'epochs': [],\n    'reward_margins': [],\n    'preferred_rewards': [],\n    'rejected_rewards': [],\n    # In-distribution validation (CARA labels)\n    'in_dist_val_accuracy': [],\n    'in_dist_val_loss': [],\n    'in_dist_error_type_accuracy': {'too_risky': [], 'too_risk_averse': [], 'other': []},\n    # Out-of-distribution validation (Cooperate labels)\n    'out_dist_val_accuracy': [],\n    'out_dist_val_loss': [],\n    'out_dist_error_type_accuracy': {'too_risky': [], 'too_risk_averse': [], 'other': []},\n}\n\n# Legacy aliases for backward compatibility\nhistory['val_accuracy'] = history['out_dist_val_accuracy']\nhistory['val_loss'] = history['out_dist_val_loss']\nhistory['error_type_accuracy'] = history['out_dist_error_type_accuracy']\n\nprint(\"Starting training...\")\nprint(f\"Training data will be re-randomized each epoch for 'both' bucket cases\")\nprint(f\"Validating on both in-distribution (CARA) and out-of-distribution (Cooperate) sets\\n\")\n\nbest_val_accuracy = 0.0\nglobal_step = 0\n\n# Store initial weights for comparison\ninitial_weight = model.reward_head.weight.clone().detach()\ninitial_bias = model.reward_head.bias.clone().detach()\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"{'='*60}\")\n    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n    print(f\"{'='*60}\")\n    \n    # Recreate training dataset with epoch-specific randomization\n    # This ensures 'both' bucket cases get new random label choices each epoch\n    if epoch > 0:\n        print(f\"  Re-randomizing training data for epoch {epoch + 1}...\")\n        train_dataset = recreate_training_dataset(epoch)\n    \n    # Create dataloader for this epoch\n    train_dataloader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=True,\n        pin_memory=True if torch.cuda.is_available() else False,\n    )\n    \n    print(f\"  Batches this epoch: {len(train_dataloader)}\")\n    \n    model.train()\n    epoch_loss = 0.0\n    epoch_preferred_rewards = []\n    epoch_rejected_rewards = []\n    epoch_margins = []\n    \n    for step, batch in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        \n        # Forward pass for preferred options\n        preferred_rewards = model(\n            input_ids=batch['preferred_input_ids'].to(device),\n            attention_mask=batch['preferred_attention_mask'].to(device)\n        )\n        \n        # Forward pass for rejected options\n        rejected_rewards = model(\n            input_ids=batch['rejected_input_ids'].to(device),\n            attention_mask=batch['rejected_attention_mask'].to(device)\n        )\n        \n        # Track reward statistics\n        epoch_preferred_rewards.extend(preferred_rewards.detach().cpu().tolist())\n        epoch_rejected_rewards.extend(rejected_rewards.detach().cpu().tolist())\n        reward_margin = (preferred_rewards - rejected_rewards).detach()\n        epoch_margins.extend(reward_margin.cpu().tolist())\n        \n        # Compute Bradley-Terry loss\n        loss = bradley_terry_loss(preferred_rewards, rejected_rewards)\n        \n        # Check for NaN loss\n        if torch.isnan(loss):\n            print(f\"  WARNING: NaN loss detected at step {step + 1}\")\n            print(f\"    Preferred rewards: {preferred_rewards}\")\n            print(f\"    Rejected rewards: {rejected_rewards}\")\n            continue\n        \n        # Backward pass\n        loss.backward()\n        epoch_loss += loss.item()\n        \n        # Detailed diagnostics on first few steps\n        if global_step < 3:\n            print(f\"\\n  === Diagnostics for Step {global_step + 1} ===\")\n            \n            # Reward statistics\n            print(f\"  Rewards:\")\n            print(f\"    Preferred: mean={preferred_rewards.mean().item():.4f}, std={preferred_rewards.std().item():.4f}\")\n            print(f\"    Rejected:  mean={rejected_rewards.mean().item():.4f}, std={rejected_rewards.std().item():.4f}\")\n            print(f\"    Margin:    mean={reward_margin.mean().item():.4f}, std={reward_margin.std().item():.4f}\")\n            print(f\"    Loss:      {loss.item():.4f}\")\n            \n            # Gradient statistics - reward head\n            print(f\"  Gradients (Reward Head):\")\n            for name, param in model.reward_head.named_parameters():\n                if param.grad is not None:\n                    grad_norm = param.grad.norm().item()\n                    grad_mean = param.grad.mean().item()\n                    grad_max = param.grad.abs().max().item()\n                    print(f\"    {name}: norm={grad_norm:.6f}, mean={grad_mean:.6f}, max={grad_max:.6f}\")\n                else:\n                    print(f\"    {name}: NO GRADIENT!\")\n            \n            # Gradient statistics - LoRA layers\n            lora_grad_norms = []\n            for name, param in model.backbone.named_parameters():\n                if param.requires_grad and param.grad is not None:\n                    lora_grad_norms.append(param.grad.norm().item())\n            if lora_grad_norms:\n                print(f\"  Gradients (LoRA layers):\")\n                print(f\"    mean_norm={np.mean(lora_grad_norms):.6f}, max_norm={max(lora_grad_norms):.6f}\")\n            \n            # Parameter statistics\n            print(f\"  Parameters:\")\n            weight_change = (model.reward_head.weight - initial_weight).abs().max().item()\n            bias_change = (model.reward_head.bias - initial_bias).abs().max().item()\n            print(f\"    Reward head weight max change: {weight_change:.6f}\")\n            print(f\"    Reward head bias max change: {bias_change:.6f}\")\n            print()\n        \n        # Optimizer step\n        optimizer.step()\n        global_step += 1\n        \n        # Log every 50 steps (adjusted for smaller dataset)\n        if (step + 1) % 50 == 0:\n            avg_loss = epoch_loss / (step + 1)\n            weight_change = (model.reward_head.weight - initial_weight).abs().max().item()\n            recent_margin = np.mean(epoch_margins[-50:]) if len(epoch_margins) >= 50 else np.mean(epoch_margins)\n            \n            print(f\"  Step {step + 1}/{len(train_dataloader)} | \"\n                  f\"Loss: {avg_loss:.4f} | \"\n                  f\"Margin: {recent_margin:+.4f} | \"\n                  f\"Weight: {weight_change:.6f}\")\n            \n            history['train_loss'].append(avg_loss)\n            history['train_steps'].append(global_step)\n    \n    # End of epoch statistics\n    avg_train_loss = epoch_loss / len(train_dataloader)\n    \n    # Weight changes\n    total_weight_change = (model.reward_head.weight - initial_weight).abs().mean().item()\n    total_bias_change = (model.reward_head.bias - initial_bias).abs().mean().item()\n    \n    # Reward statistics\n    mean_preferred = np.mean(epoch_preferred_rewards)\n    mean_rejected = np.mean(epoch_rejected_rewards)\n    mean_margin = np.mean(epoch_margins)\n    std_margin = np.std(epoch_margins)\n    percent_correct = np.mean([m > 0 for m in epoch_margins]) * 100\n    \n    print(f\"\\n  Epoch {epoch + 1} Summary:\")\n    print(f\"    Loss: {avg_train_loss:.4f}\")\n    print(f\"    Rewards: preferred={mean_preferred:+.4f}, rejected={mean_rejected:+.4f}\")\n    print(f\"    Margin: mean={mean_margin:+.4f}, std={std_margin:.4f}\")\n    print(f\"    Correct ranking: {percent_correct:.1f}% (preferred > rejected)\")\n    print(f\"    Weight changes: weight={total_weight_change:.6f}, bias={total_bias_change:.6f}\")\n    \n    # Store for history\n    history['reward_margins'].append(mean_margin)\n    history['preferred_rewards'].append(mean_preferred)\n    history['rejected_rewards'].append(mean_rejected)\n    history['epochs'].append(epoch + 1)\n    \n    # ==========================================================================\n    # IN-DISTRIBUTION VALIDATION (CARA labels)\n    # ==========================================================================\n    print(f\"\\n  Running in-distribution validation (CARA labels)...\")\n    in_dist_eval = evaluate_model(model, in_dist_val_dataset)\n    \n    in_dist_accuracy = in_dist_eval['accuracy']\n    in_dist_loss = in_dist_eval['avg_loss']\n    in_dist_error_breakdown = in_dist_eval['error_type_breakdown']\n    \n    print(f\"    In-dist accuracy: {in_dist_accuracy:.4f} ({in_dist_accuracy*100:.2f}%)\")\n    print(f\"    In-dist loss: {in_dist_loss:.4f}\")\n    \n    if in_dist_error_breakdown:\n        print_error_type_breakdown(in_dist_error_breakdown, \"In-Dist Error Type Breakdown\")\n    \n    # Save to history\n    history['in_dist_val_accuracy'].append(in_dist_accuracy)\n    history['in_dist_val_loss'].append(in_dist_loss)\n    \n    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n        if error_type in in_dist_error_breakdown:\n            history['in_dist_error_type_accuracy'][error_type].append(\n                in_dist_error_breakdown[error_type]['accuracy']\n            )\n    \n    # ==========================================================================\n    # OUT-OF-DISTRIBUTION VALIDATION (Cooperate labels)\n    # ==========================================================================\n    print(f\"\\n  Running out-of-distribution validation (Cooperate labels)...\")\n    out_dist_eval = evaluate_model(model, out_dist_val_dataset)\n    \n    out_dist_accuracy = out_dist_eval['accuracy']\n    out_dist_loss = out_dist_eval['avg_loss']\n    out_dist_pref_scores = out_dist_eval['preferred_scores']\n    out_dist_rej_scores = out_dist_eval['rejected_scores']\n    out_dist_error_breakdown = out_dist_eval['error_type_breakdown']\n    \n    out_dist_margin = np.mean(out_dist_pref_scores) - np.mean(out_dist_rej_scores)\n    \n    print(f\"    Out-dist accuracy: {out_dist_accuracy:.4f} ({out_dist_accuracy*100:.2f}%)\")\n    print(f\"    Out-dist loss: {out_dist_loss:.4f}\")\n    print(f\"    Out-dist margin: {out_dist_margin:+.4f}\")\n    \n    if out_dist_error_breakdown:\n        print_error_type_breakdown(out_dist_error_breakdown, \"Out-Dist Error Type Breakdown\")\n    \n    # Save to history\n    history['out_dist_val_accuracy'].append(out_dist_accuracy)\n    history['out_dist_val_loss'].append(out_dist_loss)\n    \n    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n        if error_type in out_dist_error_breakdown:\n            history['out_dist_error_type_accuracy'][error_type].append(\n                out_dist_error_breakdown[error_type]['accuracy']\n            )\n    \n    # ==========================================================================\n    # CHECKPOINT SAVING (based on out-of-distribution accuracy for generalization)\n    # ==========================================================================\n    if out_dist_accuracy > best_val_accuracy:\n        best_val_accuracy = out_dist_accuracy\n        checkpoint_dir = f\"outputs/best_model_epoch{epoch+1}\"\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        \n        # Save LoRA adapter weights\n        model.backbone.save_pretrained(checkpoint_dir)\n        \n        # Save reward head separately\n        torch.save({\n            'reward_head_state_dict': model.reward_head.state_dict(),\n            'epoch': epoch + 1,\n            'in_dist_val_accuracy': in_dist_accuracy,\n            'out_dist_val_accuracy': out_dist_accuracy,\n            'in_dist_val_loss': in_dist_loss,\n            'out_dist_val_loss': out_dist_loss,\n            'in_dist_error_breakdown': in_dist_error_breakdown,\n            'out_dist_error_breakdown': out_dist_error_breakdown,\n            'optimizer_state_dict': optimizer.state_dict(),\n            'lora_config': {\n                'r': LORA_R,\n                'alpha': LORA_ALPHA,\n                'dropout': LORA_DROPOUT,\n                'target_modules': LORA_TARGET_MODULES,\n            }\n        }, os.path.join(checkpoint_dir, \"reward_head.pt\"))\n        \n        print(f\"\\n    New best! Saved checkpoint: {checkpoint_dir}\")\n    \n    # Clear GPU cache\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    print()\n\nprint(f\"\\n{'='*60}\")\nprint(\"Training Complete!\")\nprint(f\"{'='*60}\")\nprint(f\"Best out-of-dist validation accuracy: {best_val_accuracy:.4f} ({best_val_accuracy*100:.2f}%)\")\nprint(f\"Total steps: {global_step}\")\nprint(f\"Final model saved to: outputs/best_model_epoch*/\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Visualization and Results\n\nPlot training curves, compare against baseline, and analyze model performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FINAL EVALUATION ON BOTH VALIDATION SETS\n# =============================================================================\nprint(\"Evaluating final model on both validation sets...\")\n\n# In-distribution final evaluation\nprint(\"\\nIn-distribution validation (CARA labels)...\")\nfinal_in_dist_eval = evaluate_model(model, in_dist_val_dataset)\nfinal_in_dist_accuracy = final_in_dist_eval['accuracy']\nfinal_in_dist_loss = final_in_dist_eval['avg_loss']\nfinal_in_dist_pref_scores = final_in_dist_eval['preferred_scores']\nfinal_in_dist_rej_scores = final_in_dist_eval['rejected_scores']\nfinal_in_dist_error_breakdown = final_in_dist_eval['error_type_breakdown']\nfinal_in_dist_margins = np.array(final_in_dist_pref_scores) - np.array(final_in_dist_rej_scores)\n\n# Out-of-distribution final evaluation\nprint(\"Out-of-distribution validation (Cooperate labels)...\")\nfinal_out_dist_eval = evaluate_model(model, out_dist_val_dataset)\nfinal_out_dist_accuracy = final_out_dist_eval['accuracy']\nfinal_out_dist_loss = final_out_dist_eval['avg_loss']\nfinal_out_dist_pref_scores = final_out_dist_eval['preferred_scores']\nfinal_out_dist_rej_scores = final_out_dist_eval['rejected_scores']\nfinal_out_dist_error_breakdown = final_out_dist_eval['error_type_breakdown']\nfinal_out_dist_per_example = final_out_dist_eval['per_example_results']\nfinal_out_dist_margins = np.array(final_out_dist_pref_scores) - np.array(final_out_dist_rej_scores)\n\n# Legacy aliases for backward compatibility\nfinal_accuracy = final_out_dist_accuracy\nfinal_loss = final_out_dist_loss\npreferred_scores = final_out_dist_pref_scores\nrejected_scores = final_out_dist_rej_scores\nfinal_error_breakdown = final_out_dist_error_breakdown\nper_example_results = final_out_dist_per_example\nreward_margins = final_out_dist_margins\n\nprint(f\"\\n{'='*60}\")\nprint(\"FINAL VALIDATION RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"\\nIn-Distribution (CARA labels):\")\nprint(f\"  Accuracy: {final_in_dist_accuracy:.4f} ({final_in_dist_accuracy*100:.2f}%)\")\nprint(f\"  Loss: {final_in_dist_loss:.4f}\")\nprint(f\"  Mean margin: {np.mean(final_in_dist_margins):.4f}\")\n\nprint(f\"\\nOut-of-Distribution (Cooperate labels):\")\nprint(f\"  Accuracy: {final_out_dist_accuracy:.4f} ({final_out_dist_accuracy*100:.2f}%)\")\nprint(f\"  Loss: {final_out_dist_loss:.4f}\")\nprint(f\"  Mean margin: {np.mean(final_out_dist_margins):.4f}\")\n\n# Print error type breakdowns\nif final_in_dist_error_breakdown:\n    print_error_type_breakdown(final_in_dist_error_breakdown, \"In-Dist Final Error Type Breakdown\")\nif final_out_dist_error_breakdown:\n    print_error_type_breakdown(final_out_dist_error_breakdown, \"Out-Dist Final Error Type Breakdown\")\n\n# =============================================================================\n# BASELINE vs TRAINED COMPARISON\n# =============================================================================\nprint(f\"\\n{'='*60}\")\nprint(\"BASELINE vs TRAINED MODEL COMPARISON\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\n                         Baseline    Trained     Improvement\")\nprint(f\"  In-Dist Accuracy:      {baseline_in_dist_results['accuracy']*100:6.2f}%     {final_in_dist_accuracy*100:6.2f}%     {(final_in_dist_accuracy - baseline_in_dist_results['accuracy'])*100:+6.2f}%\")\nprint(f\"  Out-Dist Accuracy:     {baseline_out_dist_results['accuracy']*100:6.2f}%     {final_out_dist_accuracy*100:6.2f}%     {(final_out_dist_accuracy - baseline_out_dist_results['accuracy'])*100:+6.2f}%\")\nprint(f\"  In-Dist Loss:          {baseline_in_dist_results['loss']:6.4f}      {final_in_dist_loss:6.4f}      {final_in_dist_loss - baseline_in_dist_results['loss']:+6.4f}\")\nprint(f\"  Out-Dist Loss:         {baseline_out_dist_results['loss']:6.4f}      {final_out_dist_loss:6.4f}      {final_out_dist_loss - baseline_out_dist_results['loss']:+6.4f}\")\n\n# Compare error type breakdown for out-of-dist\nif final_out_dist_error_breakdown and baseline_out_dist_results.get('error_type_breakdown'):\n    print(f\"\\n  Out-Dist Error Type Accuracy Comparison:\")\n    print(f\"  {'Category':<20} {'Baseline':>10} {'Trained':>10} {'Improvement':>12}\")\n    print(f\"  {'-'*54}\")\n    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n        base_acc = baseline_out_dist_results['error_type_breakdown'].get(error_type, {}).get('accuracy', 0) * 100\n        train_acc = final_out_dist_error_breakdown.get(error_type, {}).get('accuracy', 0) * 100\n        improvement = train_acc - base_acc\n        print(f\"  {error_type:<20} {base_acc:>9.1f}% {train_acc:>9.1f}% {improvement:>+11.1f}%\")\n\nprint(f\"{'='*60}\")\n\n# =============================================================================\n# CREATE COMPREHENSIVE VISUALIZATIONS (5x3 grid with in-dist overlays)\n# =============================================================================\nfig = plt.figure(figsize=(20, 22))\ngs = fig.add_gridspec(5, 3, hspace=0.35, wspace=0.3)\n\nfig.suptitle('Reward Model Training Results\\nTrained on CARA (Risk-Aversion) | Validated on Both In-Dist and Out-of-Dist', \n             fontsize=16, fontweight='bold', y=0.995)\n\n# =============================================================================\n# Row 1: Training Progress\n# =============================================================================\n\n# Plot 1: Training Loss Over Time\nax1 = fig.add_subplot(gs[0, 0])\nif len(history['train_steps']) > 0:\n    ax1.plot(history['train_steps'], history['train_loss'], 'b-', linewidth=2, label='Training Loss')\n    ax1.axhline(y=baseline_out_dist_results['loss'], color='gray', linestyle='--', alpha=0.7, \n                label=f'Out-Dist Baseline ({baseline_out_dist_results[\"loss\"]:.3f})')\n    ax1.set_xlabel('Training Steps')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training Loss Over Time')\n    ax1.legend(fontsize=8)\n    ax1.grid(True, alpha=0.3)\n\n# Plot 2: Validation Accuracy Over Epochs (BOTH in-dist and out-of-dist)\nax2 = fig.add_subplot(gs[0, 1])\nif len(history['epochs']) > 0:\n    # Out-of-distribution (solid line)\n    epochs_with_baseline = [0] + history['epochs']\n    out_dist_acc_with_baseline = [baseline_out_dist_results['accuracy']] + history['out_dist_val_accuracy']\n    ax2.plot(epochs_with_baseline, out_dist_acc_with_baseline, 'g-o', linewidth=2, markersize=8, \n             label='Out-Dist (Cooperate)')\n    \n    # In-distribution (dashed line)\n    in_dist_acc_with_baseline = [baseline_in_dist_results['accuracy']] + history['in_dist_val_accuracy']\n    ax2.plot(epochs_with_baseline, in_dist_acc_with_baseline, 'b--s', linewidth=2, markersize=7, \n             label='In-Dist (CARA)')\n    \n    ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\n    ax2.scatter([0], [baseline_out_dist_results['accuracy']], color='orange', s=100, zorder=5, marker='D')\n    ax2.scatter([0], [baseline_in_dist_results['accuracy']], color='orange', s=100, zorder=5, marker='D', \n                label='Baseline')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Validation Accuracy Over Training\\n(In-Dist vs Out-of-Dist)')\n    ax2.set_ylim([0, 1])\n    ax2.legend(fontsize=8, loc='lower right')\n    ax2.grid(True, alpha=0.3)\n\n# Plot 3: Reward Margin Progression (Training)\nax3 = fig.add_subplot(gs[0, 2])\nif len(history['epochs']) > 0 and len(history['reward_margins']) > 0:\n    epochs_with_baseline = [0] + history['epochs']\n    margins_with_baseline = [baseline_out_dist_results['mean_margin']] + history['reward_margins']\n    ax3.plot(epochs_with_baseline, margins_with_baseline, 'purple', linewidth=2, marker='s', markersize=8)\n    ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='No Preference')\n    ax3.axhline(y=baseline_out_dist_results['mean_margin'], color='gray', linestyle=':', alpha=0.7, \n                label=f'Baseline ({baseline_out_dist_results[\"mean_margin\"]:.3f})')\n    ax3.scatter([0], [baseline_out_dist_results['mean_margin']], color='orange', s=100, zorder=5, marker='s', \n                label='Before Training')\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('Mean Reward Margin')\n    ax3.set_title('Preference Strength (Training)\\n(Preferred - Rejected)')\n    ax3.legend(fontsize=8)\n    ax3.grid(True, alpha=0.3)\n\n# =============================================================================\n# Row 2: Score Distributions (Out-of-Dist Trained Model)\n# =============================================================================\n\n# Plot 4: Score Distribution Comparison (Histogram) - Out-Dist Trained\nax4 = fig.add_subplot(gs[1, 0])\nbins = np.linspace(min(min(preferred_scores), min(rejected_scores)),\n                  max(max(preferred_scores), max(rejected_scores)), 30)\nax4.hist(preferred_scores, bins=bins, alpha=0.6, label='Preferred (Cooperate)', \n         color='green', density=True, edgecolor='black', linewidth=0.5)\nax4.hist(rejected_scores, bins=bins, alpha=0.6, label='Rejected (Non-Cooperate)', \n         color='red', density=True, edgecolor='black', linewidth=0.5)\nax4.set_xlabel('Reward Score')\nax4.set_ylabel('Density')\nax4.set_title('Out-Dist Trained: Score Distribution')\nax4.legend()\nax4.grid(True, alpha=0.3, axis='y')\n\n# Plot 5: Scatter Plot - Preferred vs Rejected Scores (Out-Dist Trained)\nax5 = fig.add_subplot(gs[1, 1])\nax5.scatter(rejected_scores, preferred_scores, alpha=0.5, s=30, c=reward_margins, \n            cmap='RdYlGn', edgecolors='black', linewidth=0.5)\nmin_val = min(min(rejected_scores), min(preferred_scores))\nmax_val = max(max(rejected_scores), max(preferred_scores))\nax5.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, linewidth=2, label='Equal Scores')\nax5.fill_between([min_val, max_val], [min_val, max_val], [max_val, max_val],\n                alpha=0.15, color='green', label='Correct (Preferred > Rejected)')\nax5.fill_between([min_val, max_val], [min_val, min_val], [min_val, max_val],\n                alpha=0.15, color='red', label='Incorrect')\nax5.set_xlabel('Rejected Score')\nax5.set_ylabel('Preferred Score')\nax5.set_title('Out-Dist Trained: Pairwise Comparison')\nax5.legend(fontsize=8)\nax5.grid(True, alpha=0.3)\nax5.axis('equal')\n\n# Plot 6: Reward Margin Distribution (Out-Dist Trained)\nax6 = fig.add_subplot(gs[1, 2])\nax6.hist(reward_margins, bins=40, alpha=0.7, color='purple', edgecolor='black', linewidth=0.5)\nax6.axvline(x=0, color='r', linestyle='--', linewidth=2, label='No Preference')\nax6.axvline(x=np.mean(reward_margins), color='g', linestyle='-', linewidth=2, \n            label=f'Out-Dist Mean: {np.mean(reward_margins):.3f}')\nax6.axvline(x=np.mean(final_in_dist_margins), color='b', linestyle='--', linewidth=2, \n            label=f'In-Dist Mean: {np.mean(final_in_dist_margins):.3f}')\nax6.set_xlabel('Reward Margin (Preferred - Rejected)')\nax6.set_ylabel('Count')\nax6.set_title('Trained: Margin Distribution Comparison')\nax6.legend(fontsize=8)\nax6.grid(True, alpha=0.3, axis='y')\n\n# =============================================================================\n# Row 3: Error Type Breakdown (with in-dist comparison)\n# =============================================================================\n\n# Plot 7: Error Type Accuracy Comparison (Baseline vs Trained, In-Dist vs Out-Dist)\nax7 = fig.add_subplot(gs[2, 0])\nif final_out_dist_error_breakdown:\n    error_types = ['too_risky', 'too_risk_averse', 'other']\n    error_type_labels = ['Too Risky', 'Too Risk-Averse', 'Other']\n    x = np.arange(len(error_types))\n    width = 0.2\n    \n    # Out-dist baseline\n    out_baseline_accs = [baseline_out_dist_results.get('error_type_breakdown', {}).get(et, {}).get('accuracy', 0) * 100 \n                        for et in error_types]\n    # Out-dist trained\n    out_trained_accs = [final_out_dist_error_breakdown.get(et, {}).get('accuracy', 0) * 100 \n                       for et in error_types]\n    # In-dist trained (if available)\n    in_trained_accs = [final_in_dist_error_breakdown.get(et, {}).get('accuracy', 0) * 100 \n                      for et in error_types] if final_in_dist_error_breakdown else [0, 0, 0]\n    \n    bars1 = ax7.bar(x - width, out_baseline_accs, width, label='Out-Dist Baseline', color='gray', alpha=0.7)\n    bars2 = ax7.bar(x, out_trained_accs, width, label='Out-Dist Trained', color='green', alpha=0.7)\n    bars3 = ax7.bar(x + width, in_trained_accs, width, label='In-Dist Trained', color='blue', alpha=0.7)\n    \n    ax7.axhline(y=50, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\n    ax7.set_ylabel('Accuracy (%)')\n    ax7.set_title('Accuracy by Error Type\\n(Baseline vs Trained, In vs Out)')\n    ax7.set_xticks(x)\n    ax7.set_xticklabels(error_type_labels, fontsize=9)\n    ax7.legend(fontsize=7, loc='upper right')\n    ax7.set_ylim([0, 100])\n    ax7.grid(True, alpha=0.3, axis='y')\n\n# Plot 8: Scatter Plot Colored by Error Type (Out-Dist)\nax8 = fig.add_subplot(gs[2, 1])\nif per_example_results:\n    colors = {'too_risky': 'orange', 'too_risk_averse': 'blue', 'other': 'gray'}\n    labels = {'too_risky': 'Too Risky', 'too_risk_averse': 'Too Risk-Averse', 'other': 'Other'}\n    \n    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n        examples = [r for r in per_example_results if r['error_type'] == error_type]\n        if examples:\n            pref = [r['preferred_score'] for r in examples]\n            rej = [r['rejected_score'] for r in examples]\n            ax8.scatter(rej, pref, alpha=0.5, s=30, c=colors[error_type], \n                       label=f'{labels[error_type]} (n={len(examples)})', edgecolors='black', linewidth=0.3)\n    \n    min_val = min(min(preferred_scores), min(rejected_scores))\n    max_val = max(max(preferred_scores), max(rejected_scores))\n    ax8.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, linewidth=2)\n    ax8.set_xlabel('Rejected Score')\n    ax8.set_ylabel('Preferred Score')\n    ax8.set_title('Out-Dist: Pairwise by Error Type')\n    ax8.legend(fontsize=8)\n    ax8.grid(True, alpha=0.3)\n    ax8.axis('equal')\n\n# Plot 9: Error Type Accuracy Over Epochs (Both in-dist and out-of-dist)\nax9 = fig.add_subplot(gs[2, 2])\nif history['out_dist_error_type_accuracy']['too_risky']:\n    epochs_for_plot = history['epochs']\n    colors = {'too_risky': 'orange', 'too_risk_averse': 'blue', 'other': 'gray'}\n    \n    # Out-of-dist (solid lines)\n    for error_type in ['too_risky', 'too_risk_averse']:\n        if history['out_dist_error_type_accuracy'][error_type]:\n            baseline_acc = baseline_out_dist_results.get('error_type_breakdown', {}).get(error_type, {}).get('accuracy', 0)\n            accs = [baseline_acc] + history['out_dist_error_type_accuracy'][error_type]\n            epochs_with_baseline = [0] + epochs_for_plot\n            ax9.plot(epochs_with_baseline, [a * 100 for a in accs], \n                    color=colors[error_type], marker='o', linewidth=2, \n                    markersize=6, label=f'Out: {error_type.replace(\"_\", \" \").title()}')\n    \n    # In-dist (dashed lines)\n    for error_type in ['too_risky', 'too_risk_averse']:\n        if history['in_dist_error_type_accuracy'][error_type]:\n            baseline_acc = baseline_in_dist_results.get('error_type_breakdown', {}).get(error_type, {}).get('accuracy', 0)\n            accs = [baseline_acc] + history['in_dist_error_type_accuracy'][error_type]\n            epochs_with_baseline = [0] + epochs_for_plot\n            ax9.plot(epochs_with_baseline, [a * 100 for a in accs], \n                    color=colors[error_type], marker='s', linewidth=2, linestyle='--',\n                    markersize=5, alpha=0.7, label=f'In: {error_type.replace(\"_\", \" \").title()}')\n    \n    ax9.axhline(y=50, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\n    ax9.set_xlabel('Epoch')\n    ax9.set_ylabel('Accuracy (%)')\n    ax9.set_title('Error Type Accuracy Over Training\\n(Solid=Out-Dist, Dashed=In-Dist)')\n    ax9.legend(fontsize=7, loc='lower right', ncol=2)\n    ax9.set_ylim([0, 100])\n    ax9.grid(True, alpha=0.3)\n\n# =============================================================================\n# Row 4: Baseline Comparison and Performance Analysis\n# =============================================================================\n\n# Plot 10: Baseline Score Distribution (Out-Dist)\nax10 = fig.add_subplot(gs[3, 0])\nbaseline_pref = np.array(baseline_out_dist_results['preferred_scores'])\nbaseline_rej = np.array(baseline_out_dist_results['rejected_scores'])\nbins_baseline = np.linspace(min(min(baseline_pref), min(baseline_rej)),\n                           max(max(baseline_pref), max(baseline_rej)), 30)\nax10.hist(baseline_pref, bins=bins_baseline, alpha=0.6, label='Preferred (Cooperate)', \n         color='green', density=True, edgecolor='black', linewidth=0.5)\nax10.hist(baseline_rej, bins=bins_baseline, alpha=0.6, label='Rejected (Non-Cooperate)', \n         color='red', density=True, edgecolor='black', linewidth=0.5)\nax10.set_xlabel('Reward Score')\nax10.set_ylabel('Density')\nax10.set_title('Baseline (Untrained): Score Distribution')\nax10.legend()\nax10.grid(True, alpha=0.3, axis='y')\n\n# Plot 11: Ranking Performance (Correct vs Incorrect by Type) - Out-Dist\nax11 = fig.add_subplot(gs[3, 1])\nif final_out_dist_error_breakdown:\n    error_types = ['too_risky', 'too_risk_averse', 'other']\n    error_type_labels = ['Too Risky', 'Too Risk-Averse', 'Other']\n    \n    correct_counts = [final_out_dist_error_breakdown.get(et, {}).get('correct', 0) for et in error_types]\n    incorrect_counts = [final_out_dist_error_breakdown.get(et, {}).get('total', 0) - \n                       final_out_dist_error_breakdown.get(et, {}).get('correct', 0) for et in error_types]\n    \n    x = np.arange(len(error_types))\n    width = 0.35\n    \n    bars1 = ax11.bar(x - width/2, correct_counts, width, label='Correct', \n                     color='green', alpha=0.7, edgecolor='black')\n    bars2 = ax11.bar(x + width/2, incorrect_counts, width, label='Incorrect', \n                     color='red', alpha=0.7, edgecolor='black')\n    \n    ax11.set_ylabel('Number of Examples')\n    ax11.set_title('Out-Dist: Correct vs Incorrect by Type')\n    ax11.set_xticks(x)\n    ax11.set_xticklabels(error_type_labels, fontsize=9)\n    ax11.legend(fontsize=8)\n    ax11.grid(True, alpha=0.3, axis='y')\n\n# Plot 12: Margin Distribution by Error Type - Out-Dist\nax12 = fig.add_subplot(gs[3, 2])\nif per_example_results:\n    colors = {'too_risky': 'orange', 'too_risk_averse': 'blue', 'other': 'gray'}\n    labels = {'too_risky': 'Too Risky', 'too_risk_averse': 'Too Risk-Averse', 'other': 'Other'}\n    \n    box_data = []\n    box_labels = []\n    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n        margins_for_type = [r['margin'] for r in per_example_results if r['error_type'] == error_type]\n        if margins_for_type:\n            box_data.append(margins_for_type)\n            box_labels.append(labels[error_type])\n    \n    if box_data:\n        bp = ax12.boxplot(box_data, labels=box_labels, patch_artist=True)\n        for patch, error_type in zip(bp['boxes'], ['too_risky', 'too_risk_averse', 'other']):\n            if error_type in colors:\n                patch.set_facecolor(colors[error_type])\n                patch.set_alpha(0.6)\n        \n        ax12.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='No Preference')\n        ax12.set_ylabel('Reward Margin')\n        ax12.set_title('Out-Dist: Margin by Error Type')\n        ax12.grid(True, alpha=0.3, axis='y')\n\n# =============================================================================\n# Row 5: Summary and CDFs\n# =============================================================================\n\n# Plot 13: Overall Summary Bar Chart (In-Dist and Out-Dist)\nax13 = fig.add_subplot(gs[4, 0])\ncategories = ['Baseline\\nIn-Dist', 'Trained\\nIn-Dist', 'Baseline\\nOut-Dist', 'Trained\\nOut-Dist']\naccuracies = [baseline_in_dist_results['accuracy'] * 100, final_in_dist_accuracy * 100,\n              baseline_out_dist_results['accuracy'] * 100, final_out_dist_accuracy * 100]\ncolors_bar = ['lightblue', 'blue', 'lightgreen', 'green']\nbars = ax13.bar(categories, accuracies, color=colors_bar, alpha=0.8, edgecolor='black', linewidth=1.5)\nax13.axhline(y=50, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\nfor bar, acc in zip(bars, accuracies):\n    height = bar.get_height()\n    ax13.text(bar.get_x() + bar.get_width()/2., height + 1,\n             f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\nax13.set_ylabel('Accuracy (%)')\nax13.set_title('Overall Accuracy Comparison')\nax13.set_ylim([0, 100])\nax13.legend()\nax13.grid(True, alpha=0.3, axis='y')\n\n# Plot 14: Score Evolution Over Training\nax14 = fig.add_subplot(gs[4, 1])\nif len(history['epochs']) > 0 and len(history['preferred_rewards']) > 0:\n    epochs_with_baseline = [0] + history['epochs']\n    pref_with_baseline = [np.mean(baseline_out_dist_results['preferred_scores'])] + history['preferred_rewards']\n    rej_with_baseline = [np.mean(baseline_out_dist_results['rejected_scores'])] + history['rejected_rewards']\n    ax14.plot(epochs_with_baseline, pref_with_baseline, 'g-o', \n             linewidth=2, markersize=8, label='Preferred (CARA)')\n    ax14.plot(epochs_with_baseline, rej_with_baseline, 'r-s', \n             linewidth=2, markersize=8, label='Rejected')\n    ax14.scatter([0, 0], [pref_with_baseline[0], rej_with_baseline[0]], \n                color='orange', s=100, zorder=5, marker='D', label='Baseline')\n    ax14.set_xlabel('Epoch')\n    ax14.set_ylabel('Mean Reward Score')\n    ax14.set_title('Training Score Evolution')\n    ax14.legend()\n    ax14.grid(True, alpha=0.3)\n\n# Plot 15: CDFs (In-Dist and Out-Dist comparison)\nax15 = fig.add_subplot(gs[4, 2])\n# Out-dist trained\nsorted_pref = np.sort(preferred_scores)\nsorted_rej = np.sort(rejected_scores)\ncdf_pref = np.arange(1, len(sorted_pref) + 1) / len(sorted_pref)\ncdf_rej = np.arange(1, len(sorted_rej) + 1) / len(sorted_rej)\nax15.plot(sorted_pref, cdf_pref, 'g-', linewidth=2, label='Out-Dist: Preferred')\nax15.plot(sorted_rej, cdf_rej, 'r-', linewidth=2, label='Out-Dist: Rejected')\n\n# In-dist trained\nsorted_in_pref = np.sort(final_in_dist_pref_scores)\nsorted_in_rej = np.sort(final_in_dist_rej_scores)\ncdf_in_pref = np.arange(1, len(sorted_in_pref) + 1) / len(sorted_in_pref)\ncdf_in_rej = np.arange(1, len(sorted_in_rej) + 1) / len(sorted_in_rej)\nax15.plot(sorted_in_pref, cdf_in_pref, 'g--', linewidth=1.5, alpha=0.7, label='In-Dist: Preferred')\nax15.plot(sorted_in_rej, cdf_in_rej, 'r--', linewidth=1.5, alpha=0.7, label='In-Dist: Rejected')\n\nax15.set_xlabel('Reward Score')\nax15.set_ylabel('Cumulative Probability')\nax15.set_title('CDF: In-Dist (dashed) vs Out-Dist (solid)')\nax15.legend(fontsize=8)\nax15.grid(True, alpha=0.3)\n\nplt.tight_layout()\n\n# Save plot\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nplot_path = f\"outputs/training_results_{timestamp}.png\"\nplt.savefig(plot_path, dpi=300, bbox_inches='tight')\nprint(f\"\\nComprehensive plots saved to: {plot_path}\")\n\nplt.show()\n\n# =============================================================================\n# SAVE RESULTS TO JSON\n# =============================================================================\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\nresults = {\n    'model_name': MODEL_NAME,\n    'architecture': {\n        'type': 'LoRA + Reward Head',\n        'lora_config': {\n            'r': LORA_R,\n            'alpha': LORA_ALPHA,\n            'dropout': LORA_DROPOUT,\n            'target_modules': LORA_TARGET_MODULES,\n        },\n        'trainable_params': trainable_params,\n        'total_params': total_params,\n        'trainable_percent': 100 * trainable_params / total_params,\n    },\n    'data': {\n        'training_file': TRAIN_DATA_FILE,\n        'out_dist_validation_file': VAL_DATA_FILE,\n        'in_dist_val_split': IN_DIST_VAL_SPLIT,\n        'training_label_type': 'CARA (risk-aversion)',\n        'in_dist_validation_label_type': 'CARA (risk-aversion)',\n        'out_dist_validation_label_type': 'cooperate',\n    },\n    'baseline': {\n        'in_dist': {\n            'accuracy': float(baseline_in_dist_results['accuracy']),\n            'loss': float(baseline_in_dist_results['loss']),\n            'mean_margin': float(baseline_in_dist_results['mean_margin']),\n            'error_type_breakdown': {\n                et: {k: float(v) if isinstance(v, (int, float)) else v \n                     for k, v in stats.items()}\n                for et, stats in baseline_in_dist_results.get('error_type_breakdown', {}).items()\n            },\n        },\n        'out_dist': {\n            'accuracy': float(baseline_out_dist_results['accuracy']),\n            'loss': float(baseline_out_dist_results['loss']),\n            'mean_margin': float(baseline_out_dist_results['mean_margin']),\n            'error_type_breakdown': {\n                et: {k: float(v) if isinstance(v, (int, float)) else v \n                     for k, v in stats.items()}\n                for et, stats in baseline_out_dist_results.get('error_type_breakdown', {}).items()\n            },\n        },\n    },\n    'trained': {\n        'in_dist': {\n            'final_accuracy': float(final_in_dist_accuracy),\n            'final_loss': float(final_in_dist_loss),\n            'mean_margin': float(np.mean(final_in_dist_margins)),\n            'error_type_breakdown': {\n                et: {k: float(v) if isinstance(v, (int, float)) else v \n                     for k, v in stats.items()}\n                for et, stats in final_in_dist_error_breakdown.items()\n            } if final_in_dist_error_breakdown else {},\n        },\n        'out_dist': {\n            'final_accuracy': float(final_out_dist_accuracy),\n            'final_loss': float(final_out_dist_loss),\n            'best_accuracy': float(best_val_accuracy),\n            'mean_margin': float(np.mean(final_out_dist_margins)),\n            'error_type_breakdown': {\n                et: {k: float(v) if isinstance(v, (int, float)) else v \n                     for k, v in stats.items()}\n                for et, stats in final_out_dist_error_breakdown.items()\n            } if final_out_dist_error_breakdown else {},\n        },\n    },\n    'improvement': {\n        'in_dist': {\n            'accuracy_gain': float(final_in_dist_accuracy - baseline_in_dist_results['accuracy']),\n            'accuracy_gain_percent': float((final_in_dist_accuracy - baseline_in_dist_results['accuracy']) * 100),\n        },\n        'out_dist': {\n            'accuracy_gain': float(final_out_dist_accuracy - baseline_out_dist_results['accuracy']),\n            'accuracy_gain_percent': float((final_out_dist_accuracy - baseline_out_dist_results['accuracy']) * 100),\n        },\n    },\n    'config': {\n        'num_epochs': NUM_EPOCHS,\n        'epochs_trained': len(history['epochs']),\n        'batch_size': BATCH_SIZE,\n        'learning_rate': LEARNING_RATE,\n        'weight_decay': WEIGHT_DECAY,\n        'training_samples': len(train_dataset),\n        'in_dist_validation_samples': len(in_dist_val_dataset),\n        'out_dist_validation_samples': len(out_dist_val_dataset),\n    },\n    'timestamp': timestamp\n}\n\nresults_path = f\"outputs/results_{timestamp}.json\"\nwith open(results_path, 'w') as f:\n    json.dump(results, f, indent=2)\nprint(f\"Results saved to: {results_path}\")\n\n# Save training history\nhistory_path = f\"outputs/training_history_{timestamp}.json\"\n# Convert numpy arrays to lists for JSON serialization\nhistory_serializable = {}\nfor key, value in history.items():\n    if isinstance(value, dict):\n        history_serializable[key] = {k: v if not isinstance(v, np.ndarray) else v.tolist() \n                                     for k, v in value.items()}\n    elif isinstance(value, np.ndarray):\n        history_serializable[key] = value.tolist()\n    else:\n        history_serializable[key] = value\n\nwith open(history_path, 'w') as f:\n    json.dump(history_serializable, f, indent=2)\nprint(f\"Training history saved to: {history_path}\")\n\n# =============================================================================\n# FINAL SUMMARY\n# =============================================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"Experiment Complete!\")\nprint(\"=\"*60)\nprint(f\"\\nDataset Summary:\")\nprint(f\"  Training: {len(train_dataset)} examples (CARA labels)\")\nprint(f\"  In-Dist Validation: {len(in_dist_val_dataset)} examples (CARA labels)\")\nprint(f\"  Out-of-Dist Validation: {len(out_dist_val_dataset)} examples (Cooperate labels)\")\n\nprint(f\"\\nAccuracy Results:\")\nprint(f\"  In-Distribution:\")\nprint(f\"    Baseline: {baseline_in_dist_results['accuracy']*100:.1f}%\")\nprint(f\"    Trained:  {final_in_dist_accuracy*100:.1f}%\")\nprint(f\"    Improvement: {(final_in_dist_accuracy - baseline_in_dist_results['accuracy'])*100:+.1f}%\")\nprint(f\"  Out-of-Distribution:\")\nprint(f\"    Baseline: {baseline_out_dist_results['accuracy']*100:.1f}%\")\nprint(f\"    Trained:  {final_out_dist_accuracy*100:.1f}%\")\nprint(f\"    Improvement: {(final_out_dist_accuracy - baseline_out_dist_results['accuracy'])*100:+.1f}%\")\n\nprint(f\"\\nTrainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\n\n# Find checkpoints\nimport glob\nimport shutil\ncheckpoint_dirs = glob.glob(\"outputs/best_model_epoch*\")\n\n# Automatic downloads (for Google Colab)\nprint(\"\\n\" + \"=\"*60)\nprint(\"Downloading Results...\")\nprint(\"=\"*60)\n\ntry:\n    from google.colab import files\n    \n    print(f\"Downloading: {plot_path}\")\n    files.download(plot_path)\n    \n    print(f\"Downloading: {results_path}\")\n    files.download(results_path)\n    \n    print(f\"Downloading: {history_path}\")\n    files.download(history_path)\n    \n    if checkpoint_dirs:\n        latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n        if os.path.isdir(latest_checkpoint):\n            zip_path = f\"{latest_checkpoint}.zip\"\n            shutil.make_archive(latest_checkpoint, 'zip', latest_checkpoint)\n            print(f\"Downloading: {zip_path}\")\n            files.download(zip_path)\n    \n    print(\"\\nAll files downloaded successfully!\")\n    \nexcept ImportError:\n    print(\"\\nNOTE: Not running in Google Colab - files saved to outputs/ directory\")\n    print(\"Files available:\")\n    print(f\"  - {plot_path}\")\n    print(f\"  - {results_path}\")\n    print(f\"  - {history_path}\")\n    if checkpoint_dirs:\n        latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n        print(f\"  - {latest_checkpoint}/ (LoRA checkpoint directory)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}