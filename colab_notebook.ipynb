{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Risk-Averse Reward Model Training\n\nThis notebook implements the complete risk aversion experiment for training **Qwen3-8B** to prefer risk-averse choices over risk-neutral ones.\n\n## Features\n- **CSV Data Loading**: Loads scenarios from `11_7_low_stakes_training_set.csv`\n- **Pure Single-Input Training**: Binary classification approach (risk-averse=1.0, risk-neutral=0.0)\n- **Low Stakes Training**: 1,000 situations with low-stakes gambles\n- **Larger Model**: Qwen3-8B (8 billion parameters) for better learning capacity\n- **GPU Optimizations**: fp16 mixed precision, gradient checkpointing, fused AdamW\n- **Comprehensive Visualization**: 4-panel plots showing training progress and results\n- **CARA vs LINEAR Utility**: Trains on CARA (risk-averse) vs LINEAR (risk-neutral) best options\n\n## Requirements\n- Google Colab with **High-RAM GPU** (A100 recommended for 8B model)\n- Runtime → Change runtime type → GPU → A100\n- Upload `11_7_low_stakes_training_set.csv` to Colab (in data/ folder or root)\n\n**Memory Optimized for 8B Model:** Uses gradient checkpointing, smaller batch size, and fp16 for memory efficiency\n\n## Expected Output\n- Training plots saved to `outputs/training_results_YYYYMMDD_HHMMSS.png`\n- Results JSON saved to `outputs/experiment_results.json`\n- Model saved to `risk_averse_model/`\n\n## Training Scale\n- **1,000 situations** (low-stakes dataset)\n- **2,000 training examples** (2 per situation)\n- **10 epochs**\n- **~2,500 total training steps**\n- Estimated training time: **30-60 minutes** on A100 GPU"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets accelerate torch pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "print(\"✓ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading Class\n",
    "\n",
    "Loads risk scenarios from CSV file with proper prompt modification and grouping by situation_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class RiskAversionDataLoader:\n    \"\"\"Load and process data from CSV file for risk aversion training\"\"\"\n    \n    def __init__(self, csv_file_path=\"11_7_low_stakes_training_set.csv\"):\n        self.csv_file_path = csv_file_path\n        \n    def load_and_process_data(self) -> pd.DataFrame:\n        \"\"\"Load CSV data and process it for training\n        \n        New format (11_7_low_stakes_training_set.csv):\n        - Multiple rows per situation (one per option)\n        - is_best_cara = True marks risk-averse option\n        - is_best_linear = True marks risk-neutral option\n        - option_index is 0-indexed, prompts use 1-indexed numbers\n        \"\"\"\n        # Check if CSV file exists\n        if not os.path.exists(self.csv_file_path):\n            raise FileNotFoundError(\n                f\"Required data file '{self.csv_file_path}' not found. \"\n                f\"Please ensure the CSV file is uploaded to Colab.\"\n            )\n        \n        # Load the CSV file\n        df = pd.read_csv(self.csv_file_path)\n        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n        \n        # Check required columns exist (new format)\n        required_columns = ['situation_id', 'prompt_text', 'option_index', 'is_best_cara', 'is_best_linear']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(\n                f\"Missing required columns in CSV: {missing_columns}. \"\n                f\"Available columns: {list(df.columns)}\"\n            )\n        \n        # Group by situation_id to get unique situations\n        situations = []\n        situations_skipped = 0\n        \n        for situation_id, group in df.groupby('situation_id'):\n            # Find risk-averse option (CARA best)\n            cara_rows = group[group['is_best_cara'] == True]\n            if len(cara_rows) == 0:\n                situations_skipped += 1\n                continue\n            cara_option = cara_rows.iloc[0]\n            \n            # Find risk-neutral option (LINEAR best)\n            linear_rows = group[group['is_best_linear'] == True]\n            if len(linear_rows) == 0:\n                situations_skipped += 1\n                continue\n            linear_option = linear_rows.iloc[0]\n            \n            # Get prompt text from first row (same for all options)\n            prompt_text = group.iloc[0]['prompt_text']\n            \n            # Convert 0-indexed option_index to 1-indexed option numbers (as shown in prompt)\n            correct_label = str(cara_option['option_index'] + 1)\n            incorrect_label = str(linear_option['option_index'] + 1)\n            \n            situations.append({\n                'situation_id': situation_id,\n                'prompt_text': prompt_text,\n                'correct_label': correct_label,  # Risk-averse option number\n                'incorrect_label': incorrect_label,  # Risk-neutral option number\n                'num_options': len(group)\n            })\n        \n        if situations_skipped > 0:\n            print(f\"Warning: Skipped {situations_skipped} situations missing CARA or LINEAR best options\")\n        \n        result_df = pd.DataFrame(situations)\n        print(f\"Processed into {len(result_df)} unique situations\")\n        \n        # Display sample data\n        if len(result_df) > 0:\n            sample = result_df.iloc[0]\n            print(f\"\\nSample situation:\")\n            print(f\"Prompt: {sample['prompt_text'][:200]}...\")\n            print(f\"Risk-averse choice (CARA best): Option {sample['correct_label']}\")\n            print(f\"Risk-neutral choice (LINEAR best): Option {sample['incorrect_label']}\")\n            print(f\"Number of options in this situation: {sample['num_options']}\")\n        \n        return result_df\n\nprint(\"✓ RiskAversionDataLoader defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Classes\n",
    "\n",
    "PyTorch datasets for both pairwise ranking training and single-input evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SingleInputDataset(Dataset):\n    \"\"\"Simple dataset for pure single-input classification training\n    \n    For each situation:\n    - Returns 2 examples: risk-averse option (label=1.0) and risk-neutral option (label=0.0)\n    \n    This teaches the model absolute scoring: risk-averse=high score, risk-neutral=low score.\n    \"\"\"\n    \n    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length=128):\n        # Reset index to ensure sequential 0-based indexing (important after train_test_split)\n        self.data = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        # Expand dataset: each situation generates 2 examples\n        # Example 0: risk-averse option (label=1.0)\n        # Example 1: risk-neutral option (label=0.0)\n        self.examples = []\n        for idx in range(len(self.data)):\n            row = self.data.iloc[idx]\n            # Risk-averse example\n            self.examples.append({\n                'situation_idx': idx,\n                'is_risk_averse': True,\n                'situation_id': row['situation_id']\n            })\n            # Risk-neutral example\n            self.examples.append({\n                'situation_idx': idx,\n                'is_risk_averse': False,\n                'situation_id': row['situation_id']\n            })\n        \n        print(f\"SingleInputDataset: {len(self.examples)} examples from {len(self.data)} situations\")\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        example_info = self.examples[idx]\n        row = self.data.iloc[example_info['situation_idx']]\n        \n        is_risk_averse = example_info['is_risk_averse']\n        option_text = row['correct_label'] if is_risk_averse else row['incorrect_label']\n        input_text = f\"{row['prompt_text']}\\n\\nChosen option: {option_text}\"\n        label = 1.0 if is_risk_averse else 0.0\n        \n        encoding = self.tokenizer(\n            input_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': label,\n            'situation_id': row['situation_id']\n        }\n\n\n# Keep PairwiseRiskAversionDataset for compatibility with final evaluation\nclass PairwiseRiskAversionDataset(Dataset):\n    \"\"\"Dataset that provides pairs of risk-averse vs risk-neutral choices for evaluation\"\"\"\n    \n    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length=128):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # Create input texts for both choices\n        risk_averse_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['correct_label']}\"\n        risk_neutral_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['incorrect_label']}\"\n        \n        # Tokenize both inputs\n        risk_averse_encoding = self.tokenizer(\n            risk_averse_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        risk_neutral_encoding = self.tokenizer(\n            risk_neutral_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'risk_averse_input_ids': risk_averse_encoding['input_ids'].flatten(),\n            'risk_averse_attention_mask': risk_averse_encoding['attention_mask'].flatten(),\n            'risk_neutral_input_ids': risk_neutral_encoding['input_ids'].flatten(),\n            'risk_neutral_attention_mask': risk_neutral_encoding['attention_mask'].flatten(),\n            'situation_id': row['situation_id']\n        }\n\nprint(\"✓ Dataset classes defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Reward Model - **Pure Single-Input Classification**\n\nRisk-averse reward model with **simplified single-input training** using **Qwen3-8B**.\n\n**Note:** Model loads with automatic dtype and uses gradient checkpointing for memory efficiency.\n\n### Training Approach: Pure Single-Input Classification\n\nThis model uses **pure single-input binary classification**:\n\n| Parameter | Value |\n|-----------|-------|\n| **Model Size** | 8B parameters (Qwen3-8B) |\n| **Training Data** | 1,000 low-stakes situations |\n| **Training Examples** | 2,000 examples (2 per situation) |\n| **Epochs** | 10 epochs |\n| **Total Training Steps** | ~2,500 steps |\n| **Training Time** | ~30-60 minutes on A100 |\n\n### Dataset: Low-Stakes Gambles\n\nThe new dataset (`11_7_low_stakes_training_set.csv`) contains:\n- **1,000 unique situations** with low-stakes monetary gambles\n- Each situation has 2-5 options to choose from\n- **CARA utility** (risk-averse): `u(w) = 1 - e^(-0.01w)`\n- **LINEAR utility** (risk-neutral): `u(w) = w`\n- Each situation has one option that maximizes CARA utility (risk-averse best)\n- Each situation has one option that maximizes LINEAR utility (risk-neutral best)\n\n### Loss Function\n\n**Binary Cross-Entropy Loss (BCE)**\n\n```python\nloss = BCEWithLogitsLoss(score, label)\n```\n\nWhere:\n- **Risk-averse options** (CARA best): `label = 1.0` → model learns to output high scores (~1.0)\n- **Risk-neutral options** (LINEAR best): `label = 0.0` → model learns to output low scores (~0.0)\n- Simple, direct supervision on individual options\n\n### Training Data Distribution\n\nFor the full dataset:\n- **Risk-averse examples**: 1,000 (label=1.0)\n- **Risk-neutral examples**: 1,000 (label=0.0)\n- **Total training examples**: 2,000\n- **80/20 split**: 1,600 train / 400 validation\n\n### Memory Optimizations for 8B Model\n\n- **Gradient checkpointing**: Trade compute for memory\n- **fp16 mixed precision**: Half precision for forward/backward\n- **Batch size 1 + gradient accumulation 8**: Effective batch size 8\n- **Sequence length 128**: Sufficient for most prompts\n\n### Expected Behavior\n\nWith 8B parameters and 1K low-stakes situations, we expect:\n- **Risk-averse option scores**: High values (~0.6-0.9 in sigmoid space)\n- **Risk-neutral option scores**: Low values (~0.1-0.4 in sigmoid space)\n- **Score distributions**: Clearly separated (green vs red in histogram)\n- **Risk preference scatter**: Points in green zone (above diagonal)\n- **Accuracy**: >0.6 (clearly better than random)\n- **Risk-averse preference rate**: >0.6 (model shows consistent preference)\n\n### Why Low Stakes?\n\nLow-stakes gambles ($0-$100 range) may be easier to learn because:\n- Simpler numerical ranges for the model to understand\n- More consistent risk patterns\n- Less extreme utility differences\n\n### Why Qwen3-8B?\n\nQwen3-8B offers:\n- Latest Qwen architecture with improved capabilities\n- Strong instruction-following and reasoning\n- Efficient training and inference\n- Open weights for research use"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class RiskAverseRewardModel(nn.Module):\n    \"\"\"Reward model for scoring risk-averse behavior with pure single-input classification\"\"\"\n    \n    def __init__(self, model_name=\"Qwen/Qwen3-8B\"):\n        super().__init__()\n        # Optimized for CUDA with automatic device mapping (fp16 handled by Trainer)\n        load_kwargs = {\n            \"num_labels\": 1,\n            \"device_map\": \"auto\",\n            \"low_cpu_mem_usage\": True,\n            \"torch_dtype\": \"auto\",  # Let it choose based on hardware\n        }\n\n        print(f\"Loading {model_name}...\")\n        self.backbone = AutoModelForSequenceClassification.from_pretrained(\n            model_name,\n            **load_kwargs\n        )\n        \n        # Enable gradient checkpointing for memory efficiency with 8B model\n        self.backbone.gradient_checkpointing_enable()\n        print(\"✓ Gradient checkpointing enabled for memory efficiency\")\n        \n    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n        \"\"\"Simple single-input forward pass\n        \n        During training: labels are provided (1.0 for risk-averse, 0.0 for risk-neutral)\n        During evaluation: labels are None, just return logits\n        \"\"\"\n        # Get model device from backbone parameters\n        device = next(self.backbone.parameters()).device\n        \n        # Ensure all tensors are on the correct device\n        if input_ids is not None:\n            input_ids = input_ids.to(device)\n        if attention_mask is not None:\n            attention_mask = attention_mask.to(device)\n        if labels is not None:\n            labels = labels.to(device)\n        \n        outputs = self.backbone(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        logits = outputs.logits.squeeze(-1)\n        \n        if labels is not None:\n            # Binary cross-entropy loss for single-input classification\n            # Teaches: risk-averse options should score high (1.0), risk-neutral low (0.0)\n            loss_fn = nn.BCEWithLogitsLoss()\n            loss = loss_fn(logits, labels)\n            \n            # Debug output during training (more frequent than before)\n            if self.training and torch.rand(1).item() < 0.01:  # 1% chance\n                pred_probs = torch.sigmoid(logits).mean().item()\n                print(f\"[TRAIN] Avg logit: {logits.mean().item():.3f}, \"\n                      f\"Avg sigmoid: {pred_probs:.3f}, Target avg: {labels.mean().item():.3f}\")\n            \n            return {\"loss\": loss, \"logits\": logits}\n        \n        return {\"logits\": logits}\n\nprint(\"✓ RiskAverseRewardModel defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Function\n",
    "\n",
    "Comprehensive evaluation with bad variation handling and detailed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_model(model, tokenizer, test_df: pd.DataFrame, return_detailed=False):\n    \"\"\"Evaluate the trained model on test situations\"\"\"\n    print(f\"Evaluating model on {len(test_df)} test situations...\")\n    \n    model.eval()\n    correct_predictions = 0\n    total_predictions = 0\n    risk_averse_wins = 0\n    situations_processed = 0\n    \n    # Store detailed results for plotting\n    results = {\n        'risk_averse_scores': [],\n        'risk_neutral_scores': [],\n        'predictions': [],\n        'expected': [],\n        'situation_ids': [],\n    }\n    \n    device = next(model.parameters()).device\n    \n    with torch.no_grad():\n        for _, row in test_df.iterrows():\n            # Test risk-averse option (correct_label)\n            risk_averse_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['correct_label']}\"\n            risk_averse_encoding = tokenizer(\n                risk_averse_text,\n                truncation=True,\n                padding='max_length',\n                max_length=128,\n                return_tensors='pt'\n            )\n            risk_averse_encoding = {k: v.to(device) for k, v in risk_averse_encoding.items()}\n            risk_averse_output = model(\n                input_ids=risk_averse_encoding['input_ids'], \n                attention_mask=risk_averse_encoding['attention_mask']\n            )\n            risk_averse_score = risk_averse_output[\"logits\"].item()\n            \n            # Test risk-neutral option (incorrect_label)\n            risk_neutral_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['incorrect_label']}\"\n            risk_neutral_encoding = tokenizer(\n                risk_neutral_text,\n                truncation=True,\n                padding='max_length',\n                max_length=128,\n                return_tensors='pt'\n            )\n            risk_neutral_encoding = {k: v.to(device) for k, v in risk_neutral_encoding.items()}\n            risk_neutral_output = model(\n                input_ids=risk_neutral_encoding['input_ids'], \n                attention_mask=risk_neutral_encoding['attention_mask']\n            )\n            risk_neutral_score = risk_neutral_output[\"logits\"].item()\n            \n            # Binary classification accuracy (each option independently)\n            risk_averse_pred = torch.sigmoid(torch.tensor(risk_averse_score)).item()\n            risk_neutral_pred = torch.sigmoid(torch.tensor(risk_neutral_score)).item()\n            \n            if risk_averse_pred > 0.5:  # Correctly classified as risk-averse\n                correct_predictions += 1\n            if risk_neutral_pred <= 0.5:  # Correctly classified as risk-neutral\n                correct_predictions += 1\n            total_predictions += 2\n            \n            # Check if risk-averse option scores higher (preference)\n            if risk_averse_score > risk_neutral_score:\n                risk_averse_wins += 1\n            \n            # Store results for plotting\n            results['risk_averse_scores'].append(risk_averse_score)\n            results['risk_neutral_scores'].append(risk_neutral_score)\n            results['predictions'].extend([risk_averse_pred, risk_neutral_pred])\n            results['expected'].extend([1.0, 0.0])\n            results['situation_ids'].append(row['situation_id'])\n            \n            # Print progress every 50 situations\n            situations_processed += 1\n            if situations_processed % 50 == 0:\n                current_acc = correct_predictions / total_predictions if total_predictions > 0 else 0\n                current_pref = risk_averse_wins / situations_processed\n                print(f\"  Progress: {situations_processed}/{len(test_df)} situations | \"\n                      f\"Accuracy: {current_acc:.3f} | Risk-averse preference: {current_pref:.3f}\")\n    \n    accuracy = correct_predictions / total_predictions\n    risk_averse_preference_rate = risk_averse_wins / len(test_df)\n    \n    print(f\"\\n✓ Evaluation complete:\")\n    print(f\"  Model accuracy: {accuracy:.3f}\")\n    print(f\"  Risk-averse preference rate: {risk_averse_preference_rate:.3f}\")\n    \n    if return_detailed:\n        results['risk_averse_preference_rate'] = risk_averse_preference_rate\n        return accuracy, results\n    return accuracy\n\nprint(\"✓ Evaluation function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plotting Functions\n",
    "\n",
    "Comprehensive 4-panel visualization showing training progress, score distributions, risk preferences, and performance summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(trainer, ax):\n",
    "    \"\"\"Plot training and validation loss over time\"\"\"\n",
    "    log_history = trainer.state.log_history\n",
    "    \n",
    "    train_steps = []\n",
    "    train_losses = []\n",
    "    eval_steps = []\n",
    "    eval_losses = []\n",
    "    \n",
    "    for log_entry in log_history:\n",
    "        if 'loss' in log_entry:\n",
    "            train_steps.append(log_entry['step'])\n",
    "            train_losses.append(log_entry['loss'])\n",
    "        if 'eval_loss' in log_entry:\n",
    "            eval_steps.append(log_entry['step'])\n",
    "            eval_losses.append(log_entry['eval_loss'])\n",
    "    \n",
    "    ax.plot(train_steps, train_losses, label='Training Loss', linewidth=2, marker='o', markersize=4)\n",
    "    if eval_losses:\n",
    "        ax.plot(eval_steps, eval_losses, label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "    \n",
    "    ax.set_xlabel('Training Steps')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training Progress')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "def plot_score_distribution(eval_results, ax):\n",
    "    \"\"\"Plot distribution of model scores for risk-averse vs risk-neutral choices\"\"\"\n",
    "    risk_averse_scores = eval_results['risk_averse_scores']\n",
    "    risk_neutral_scores = eval_results['risk_neutral_scores']\n",
    "    \n",
    "    bins = np.linspace(min(min(risk_averse_scores), min(risk_neutral_scores)),\n",
    "                       max(max(risk_averse_scores), max(risk_neutral_scores)), 21)\n",
    "    ax.hist(risk_averse_scores, bins=bins, alpha=0.7, label='Risk-Averse Options', \n",
    "            color='green', density=True)\n",
    "    ax.hist(risk_neutral_scores, bins=bins, alpha=0.7, label='Risk-Neutral Options', \n",
    "            color='red', density=True)\n",
    "    \n",
    "    ax.set_xlabel('Model Score (logits)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Score Distribution by Option Type')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "def plot_risk_preference_comparison(eval_results, ax):\n",
    "    \"\"\"Plot comparison of scores for risk-averse vs risk-neutral options\"\"\"\n",
    "    risk_averse_scores = np.array(eval_results['risk_averse_scores'])\n",
    "    risk_neutral_scores = np.array(eval_results['risk_neutral_scores'])\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(risk_neutral_scores, risk_averse_scores, alpha=0.6, s=50)\n",
    "    \n",
    "    # Add diagonal line (equal preference)\n",
    "    min_val = min(risk_neutral_scores.min(), risk_averse_scores.min())\n",
    "    max_val = max(risk_neutral_scores.max(), risk_averse_scores.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Equal Preference')\n",
    "    \n",
    "    # Add preference regions\n",
    "    ax.fill_between([min_val, max_val], [min_val, max_val], [max_val, max_val], \n",
    "                    alpha=0.2, color='green', label='Risk-Averse Preferred')\n",
    "    ax.fill_between([min_val, max_val], [min_val, min_val], [min_val, max_val], \n",
    "                    alpha=0.2, color='red', label='Risk-Neutral Preferred')\n",
    "    \n",
    "    ax.set_xlabel('Risk-Neutral Option Score')\n",
    "    ax.set_ylabel('Risk-Averse Option Score')\n",
    "    ax.set_title('Risk Preference Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "def plot_performance_summary(eval_results, accuracy, ax):\n",
    "    \"\"\"Plot model performance summary statistics\"\"\"\n",
    "    risk_averse_scores = np.array(eval_results['risk_averse_scores'])\n",
    "    risk_neutral_scores = np.array(eval_results['risk_neutral_scores'])\n",
    "    \n",
    "    correctly_prefers_risk_averse = np.mean(risk_averse_scores > risk_neutral_scores)\n",
    "    avg_risk_averse_score = np.mean(risk_averse_scores)\n",
    "    avg_risk_neutral_score = np.mean(risk_neutral_scores)\n",
    "    score_difference = avg_risk_averse_score - avg_risk_neutral_score\n",
    "    \n",
    "    metrics = ['Overall\\nAccuracy', 'Risk-Averse\\nPreference Rate']\n",
    "    values = [accuracy, correctly_prefers_risk_averse]\n",
    "    colors = ['blue', 'green']\n",
    "    \n",
    "    bars = ax.bar(metrics, values, color=colors, alpha=0.7)\n",
    "    \n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Model Performance Summary')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    ax.text(0.5, 0.5, f'Avg Score Difference:\\n{score_difference:+.3f}', \n",
    "            transform=ax.transAxes, ha='center', \n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "            fontweight='bold', fontsize=12)\n",
    "\n",
    "\n",
    "def plot_results(trainer, eval_results, accuracy):\n",
    "    \"\"\"Create comprehensive plots of training and evaluation results\"\"\"\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Risk-Averse Reward Model Training Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plot_training_loss(trainer, axes[0, 0])\n",
    "    plot_score_distribution(eval_results, axes[0, 1])\n",
    "    plot_risk_preference_comparison(eval_results, axes[1, 0])\n",
    "    plot_performance_summary(eval_results, accuracy, axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"outputs/training_results_{timestamp}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Plots saved to {filename}\")\n",
    "    \n",
    "    try:\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(\"Display not available - plots saved to file only\")\n",
    "\n",
    "print(\"✓ Plotting functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Function\n",
    "\n",
    "Complete training pipeline with GPU optimizations and pairwise ranking loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_reward_model(dataset_df: pd.DataFrame, model_name=\"Qwen/Qwen3-8B\"):\n    \"\"\"Train the risk-averse reward model with pure single-input classification\"\"\"\n    print(f\"Training reward model with {len(dataset_df)} situations using single-input classification...\")\n    print(f\"Model: {model_name}\")\n    print(f\"Training scale: ALL situations, 10 epochs\")\n    \n    # Initialize tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    model = RiskAverseRewardModel(model_name)\n    \n    # Set padding token in model config (required for batch sizes > 1)\n    model.backbone.config.pad_token_id = tokenizer.pad_token_id\n    \n    # Split data\n    train_df, val_df = train_test_split(dataset_df, test_size=0.2, random_state=42)\n    \n    # Create single-input datasets\n    train_dataset = SingleInputDataset(train_df, tokenizer, max_length=128)\n    val_dataset = SingleInputDataset(val_df, tokenizer, max_length=128)\n    \n    print(f\"Training: {len(train_df)} situations → {len(train_dataset)} examples (2 per situation)\")\n    print(f\"Validation: {len(val_df)} situations → {len(val_dataset)} examples (2 per situation)\")\n    \n    # Custom data collator that handles labels\n    def collate_fn(features):\n        input_ids = torch.stack([f['input_ids'] for f in features])\n        attention_mask = torch.stack([f['attention_mask'] for f in features])\n        labels = torch.tensor([f['labels'] for f in features], dtype=torch.float32)\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': labels\n        }\n    \n    # Training arguments optimized for 8B model on A100\n    training_args = TrainingArguments(\n        output_dir=\"./risk_averse_model\",\n        num_train_epochs=10,\n        per_device_train_batch_size=1,  # Small batch for 8B model\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=8,  # Effective batch size = 8\n        warmup_steps=100,  # Fewer warmup steps for smaller dataset\n        weight_decay=0.01,\n        learning_rate=2e-5,  # Standard for large models\n        logging_dir=\"./logs\",\n        logging_steps=50,  # More frequent logging for smaller dataset\n        report_to=\"none\",\n        eval_strategy=\"steps\",\n        eval_steps=250,  # More frequent eval for smaller dataset\n        save_steps=250,\n        save_total_limit=3,  # Keep only 3 best checkpoints\n        load_best_model_at_end=False,\n        fp16=True,\n        dataloader_pin_memory=True,\n        dataloader_num_workers=2,\n        remove_unused_columns=False,\n        optim=\"adamw_torch_fused\",\n        prediction_loss_only=True,\n        gradient_checkpointing=True,  # Memory efficiency\n    )\n    \n    # Standard Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        data_collator=collate_fn,\n    )\n    \n    print(\"✓ Model setup complete\")\n    \n    # Train\n    print(\"\\nStarting training:\")\n    print(f\"  - {len(train_dataset)} training examples\")\n    print(f\"  - 10 epochs\")\n    print(f\"  - Effective batch size: 8 (1 × 8 gradient accumulation)\")\n    print(f\"  - Estimated steps: ~{len(train_dataset) * 10 // 8}\")\n    print(\"\\nWatch for [TRAIN] debug outputs showing model learning progress\")\n    print(\"Estimated time: 30-60 minutes on A100 GPU...\\n\")\n    \n    trainer.train()\n    \n    return model, tokenizer, trainer\n\nprint(\"✓ Training function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Main Experiment Function\n",
    "\n",
    "Orchestrates the complete experiment from data loading to visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_experiment():\n    \"\"\"Run the complete risk aversion experiment with low-stakes training data\"\"\"\n    print(\"=== Risk-Averse Reward Model Experiment (Low Stakes) ===\")\n    print(f\"PyTorch device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n    print(f\"Model: Qwen3-8B (8 billion parameters)\")\n    print(f\"Dataset: 11_7_low_stakes_training_set.csv\")\n    \n    # Load data from CSV\n    print(\"\\n1. Loading risk scenario data from CSV...\")\n    loader = RiskAversionDataLoader()\n    full_dataset_df = loader.load_and_process_data()\n    \n    # Use ALL situations (no limiting!)\n    dataset_df = full_dataset_df\n    print(f\"Using ALL {len(dataset_df)} situations for training (no limit)\")\n    \n    # Split into train/test\n    train_df, test_df = train_test_split(dataset_df, test_size=0.2, random_state=42)\n    print(f\"Train: {len(train_df)} situations ({len(train_df)*2} examples)\")\n    print(f\"Test: {len(test_df)} situations ({len(test_df)*2} examples)\")\n    \n    # Train model\n    print(f\"\\n2. Training reward model with Qwen3-8B...\")\n    print(\"⚠️  This will take approximately 30-60 minutes on A100 GPU\")\n    model, tokenizer, trainer = train_reward_model(train_df)\n    \n    # Evaluate\n    print(f\"\\n3. Evaluating model...\")\n    accuracy, eval_results = evaluate_model(model, tokenizer, test_df, return_detailed=True)\n    \n    # Plot results\n    print(f\"\\n4. Creating visualizations...\")\n    plot_results(trainer, eval_results, accuracy)\n    \n    # Save results\n    os.makedirs(\"outputs\", exist_ok=True)\n    results = {\n        \"num_training_situations\": len(train_df),\n        \"num_test_situations\": len(test_df),\n        \"final_accuracy\": accuracy,\n        \"risk_averse_preference_rate\": eval_results['risk_averse_preference_rate'],\n        \"model_name\": \"Qwen/Qwen3-8B\",\n        \"training_epochs\": 10,\n        \"dataset\": \"11_7_low_stakes_training_set.csv\",\n        \"timestamp\": datetime.now().isoformat()\n    }\n    \n    with open(\"outputs/experiment_results.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n    \n    print(f\"\\n=== Experiment Complete ===\")\n    print(f\"Results saved to outputs/experiment_results.json\")\n    print(f\"Final accuracy: {accuracy:.3f}\")\n    print(f\"Risk-averse preference rate: {eval_results['risk_averse_preference_rate']:.3f}\")\n    \n    risk_averse_scores = np.array(eval_results['risk_averse_scores'])\n    risk_neutral_scores = np.array(eval_results['risk_neutral_scores'])\n    score_difference = np.mean(risk_averse_scores) - np.mean(risk_neutral_scores)\n    print(f\"Average score difference (risk-averse - risk-neutral): {score_difference:+.3f}\")\n    \n    return model, tokenizer, results\n\nprint(\"✓ Main experiment function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Run the Experiment\n\nExecute the complete training and evaluation pipeline.\n\n**Before running:** Ensure you have uploaded `11_7_low_stakes_training_set.csv` to this Colab environment (either in a `data/` folder or in the root directory)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run the experiment\ntry:\n    model, tokenizer, results = run_experiment()\n    print(\"\\n✓ Experiment completed successfully!\")\nexcept FileNotFoundError as e:\n    print(f\"\\n✗ Error: {e}\")\n    print(\"Please upload '11_7_low_stakes_training_set.csv' to Colab.\")\nexcept Exception as e:\n    print(f\"\\n✗ Experiment failed: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Model on Sample Scenario (Optional)\n",
    "\n",
    "Run inference on a specific scenario to see how the model scores different options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze sample training data to understand what the model sees\ntry:\n    print(\"=\"*80)\n    print(\"ANALYZING SAMPLE TRAINING DATA\")\n    print(\"=\"*80)\n    \n    loader = RiskAversionDataLoader()\n    dataset_df = loader.load_and_process_data()\n    \n    # Show first example\n    sample = dataset_df.iloc[0]\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"SAMPLE SITUATION #1\")\n    print(\"=\"*80)\n    \n    print(\"\\nPROMPT TEXT:\")\n    print(\"-\"*80)\n    print(sample['prompt_text'])\n    print(\"-\"*80)\n    \n    print(f\"\\nRisk-averse choice (correct_label): {sample['correct_label']}\")\n    print(f\"Risk-neutral choice (incorrect_label): {sample['incorrect_label']}\")\n    \n    # Show what the model actually receives during training\n    print(\"\\n\" + \"=\"*80)\n    print(\"WHAT THE MODEL SEES DURING TRAINING\")\n    print(\"=\"*80)\n    \n    print(\"\\nRISK-AVERSE EXAMPLE (label=1.0):\")\n    print(\"-\"*80)\n    risk_averse_input = f\"{sample['prompt_text']}\\n\\nChosen option: {sample['correct_label']}\"\n    print(risk_averse_input)\n    print(\"-\"*80)\n    \n    print(\"\\nRISK-NEUTRAL EXAMPLE (label=0.0):\")\n    print(\"-\"*80)\n    risk_neutral_input = f\"{sample['prompt_text']}\\n\\nChosen option: {sample['incorrect_label']}\"\n    print(risk_neutral_input)\n    print(\"-\"*80)\n    \n    # Show a second example for comparison\n    print(\"\\n\\n\" + \"=\"*80)\n    print(\"SAMPLE SITUATION #2 (for comparison)\")\n    print(\"=\"*80)\n    \n    sample2 = dataset_df.iloc[1]\n    print(\"\\nPROMPT TEXT:\")\n    print(\"-\"*80)\n    print(sample2['prompt_text'])\n    print(\"-\"*80)\n    print(f\"\\nRisk-averse choice: {sample2['correct_label']}\")\n    print(f\"Risk-neutral choice: {sample2['incorrect_label']}\")\n    \n    # Analysis\n    print(\"\\n\\n\" + \"=\"*80)\n    print(\"ANALYSIS: CAN THE MODEL LEARN FROM THIS?\")\n    print(\"=\"*80)\n    \n    print(\"\\nQUESTIONS TO ASK:\")\n    print(\"1. Does the prompt contain information about probabilities or utilities?\")\n    print(\"2. Are the options distinguishable by anything other than their letter?\")\n    print(\"3. Is there ANY textual difference between risk-averse and risk-neutral options?\")\n    print(\"4. Or is the model being asked to memorize 'A is always risk-averse'?\")\n    \n    print(\"\\nIf the options are just letters (A, B, C) with no context about what\")\n    print(\"they mean, the model has NOTHING to learn from. It would be like asking\")\n    print(\"the model to learn 'always prefer A over B' without any reason why.\")\n    \nexcept Exception as e:\n    print(f\"Analysis failed: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Download Results (Optional)\n",
    "\n",
    "Download the results and plots from Colab to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results (optional)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Download experiment results JSON\n",
    "    if os.path.exists(\"outputs/experiment_results.json\"):\n",
    "        files.download(\"outputs/experiment_results.json\")\n",
    "    \n",
    "    # Download the latest plot\n",
    "    import glob\n",
    "    plot_files = glob.glob(\"outputs/training_results_*.png\")\n",
    "    if plot_files:\n",
    "        latest_plot = max(plot_files, key=os.path.getctime)\n",
    "        files.download(latest_plot)\n",
    "        print(f\"Downloaded: {latest_plot}\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab - skip download\")\n",
    "except Exception as e:\n",
    "    print(f\"Download failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}