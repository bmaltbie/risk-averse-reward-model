{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Risk-Averse Reward Model Training\n\nThis notebook implements the complete risk aversion experiment for training **Qwen3-8B** to prefer risk-averse choices over risk-neutral ones.\n\n## Features\n- **Separate Train/Val Data**: Uses dedicated training and validation CSV files with different label types\n- **CARA-based Training**: Trains on CARA (risk-averse) labels with smart incorrect label selection\n- **Cooperation-based Validation**: Validates on cooperate labels to test generalization\n- **Per-Epoch Re-randomization**: Training data with \"both\" bucket_label gets re-randomized each epoch\n\n## Data Files\n- **Training**: `data/2025_12_5_training_set_low_stakes_balanced.csv` (CARA labels)\n- **Validation**: `data/2025_12_5_val_set_medium_stakes_balanced.csv` (cooperate labels)\n\n**Memory Optimized for 8B Model:** Uses gradient checkpointing, smaller batch size, and fp16 for memory efficiency"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q transformers datasets accelerate torch pandas numpy scikit-learn matplotlib seaborn hf_transfer peft\n\nprint(\"✓ Dependencies installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom sklearn.model_selection import train_test_split\nimport json\nfrom typing import List, Dict, Tuple\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nwarnings.filterwarnings('ignore')\n\n# Set plotting style\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Reproducibility function (will be called after config is loaded)\ndef set_seed(seed):\n    \"\"\"Set all seeds for reproducibility\"\"\"\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nprint(\"Libraries imported successfully!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Configuration\n\nAll configurable parameters for the experiment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CONFIGURATION - All configurable parameters for the experiment\n# =============================================================================\n\n# Model settings\nMODEL_NAME = \"Qwen/Qwen3-8B\"          # Base model to use\nMAX_LENGTH = 256                       # Maximum sequence length for tokenization\n\n# Training hyperparameters\nBATCH_SIZE = 2                         # Batch size per forward pass\nLEARNING_RATE = 2e-4                   # Learning rate for LoRA layers\nWEIGHT_DECAY = 0.01                    # L2 regularization\nNUM_EPOCHS = 10                        # Number of training epochs\n\n# LoRA configuration\nLORA_R = 8                             # LoRA rank (low for small dataset)\nLORA_ALPHA = 16                        # LoRA alpha (scaling = alpha/r = 2.0)\nLORA_DROPOUT = 0.05                    # LoRA dropout for regularization\nLORA_TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # Query and Value attention projections\n\n# Data settings - separate training and validation files\nTRAIN_DATA_FILE = \"data/2025_12_5_training_set_low_stakes_balanced.csv\"\nVAL_DATA_FILE = \"data/2025_12_5_val_set_medium_stakes_balanced.csv\"\nRANDOM_SEED = 42                       # Random seed for reproducibility\n\n# =============================================================================\n# Set seed for reproducibility\nset_seed(RANDOM_SEED)\n\nprint(\"Configuration loaded:\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Random seed: {RANDOM_SEED}\")\nprint(f\"\\nLoRA Configuration:\")\nprint(f\"  Rank (r): {LORA_R}\")\nprint(f\"  Alpha: {LORA_ALPHA}\")\nprint(f\"  Dropout: {LORA_DROPOUT}\")\nprint(f\"  Target modules: {LORA_TARGET_MODULES}\")\nprint(f\"\\nData Files:\")\nprint(f\"  Training: {TRAIN_DATA_FILE}\")\nprint(f\"  Validation: {VAL_DATA_FILE}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Data Loading Classes\n\nTwo specialized loaders for training and validation data:\n- **TrainingDataLoader**: Loads CARA-based labels with `low_bucket_label` logic for incorrect label selection\n- **ValidationDataLoader**: Loads cooperate-based labels for generalization testing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TrainingDataLoader:\n    \"\"\"Load and process training data with CARA-based labels and low_bucket_label logic.\n    \n    This loader handles the special logic for selecting incorrect labels based on\n    the low_bucket_label field:\n    - \"010_only\": use CARA_alpha_0_10_best_labels as incorrect (avoid over-risk-aversion)\n    - \"lin_only\": use linear_best_labels as incorrect (avoid being linear/risk-neutral)\n    - \"both\": randomly choose between the two (re-randomizes each epoch)\n    \"\"\"\n    \n    def __init__(self, csv_file_path: str, epoch: int = 0, random_seed: int = 42):\n        \"\"\"\n        Args:\n            csv_file_path: Path to training CSV file\n            epoch: Current epoch number (used for reproducible per-epoch randomization)\n            random_seed: Base random seed for reproducibility\n        \"\"\"\n        self.csv_file_path = csv_file_path\n        self.epoch = epoch\n        self.rng = np.random.default_rng(random_seed + epoch)  # Per-epoch randomization\n        \n    def load_and_process_data(self) -> pd.DataFrame:\n        \"\"\"Load CSV data and process it for training.\n        \n        Returns:\n            DataFrame with columns: situation_id, prompt_text, correct_label, incorrect_label, low_bucket_label\n        \"\"\"\n        # Check if CSV file exists\n        if not os.path.exists(self.csv_file_path):\n            raise FileNotFoundError(\n                f\"Required training data file '{self.csv_file_path}' not found. \"\n                f\"Please ensure the CSV file exists.\"\n            )\n        \n        # Load the CSV file\n        df = pd.read_csv(self.csv_file_path)\n        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n        \n        # Check required columns exist\n        required_columns = ['situation_id', 'prompt_text', 'CARA_correct_labels', 'low_bucket_label']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(\n                f\"Missing required columns in training CSV: {missing_columns}. \"\n                f\"Available columns: {list(df.columns)}\"\n            )\n        \n        # Group by situation_id, take first row of each group (all rows have same labels)\n        situations = df.groupby('situation_id').first().reset_index()\n        print(f\"Found {len(situations)} unique situations\")\n        \n        processed = []\n        skipped = 0\n        \n        for _, row in situations.iterrows():\n            try:\n                prompt_text = row['prompt_text']\n                \n                # Parse JSON array for correct labels\n                correct_labels = json.loads(row['CARA_correct_labels'])\n                if not correct_labels:\n                    skipped += 1\n                    continue\n                \n                # Get low_bucket_label and determine incorrect labels\n                low_bucket = row['low_bucket_label'].strip('\"')  # Remove surrounding quotes\n                \n                if low_bucket == '010_only':\n                    # Use CARA_alpha_0_10_best_labels as incorrect\n                    incorrect_labels = json.loads(row['CARA_alpha_0_10_best_labels'])\n                elif low_bucket == 'lin_only':\n                    # Use linear_best_labels as incorrect\n                    incorrect_labels = json.loads(row['linear_best_labels'])\n                elif low_bucket == 'both':\n                    # Randomly choose between linear and alpha_0_10 (re-randomizes each epoch)\n                    if self.rng.random() < 0.5:\n                        incorrect_labels = json.loads(row['linear_best_labels'])\n                    else:\n                        incorrect_labels = json.loads(row['CARA_alpha_0_10_best_labels'])\n                else:\n                    # Fallback: use CARA_incorrect_labels if available\n                    incorrect_labels = json.loads(row.get('CARA_incorrect_labels', '[]'))\n                \n                if not incorrect_labels:\n                    skipped += 1\n                    continue\n                \n                # Randomly select one label from each array\n                correct_label = str(self.rng.choice(correct_labels))\n                incorrect_label = str(self.rng.choice(incorrect_labels))\n                \n                processed.append({\n                    'situation_id': row['situation_id'],\n                    'prompt_text': prompt_text,\n                    'correct_label': correct_label,\n                    'incorrect_label': incorrect_label,\n                    'low_bucket_label': low_bucket,\n                })\n                \n            except (json.JSONDecodeError, KeyError) as e:\n                print(f\"Warning: Error processing situation {row['situation_id']}: {e}\")\n                skipped += 1\n                continue\n        \n        if skipped > 0:\n            print(f\"Warning: Skipped {skipped} situations due to missing/empty labels\")\n        \n        result_df = pd.DataFrame(processed)\n        print(f\"Processed into {len(result_df)} training examples (epoch {self.epoch})\")\n        \n        # Display low_bucket_label distribution\n        if 'low_bucket_label' in result_df.columns and len(result_df) > 0:\n            print(f\"\\nlow_bucket_label distribution:\")\n            for label, count in result_df['low_bucket_label'].value_counts().items():\n                print(f\"  {label}: {count} ({100*count/len(result_df):.1f}%)\")\n        \n        return result_df\n\n\nclass ValidationDataLoader:\n    \"\"\"Load and process validation data with cooperate-based labels.\n    \n    For validation, we use cooperate_correct_labels and cooperate_incorrect_labels\n    because cooperation is the goal we're validating for.\n    \"\"\"\n    \n    def __init__(self, csv_file_path: str, random_seed: int = 42):\n        \"\"\"\n        Args:\n            csv_file_path: Path to validation CSV file\n            random_seed: Random seed for reproducibility\n        \"\"\"\n        self.csv_file_path = csv_file_path\n        self.rng = np.random.default_rng(random_seed)\n    \n    def load_and_process_data(self) -> pd.DataFrame:\n        \"\"\"Load CSV data and process it for validation.\n        \n        Returns:\n            DataFrame with columns: situation_id, prompt_text, correct_label, incorrect_label\n        \"\"\"\n        # Check if CSV file exists\n        if not os.path.exists(self.csv_file_path):\n            raise FileNotFoundError(\n                f\"Required validation data file '{self.csv_file_path}' not found. \"\n                f\"Please ensure the CSV file exists.\"\n            )\n        \n        # Load the CSV file\n        df = pd.read_csv(self.csv_file_path)\n        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n        \n        # Check required columns exist\n        required_columns = ['situation_id', 'prompt_text', 'cooperate_correct_labels', 'cooperate_incorrect_labels']\n        missing_columns = [col for col in required_columns if col not in df.columns]\n        if missing_columns:\n            raise ValueError(\n                f\"Missing required columns in validation CSV: {missing_columns}. \"\n                f\"Available columns: {list(df.columns)}\"\n            )\n        \n        # Group by situation_id, take first row of each group\n        situations = df.groupby('situation_id').first().reset_index()\n        print(f\"Found {len(situations)} unique situations\")\n        \n        processed = []\n        skipped = 0\n        \n        for _, row in situations.iterrows():\n            try:\n                # Parse JSON arrays\n                correct_labels = json.loads(row['cooperate_correct_labels'])\n                incorrect_labels = json.loads(row['cooperate_incorrect_labels'])\n                \n                # Skip situations with empty labels\n                if not correct_labels or not incorrect_labels:\n                    skipped += 1\n                    continue\n                \n                # Randomly select one label from each array\n                correct_label = str(self.rng.choice(correct_labels))\n                incorrect_label = str(self.rng.choice(incorrect_labels))\n                \n                processed.append({\n                    'situation_id': row['situation_id'],\n                    'prompt_text': row['prompt_text'],\n                    'correct_label': correct_label,\n                    'incorrect_label': incorrect_label,\n                })\n                \n            except (json.JSONDecodeError, KeyError) as e:\n                print(f\"Warning: Error processing situation {row['situation_id']}: {e}\")\n                skipped += 1\n                continue\n        \n        if skipped > 0:\n            print(f\"Warning: Skipped {skipped} situations due to missing/empty labels\")\n        \n        result_df = pd.DataFrame(processed)\n        print(f\"Processed into {len(result_df)} validation examples\")\n        \n        return result_df\n\n\nprint(\"TrainingDataLoader and ValidationDataLoader defined\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Load and Validate Data\n\nLoad the separate training and validation data files.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load training data (epoch 0 for initial load)\ntrain_loader = TrainingDataLoader(TRAIN_DATA_FILE, epoch=0, random_seed=RANDOM_SEED)\ntrain_df = train_loader.load_and_process_data()\n\n# Load validation data (fixed, no per-epoch changes)\nval_loader = ValidationDataLoader(VAL_DATA_FILE, random_seed=RANDOM_SEED)\nval_df = val_loader.load_and_process_data()\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Dataset Summary:\")\nprint(f\"  Training file: {TRAIN_DATA_FILE}\")\nprint(f\"  Training situations: {len(train_df)}\")\nprint(f\"  Validation file: {VAL_DATA_FILE}\")\nprint(f\"  Validation situations: {len(val_df)}\")\nprint(f\"{'='*60}\")\n\n# Validate data format\nassert 'prompt_text' in train_df.columns, \"Missing prompt_text column in training data\"\nassert 'correct_label' in train_df.columns, \"Missing correct_label column in training data\"\nassert 'incorrect_label' in train_df.columns, \"Missing incorrect_label column in training data\"\n\nassert 'prompt_text' in val_df.columns, \"Missing prompt_text column in validation data\"\nassert 'correct_label' in val_df.columns, \"Missing correct_label column in validation data\"\nassert 'incorrect_label' in val_df.columns, \"Missing incorrect_label column in validation data\"\n\nprint(\"\\nData validation passed!\")\nprint(\"Data loading complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Pairwise Dataset for Reward Modeling\n\nDataset class that provides pairs of (preferred, rejected) options for Bradley-Terry loss.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PairwiseRewardDataset(Dataset):\n    \"\"\"Dataset for pairwise reward model training with Bradley-Terry loss\"\"\"\n    \n    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length: int = 256):\n        \"\"\"\n        Args:\n            dataframe: DataFrame with columns: prompt_text, correct_label, incorrect_label\n            tokenizer: Tokenizer for encoding text\n            max_length: Maximum sequence length\n        \"\"\"\n        self.data = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # Format: prompt + chosen option\n        preferred_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['correct_label']}\"\n        rejected_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['incorrect_label']}\"\n        \n        # Tokenize both options\n        preferred_encoding = self.tokenizer(\n            preferred_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        rejected_encoding = self.tokenizer(\n            rejected_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'preferred_input_ids': preferred_encoding['input_ids'].squeeze(0),\n            'preferred_attention_mask': preferred_encoding['attention_mask'].squeeze(0),\n            'rejected_input_ids': rejected_encoding['input_ids'].squeeze(0),\n            'rejected_attention_mask': rejected_encoding['attention_mask'].squeeze(0),\n        }\n\nprint(\"✓ PairwiseRewardDataset defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Reward Model Architecture\n\nQwen3-8B base model with LoRA adapters (q_proj, v_proj) + trainable scalar reward head.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class RewardModel(nn.Module):\n    \"\"\"Reward model with LoRA-adapted backbone and trainable scalar reward head\"\"\"\n    \n    def __init__(\n        self, \n        model_name: str = \"Qwen/Qwen3-8B\",\n        lora_r: int = 8,\n        lora_alpha: int = 16,\n        lora_dropout: float = 0.05,\n        lora_target_modules: list = None,\n    ):\n        super().__init__()\n        \n        # Load base model in fp16\n        print(f\"Loading base model: {model_name}\")\n        self.backbone = AutoModel.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        \n        hidden_size = self.backbone.config.hidden_size\n        \n        # Add reward head BEFORE applying LoRA\n        self.reward_head = nn.Linear(hidden_size, 1, bias=True)\n        \n        # Initialize reward head with small weights\n        nn.init.normal_(self.reward_head.weight, mean=0.0, std=0.01)\n        nn.init.zeros_(self.reward_head.bias)\n        \n        # Configure and apply LoRA\n        if lora_target_modules is None:\n            lora_target_modules = [\"q_proj\", \"v_proj\"]\n        \n        lora_config = LoraConfig(\n            r=lora_r,\n            lora_alpha=lora_alpha,\n            lora_dropout=lora_dropout,\n            target_modules=lora_target_modules,\n            bias=\"none\",\n            task_type=TaskType.FEATURE_EXTRACTION,\n        )\n        \n        self.backbone = get_peft_model(self.backbone, lora_config)\n        \n        # Print trainable parameter info\n        print(f\"\\nLoRA Configuration:\")\n        print(f\"  Rank: {lora_r}\")\n        print(f\"  Alpha: {lora_alpha}\")\n        print(f\"  Dropout: {lora_dropout}\")\n        print(f\"  Target modules: {lora_target_modules}\")\n        \n        self.backbone.print_trainable_parameters()\n        \n        print(f\"\\nReward head: Linear({hidden_size} -> 1) with bias\")\n        \n        # Count total trainable parameters\n        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        total_params = sum(p.numel() for p in self.parameters())\n        print(f\"Total trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.4f}%)\")\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Forward pass to compute scalar reward from final hidden state\n        \n        Args:\n            input_ids: Input token IDs [batch_size, seq_len]\n            attention_mask: Attention mask [batch_size, seq_len]\n            \n        Returns:\n            rewards: Scalar reward scores [batch_size]\n        \"\"\"\n        # Get hidden states - gradients flow through LoRA layers\n        outputs = self.backbone(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n        )\n        \n        # Extract final hidden state h_T (last non-padding token per sequence)\n        sequence_lengths = attention_mask.sum(dim=1) - 1  # 0-indexed position\n        batch_size = input_ids.shape[0]\n        \n        # Gather the hidden state at the last token position for each sequence\n        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n        last_hidden_states = hidden_states[\n            torch.arange(batch_size, device=hidden_states.device),\n            sequence_lengths\n        ]\n        \n        # Convert to fp32 for numerical stability in reward head\n        last_hidden_states = last_hidden_states.float()\n        \n        # Compute scalar reward: r = W^T * h_T + b\n        rewards = self.reward_head(last_hidden_states).squeeze(-1)\n        \n        return rewards\n\nprint(\"RewardModel class defined (with LoRA support)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Training Setup\n\nInitialize model, tokenizer, datasets, loss function, and optimizer.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load tokenizer\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Create datasets\nprint(\"Creating datasets...\")\ntrain_dataset = PairwiseRewardDataset(train_df, tokenizer, max_length=MAX_LENGTH)\nval_dataset = PairwiseRewardDataset(val_df, tokenizer, max_length=MAX_LENGTH)\n\nprint(f\"  Training examples: {len(train_dataset)}\")\nprint(f\"  Validation examples: {len(val_dataset)}\")\n\n\n# Helper function for per-epoch re-randomization of training data\ndef recreate_training_dataset(epoch: int):\n    \"\"\"Recreate training dataset with new randomization for 'both' bucket cases.\n    \n    This function creates a fresh training dataset where situations with\n    low_bucket_label='both' get a new random choice between linear_best_labels\n    and CARA_alpha_0_10_best_labels.\n    \n    Args:\n        epoch: Current epoch number (used for reproducible randomization)\n        \n    Returns:\n        PairwiseRewardDataset with re-randomized label selections\n    \"\"\"\n    loader = TrainingDataLoader(TRAIN_DATA_FILE, epoch=epoch, random_seed=RANDOM_SEED)\n    new_train_df = loader.load_and_process_data()\n    return PairwiseRewardDataset(new_train_df, tokenizer, max_length=MAX_LENGTH)\n\n\n# Initialize model with LoRA\nprint(\"\\nInitializing reward model with LoRA...\")\nmodel = RewardModel(\n    model_name=MODEL_NAME,\n    lora_r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    lora_target_modules=LORA_TARGET_MODULES,\n)\n\n# Get device from backbone\ndevice = next(model.backbone.parameters()).device\n\n# Move reward head to same device and ensure fp32\nmodel.reward_head = model.reward_head.to(device).float()\n\nprint(f\"\\n  Device: {device}\")\nprint(f\"  Backbone dtype: {next(model.backbone.parameters()).dtype}\")\nprint(f\"  Reward head dtype: {next(model.reward_head.parameters()).dtype}\")\n\n# Bradley-Terry loss function\ndef bradley_terry_loss(preferred_rewards, rejected_rewards):\n    \"\"\"\n    Bradley-Terry pairwise ranking loss\n    Loss = -log(sigmoid(r_preferred - r_rejected))\n\n    Encourages: r_preferred > r_rejected\n    \"\"\"\n    return -torch.log(torch.sigmoid(preferred_rewards - rejected_rewards)).mean()\n\n# Optimizer - train LoRA parameters AND reward head with different learning rates\noptimizer = torch.optim.AdamW([\n    {'params': model.backbone.parameters(), 'lr': LEARNING_RATE},\n    {'params': model.reward_head.parameters(), 'lr': LEARNING_RATE * 2.5},  # Higher LR for reward head\n], weight_decay=WEIGHT_DECAY)\n\nprint(f\"\\n{'='*60}\")\nprint(\"Training Configuration:\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  LoRA parameters LR: {LEARNING_RATE}\")\nprint(f\"  Reward head LR: {LEARNING_RATE * 2.5}\")\nprint(f\"  Weight decay (L2): {WEIGHT_DECAY}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Max sequence length: {MAX_LENGTH}\")\nprint(f\"  Per-epoch re-randomization: Enabled for 'both' bucket cases\")\nprint(f\"{'='*60}\")\nprint(\"\\nTraining setup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Evaluation Function\n\nCompute pairwise accuracy: percentage of pairs where preferred option scores higher.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def evaluate_model(model, dataset, batch_size=None):\n    \"\"\"\n    Evaluate model on pairwise accuracy\n    \n    Args:\n        model: RewardModel to evaluate\n        dataset: PairwiseRewardDataset\n        batch_size: Batch size for evaluation (defaults to BATCH_SIZE * 2)\n        \n    Returns:\n        accuracy: Float, percentage of pairs where preferred scores higher\n        avg_loss: Float, average Bradley-Terry loss\n        preferred_scores: List of reward scores for preferred options\n        rejected_scores: List of reward scores for rejected options\n    \"\"\"\n    if batch_size is None:\n        batch_size = BATCH_SIZE * 2  # Can use larger batch for eval (no gradients)\n    \n    model.eval()\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    correct = 0\n    total = 0\n    total_loss = 0.0\n    preferred_scores_list = []\n    rejected_scores_list = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            # Get rewards for preferred options\n            preferred_rewards = model(\n                input_ids=batch['preferred_input_ids'].to(device),\n                attention_mask=batch['preferred_attention_mask'].to(device)\n            )\n            \n            # Get rewards for rejected options\n            rejected_rewards = model(\n                input_ids=batch['rejected_input_ids'].to(device),\n                attention_mask=batch['rejected_attention_mask'].to(device)\n            )\n            \n            # Compute loss\n            loss = bradley_terry_loss(preferred_rewards, rejected_rewards)\n            total_loss += loss.item() * len(preferred_rewards)\n            \n            # Compute accuracy: count pairs where preferred > rejected\n            correct += (preferred_rewards > rejected_rewards).sum().item()\n            total += len(preferred_rewards)\n            \n            # Store scores for analysis\n            preferred_scores_list.extend(preferred_rewards.cpu().float().numpy())\n            rejected_scores_list.extend(rejected_rewards.cpu().float().numpy())\n    \n    accuracy = correct / total if total > 0 else 0.0\n    avg_loss = total_loss / total if total > 0 else 0.0\n    \n    return accuracy, avg_loss, preferred_scores_list, rejected_scores_list\n\nprint(\"Evaluation function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Baseline Evaluation (Before Training)\n\nEvaluate the randomly initialized reward model to establish a baseline for comparison.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# BASELINE EVALUATION - Before any training\n# =============================================================================\n# This establishes how well the randomly initialized reward head performs\n# Expected: ~50% accuracy (random guessing)\n\nprint(\"=\"*60)\nprint(\"BASELINE EVALUATION (Before Training)\")\nprint(\"=\"*60)\nprint(\"\\nEvaluating randomly initialized model on validation set...\")\n\nbaseline_accuracy, baseline_loss, baseline_pref_scores, baseline_rej_scores = evaluate_model(\n    model, val_dataset\n)\n\n# Calculate baseline statistics\nbaseline_margins = np.array(baseline_pref_scores) - np.array(baseline_rej_scores)\nbaseline_correct = np.sum(baseline_margins > 0)\nbaseline_incorrect = np.sum(baseline_margins < 0)\n\nprint(f\"\\nBaseline Results (Untrained Model):\")\nprint(f\"  Accuracy: {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\nprint(f\"  Loss: {baseline_loss:.4f}\")\nprint(f\"  Mean preferred score: {np.mean(baseline_pref_scores):.4f}\")\nprint(f\"  Mean rejected score: {np.mean(baseline_rej_scores):.4f}\")\nprint(f\"  Mean margin: {np.mean(baseline_margins):.4f}\")\nprint(f\"  Correct rankings: {baseline_correct} ({100*baseline_correct/len(baseline_margins):.1f}%)\")\nprint(f\"  Incorrect rankings: {baseline_incorrect} ({100*baseline_incorrect/len(baseline_margins):.1f}%)\")\n\n# Store baseline for later comparison\nbaseline_results = {\n    'accuracy': baseline_accuracy,\n    'loss': baseline_loss,\n    'preferred_scores': baseline_pref_scores,\n    'rejected_scores': baseline_rej_scores,\n    'margins': baseline_margins.tolist(),\n    'mean_margin': float(np.mean(baseline_margins)),\n    'std_margin': float(np.std(baseline_margins)),\n}\n\nprint(f\"\\nExpected baseline: ~50% (random initialization)\")\nprint(f\"Actual baseline:   {baseline_accuracy*100:.1f}%\")\nif abs(baseline_accuracy - 0.5) < 0.1:\n    print(\"Baseline is near random as expected - model has not learned any preference yet.\")\nelse:\n    print(\"NOTE: Baseline deviates from 50% - this may indicate bias in initialization or data.\")\n\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Training Loop\n\nTrain the model with logging, validation, and checkpointing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create output directory for checkpoints\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_steps': [],\n",
    "    'val_accuracy': [],\n",
    "    'val_loss': [],\n",
    "    'epochs': [],\n",
    "    'reward_margins': [],\n",
    "    'preferred_rewards': [],\n",
    "    'rejected_rewards': [],\n",
    "}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training data will be re-randomized each epoch for 'both' bucket cases\\n\")\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "global_step = 0\n",
    "\n",
    "# Store initial weights for comparison\n",
    "initial_weight = model.reward_head.weight.clone().detach()\n",
    "initial_bias = model.reward_head.bias.clone().detach()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Recreate training dataset with epoch-specific randomization\n",
    "    # This ensures 'both' bucket cases get new random label choices each epoch\n",
    "    if epoch > 0:\n",
    "        print(f\"  Re-randomizing training data for epoch {epoch + 1}...\")\n",
    "        train_dataset = recreate_training_dataset(epoch)\n",
    "    \n",
    "    # Create dataloader for this epoch\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "    \n",
    "    print(f\"  Batches this epoch: {len(train_dataloader)}\")\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_preferred_rewards = []\n",
    "    epoch_rejected_rewards = []\n",
    "    epoch_margins = []\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass for preferred options\n",
    "        preferred_rewards = model(\n",
    "            input_ids=batch['preferred_input_ids'].to(device),\n",
    "            attention_mask=batch['preferred_attention_mask'].to(device)\n",
    "        )\n",
    "        \n",
    "        # Forward pass for rejected options\n",
    "        rejected_rewards = model(\n",
    "            input_ids=batch['rejected_input_ids'].to(device),\n",
    "            attention_mask=batch['rejected_attention_mask'].to(device)\n",
    "        )\n",
    "        \n",
    "        # Track reward statistics\n",
    "        epoch_preferred_rewards.extend(preferred_rewards.detach().cpu().tolist())\n",
    "        epoch_rejected_rewards.extend(rejected_rewards.detach().cpu().tolist())\n",
    "        reward_margin = (preferred_rewards - rejected_rewards).detach()\n",
    "        epoch_margins.extend(reward_margin.cpu().tolist())\n",
    "        \n",
    "        # Compute Bradley-Terry loss\n",
    "        loss = bradley_terry_loss(preferred_rewards, rejected_rewards)\n",
    "        \n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"  WARNING: NaN loss detected at step {step + 1}\")\n",
    "            print(f\"    Preferred rewards: {preferred_rewards}\")\n",
    "            print(f\"    Rejected rewards: {rejected_rewards}\")\n",
    "            continue\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Detailed diagnostics on first few steps\n",
    "        if global_step < 3:\n",
    "            print(f\"\\n  === Diagnostics for Step {global_step + 1} ===\")\n",
    "            \n",
    "            # Reward statistics\n",
    "            print(f\"  Rewards:\")\n",
    "            print(f\"    Preferred: mean={preferred_rewards.mean().item():.4f}, std={preferred_rewards.std().item():.4f}\")\n",
    "            print(f\"    Rejected:  mean={rejected_rewards.mean().item():.4f}, std={rejected_rewards.std().item():.4f}\")\n",
    "            print(f\"    Margin:    mean={reward_margin.mean().item():.4f}, std={reward_margin.std().item():.4f}\")\n",
    "            print(f\"    Loss:      {loss.item():.4f}\")\n",
    "            \n",
    "            # Gradient statistics - reward head\n",
    "            print(f\"  Gradients (Reward Head):\")\n",
    "            for name, param in model.reward_head.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norm = param.grad.norm().item()\n",
    "                    grad_mean = param.grad.mean().item()\n",
    "                    grad_max = param.grad.abs().max().item()\n",
    "                    print(f\"    {name}: norm={grad_norm:.6f}, mean={grad_mean:.6f}, max={grad_max:.6f}\")\n",
    "                else:\n",
    "                    print(f\"    {name}: NO GRADIENT!\")\n",
    "            \n",
    "            # Gradient statistics - LoRA layers\n",
    "            lora_grad_norms = []\n",
    "            for name, param in model.backbone.named_parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    lora_grad_norms.append(param.grad.norm().item())\n",
    "            if lora_grad_norms:\n",
    "                print(f\"  Gradients (LoRA layers):\")\n",
    "                print(f\"    mean_norm={np.mean(lora_grad_norms):.6f}, max_norm={max(lora_grad_norms):.6f}\")\n",
    "            \n",
    "            # Parameter statistics\n",
    "            print(f\"  Parameters:\")\n",
    "            weight_change = (model.reward_head.weight - initial_weight).abs().max().item()\n",
    "            bias_change = (model.reward_head.bias - initial_bias).abs().max().item()\n",
    "            print(f\"    Reward head weight max change: {weight_change:.6f}\")\n",
    "            print(f\"    Reward head bias max change: {bias_change:.6f}\")\n",
    "            print()\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        \n",
    "        # Log every 50 steps (adjusted for smaller dataset)\n",
    "        if (step + 1) % 50 == 0:\n",
    "            avg_loss = epoch_loss / (step + 1)\n",
    "            weight_change = (model.reward_head.weight - initial_weight).abs().max().item()\n",
    "            recent_margin = np.mean(epoch_margins[-50:]) if len(epoch_margins) >= 50 else np.mean(epoch_margins)\n",
    "            \n",
    "            print(f\"  Step {step + 1}/{len(train_dataloader)} | \"\n",
    "                  f\"Loss: {avg_loss:.4f} | \"\n",
    "                  f\"Margin: {recent_margin:+.4f} | \"\n",
    "                  f\"Weight: {weight_change:.6f}\")\n",
    "            \n",
    "            history['train_loss'].append(avg_loss)\n",
    "            history['train_steps'].append(global_step)\n",
    "    \n",
    "    # End of epoch statistics\n",
    "    avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "    \n",
    "    # Weight changes\n",
    "    total_weight_change = (model.reward_head.weight - initial_weight).abs().mean().item()\n",
    "    total_bias_change = (model.reward_head.bias - initial_bias).abs().mean().item()\n",
    "    \n",
    "    # Reward statistics\n",
    "    mean_preferred = np.mean(epoch_preferred_rewards)\n",
    "    mean_rejected = np.mean(epoch_rejected_rewards)\n",
    "    mean_margin = np.mean(epoch_margins)\n",
    "    std_margin = np.std(epoch_margins)\n",
    "    percent_correct = np.mean([m > 0 for m in epoch_margins]) * 100\n",
    "    \n",
    "    print(f\"\\n  Epoch {epoch + 1} Summary:\")\n",
    "    print(f\"    Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"    Rewards: preferred={mean_preferred:+.4f}, rejected={mean_rejected:+.4f}\")\n",
    "    print(f\"    Margin: mean={mean_margin:+.4f}, std={std_margin:.4f}\")\n",
    "    print(f\"    Correct ranking: {percent_correct:.1f}% (preferred > rejected)\")\n",
    "    print(f\"    Weight changes: weight={total_weight_change:.6f}, bias={total_bias_change:.6f}\")\n",
    "    \n",
    "    # Store for history\n",
    "    history['reward_margins'].append(mean_margin)\n",
    "    history['preferred_rewards'].append(mean_preferred)\n",
    "    history['rejected_rewards'].append(mean_rejected)\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"\\n  Running validation...\")\n",
    "    val_accuracy, val_loss, val_pref_scores, val_rej_scores = evaluate_model(model, val_dataset)\n",
    "    \n",
    "    val_margin = np.mean(val_pref_scores) - np.mean(val_rej_scores)\n",
    "    \n",
    "    print(f\"    Validation accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "    print(f\"    Validation loss: {val_loss:.4f}\")\n",
    "    print(f\"    Validation margin: {val_margin:+.4f}\")\n",
    "    \n",
    "    # Save to history\n",
    "    history['val_accuracy'].append(val_accuracy)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['epochs'].append(epoch + 1)\n",
    "    \n",
    "    # Save checkpoint if best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        checkpoint_dir = f\"outputs/best_model_epoch{epoch+1}\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Save LoRA adapter weights\n",
    "        model.backbone.save_pretrained(checkpoint_dir)\n",
    "        \n",
    "        # Save reward head separately\n",
    "        torch.save({\n",
    "            'reward_head_state_dict': model.reward_head.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lora_config': {\n",
    "                'r': LORA_R,\n",
    "                'alpha': LORA_ALPHA,\n",
    "                'dropout': LORA_DROPOUT,\n",
    "                'target_modules': LORA_TARGET_MODULES,\n",
    "            }\n",
    "        }, os.path.join(checkpoint_dir, \"reward_head.pt\"))\n",
    "        \n",
    "        print(f\"    New best! Saved checkpoint: {checkpoint_dir}\")\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best validation accuracy: {best_val_accuracy:.4f} ({best_val_accuracy*100:.2f}%)\")\n",
    "print(f\"Total steps: {global_step}\")\n",
    "print(f\"Final model saved to: outputs/best_model_epoch*/\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Visualization and Results\n\nPlot training curves, compare against baseline, and analyze model performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Evaluate final model on validation set\nprint(\"Evaluating final model on validation set...\")\nfinal_accuracy, final_loss, preferred_scores, rejected_scores = evaluate_model(\n    model, val_dataset\n)\n\nprint(f\"\\nFinal Validation Results:\")\nprint(f\"  Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\nprint(f\"  Loss: {final_loss:.4f}\")\nprint(f\"  Mean preferred score: {np.mean(preferred_scores):.4f}\")\nprint(f\"  Mean rejected score: {np.mean(rejected_scores):.4f}\")\nprint(f\"  Score difference: {np.mean(preferred_scores) - np.mean(rejected_scores):.4f}\")\n\n# Calculate reward margins\nreward_margins = np.array(preferred_scores) - np.array(rejected_scores)\n\n# Compare with baseline\nprint(f\"\\n{'='*60}\")\nprint(\"BASELINE vs TRAINED MODEL COMPARISON\")\nprint(f\"{'='*60}\")\nprint(f\"                    Baseline    Trained     Improvement\")\nprint(f\"  Accuracy:         {baseline_results['accuracy']*100:6.2f}%     {final_accuracy*100:6.2f}%     {(final_accuracy - baseline_results['accuracy'])*100:+6.2f}%\")\nprint(f\"  Loss:             {baseline_results['loss']:6.4f}      {final_loss:6.4f}      {final_loss - baseline_results['loss']:+6.4f}\")\nprint(f\"  Mean Margin:      {baseline_results['mean_margin']:+6.4f}     {np.mean(reward_margins):+6.4f}     {np.mean(reward_margins) - baseline_results['mean_margin']:+6.4f}\")\nprint(f\"{'='*60}\")\n\n# Create comprehensive visualizations (4x3 grid to include baseline comparisons)\nfig = plt.figure(figsize=(20, 16))\ngs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.3)\n\nfig.suptitle('Reward Model Training Results\\nTrained on CARA (Risk-Aversion) | Validated on Cooperation', \n             fontsize=16, fontweight='bold', y=0.995)\n\n# Row 1: Training Progress\n# Plot 1: Training Loss Over Time\nax1 = fig.add_subplot(gs[0, 0])\nif len(history['train_steps']) > 0:\n    ax1.plot(history['train_steps'], history['train_loss'], 'b-', linewidth=2, label='Training Loss')\n    ax1.axhline(y=baseline_results['loss'], color='gray', linestyle='--', alpha=0.7, label=f'Baseline Loss ({baseline_results[\"loss\"]:.3f})')\n    ax1.set_xlabel('Training Steps')\n    ax1.set_ylabel('Loss')\n    ax1.set_title('Training Loss Over Time')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n# Plot 2: Validation Accuracy Over Epochs (with baseline)\nax2 = fig.add_subplot(gs[0, 1])\nif len(history['epochs']) > 0:\n    # Add epoch 0 as baseline\n    epochs_with_baseline = [0] + history['epochs']\n    accuracy_with_baseline = [baseline_results['accuracy']] + history['val_accuracy']\n    ax2.plot(epochs_with_baseline, accuracy_with_baseline, 'g-o', linewidth=2, markersize=8, label='Validation Accuracy')\n    ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\n    ax2.axhline(y=baseline_results['accuracy'], color='gray', linestyle=':', alpha=0.7, label=f'Baseline ({baseline_results[\"accuracy\"]*100:.1f}%)')\n    ax2.scatter([0], [baseline_results['accuracy']], color='orange', s=100, zorder=5, marker='s', label='Before Training')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Validation Accuracy (Cooperation)')\n    ax2.set_ylim([0, 1])\n    ax2.legend(fontsize=8)\n    ax2.grid(True, alpha=0.3)\n\n# Plot 3: Reward Margin Progression (with baseline)\nax3 = fig.add_subplot(gs[0, 2])\nif len(history['epochs']) > 0 and len(history['reward_margins']) > 0:\n    epochs_with_baseline = [0] + history['epochs']\n    margins_with_baseline = [baseline_results['mean_margin']] + history['reward_margins']\n    ax3.plot(epochs_with_baseline, margins_with_baseline, 'purple', linewidth=2, marker='s', markersize=8)\n    ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='No Preference')\n    ax3.axhline(y=baseline_results['mean_margin'], color='gray', linestyle=':', alpha=0.7, label=f'Baseline ({baseline_results[\"mean_margin\"]:.3f})')\n    ax3.scatter([0], [baseline_results['mean_margin']], color='orange', s=100, zorder=5, marker='s', label='Before Training')\n    ax3.set_xlabel('Epoch')\n    ax3.set_ylabel('Mean Reward Margin')\n    ax3.set_title('Preference Strength (Training)\\n(Preferred - Rejected)')\n    ax3.legend(fontsize=8)\n    ax3.grid(True, alpha=0.3)\n\n# Row 2: Score Distributions (Trained Model)\n# Plot 4: Score Distribution Comparison (Histogram) - Trained\nax4 = fig.add_subplot(gs[1, 0])\nbins = np.linspace(min(min(preferred_scores), min(rejected_scores)),\n                  max(max(preferred_scores), max(rejected_scores)), 30)\nax4.hist(preferred_scores, bins=bins, alpha=0.6, label='Preferred (Cooperate)', \n         color='green', density=True, edgecolor='black', linewidth=0.5)\nax4.hist(rejected_scores, bins=bins, alpha=0.6, label='Rejected (Non-Cooperate)', \n         color='red', density=True, edgecolor='black', linewidth=0.5)\nax4.set_xlabel('Reward Score')\nax4.set_ylabel('Density')\nax4.set_title('Trained Model: Score Distribution (Val)')\nax4.legend()\nax4.grid(True, alpha=0.3, axis='y')\n\n# Plot 5: Scatter Plot - Preferred vs Rejected Scores (Trained)\nax5 = fig.add_subplot(gs[1, 1])\nax5.scatter(rejected_scores, preferred_scores, alpha=0.5, s=30, c=reward_margins, \n            cmap='RdYlGn', edgecolors='black', linewidth=0.5)\nmin_val = min(min(rejected_scores), min(preferred_scores))\nmax_val = max(max(rejected_scores), max(preferred_scores))\nax5.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, linewidth=2, label='Equal Scores')\nax5.fill_between([min_val, max_val], [min_val, max_val], [max_val, max_val],\n                alpha=0.15, color='green', label='Cooperate Preferred')\nax5.fill_between([min_val, max_val], [min_val, min_val], [min_val, max_val],\n                alpha=0.15, color='red', label='Non-Cooperate Preferred')\nax5.set_xlabel('Non-Cooperate Score')\nax5.set_ylabel('Cooperate Score')\nax5.set_title('Trained Model: Pairwise Comparison (Val)')\nax5.legend(fontsize=8)\nax5.grid(True, alpha=0.3)\nax5.axis('equal')\n\n# Plot 6: Reward Margin Distribution (Trained)\nax6 = fig.add_subplot(gs[1, 2])\nax6.hist(reward_margins, bins=40, alpha=0.7, color='purple', edgecolor='black', linewidth=0.5)\nax6.axvline(x=0, color='r', linestyle='--', linewidth=2, label='No Preference')\nax6.axvline(x=np.mean(reward_margins), color='g', linestyle='-', linewidth=2, \n            label=f'Trained Mean: {np.mean(reward_margins):.3f}')\nax6.axvline(x=baseline_results['mean_margin'], color='gray', linestyle=':', linewidth=2, \n            label=f'Baseline Mean: {baseline_results[\"mean_margin\"]:.3f}')\nax6.set_xlabel('Reward Margin (Preferred - Rejected)')\nax6.set_ylabel('Count')\nax6.set_title('Trained Model: Margin Distribution (Val)')\nax6.legend(fontsize=8)\nax6.grid(True, alpha=0.3, axis='y')\n\n# Row 3: Baseline Comparison\n# Plot 7: Baseline Score Distribution (for comparison)\nax7 = fig.add_subplot(gs[2, 0])\nbaseline_pref = np.array(baseline_results['preferred_scores'])\nbaseline_rej = np.array(baseline_results['rejected_scores'])\nbins_baseline = np.linspace(min(min(baseline_pref), min(baseline_rej)),\n                           max(max(baseline_pref), max(baseline_rej)), 30)\nax7.hist(baseline_pref, bins=bins_baseline, alpha=0.6, label='Preferred (Cooperate)', \n         color='green', density=True, edgecolor='black', linewidth=0.5)\nax7.hist(baseline_rej, bins=bins_baseline, alpha=0.6, label='Rejected (Non-Cooperate)', \n         color='red', density=True, edgecolor='black', linewidth=0.5)\nax7.set_xlabel('Reward Score')\nax7.set_ylabel('Density')\nax7.set_title('Baseline (Untrained): Score Distribution (Val)')\nax7.legend()\nax7.grid(True, alpha=0.3, axis='y')\n\n# Plot 8: Baseline Scatter Plot\nax8 = fig.add_subplot(gs[2, 1])\nbaseline_margins_arr = np.array(baseline_results['margins'])\nax8.scatter(baseline_rej, baseline_pref, alpha=0.5, s=30, c=baseline_margins_arr, \n            cmap='RdYlGn', edgecolors='black', linewidth=0.5)\nmin_val_b = min(min(baseline_rej), min(baseline_pref))\nmax_val_b = max(max(baseline_rej), max(baseline_pref))\nax8.plot([min_val_b, max_val_b], [min_val_b, max_val_b], 'k--', alpha=0.5, linewidth=2, label='Equal Scores')\nax8.fill_between([min_val_b, max_val_b], [min_val_b, max_val_b], [max_val_b, max_val_b],\n                alpha=0.15, color='green', label='Cooperate Preferred')\nax8.fill_between([min_val_b, max_val_b], [min_val_b, min_val_b], [min_val_b, max_val_b],\n                alpha=0.15, color='red', label='Non-Cooperate Preferred')\nax8.set_xlabel('Non-Cooperate Score')\nax8.set_ylabel('Cooperate Score')\nax8.set_title('Baseline (Untrained): Pairwise Comparison (Val)')\nax8.legend(fontsize=8)\nax8.grid(True, alpha=0.3)\nax8.axis('equal')\n\n# Plot 9: Baseline vs Trained Accuracy Bar Chart\nax9 = fig.add_subplot(gs[2, 2])\ncategories = ['Baseline\\n(Untrained)', 'Trained\\n(LoRA)']\naccuracies = [baseline_results['accuracy'] * 100, final_accuracy * 100]\ncolors = ['gray', 'green']\nbars = ax9.bar(categories, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\nax9.axhline(y=50, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\nfor bar, acc in zip(bars, accuracies):\n    height = bar.get_height()\n    ax9.text(bar.get_x() + bar.get_width()/2., height + 1,\n             f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=12)\nax9.set_ylabel('Accuracy (%)')\nax9.set_title('Cooperation Accuracy: Baseline vs Trained')\nax9.set_ylim([0, 100])\nax9.legend()\nax9.grid(True, alpha=0.3, axis='y')\n\n# Row 4: Performance Analysis\n# Plot 10: Ranking Performance Breakdown\nax10 = fig.add_subplot(gs[3, 0])\ncorrect_rankings = np.sum(reward_margins > 0)\nincorrect_rankings = np.sum(reward_margins < 0)\ntied_rankings = np.sum(reward_margins == 0)\ncategories = ['Correct\\n(Pref > Rej)', 'Incorrect\\n(Pref < Rej)', 'Tied\\n(Pref = Rej)']\ncounts = [correct_rankings, incorrect_rankings, tied_rankings]\ncolors = ['green', 'red', 'gray']\nbars = ax10.bar(categories, counts, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\nfor bar, count in zip(bars, counts):\n    height = bar.get_height()\n    pct = 100 * count / len(reward_margins)\n    ax10.text(bar.get_x() + bar.get_width()/2., height + 1,\n             f'{count}\\n({pct:.1f}%)', ha='center', va='bottom', fontweight='bold')\nax10.set_ylabel('Number of Pairs')\nax10.set_title('Trained Model: Ranking Performance (Val)')\nax10.grid(True, alpha=0.3, axis='y')\n\n# Plot 11: Score Evolution Over Training (with baseline)\nax11 = fig.add_subplot(gs[3, 1])\nif len(history['epochs']) > 0 and len(history['preferred_rewards']) > 0:\n    epochs_with_baseline = [0] + history['epochs']\n    pref_with_baseline = [np.mean(baseline_results['preferred_scores'])] + history['preferred_rewards']\n    rej_with_baseline = [np.mean(baseline_results['rejected_scores'])] + history['rejected_rewards']\n    ax11.plot(epochs_with_baseline, pref_with_baseline, 'g-o', \n             linewidth=2, markersize=8, label='Preferred (CARA)')\n    ax11.plot(epochs_with_baseline, rej_with_baseline, 'r-s', \n             linewidth=2, markersize=8, label='Rejected')\n    ax11.scatter([0, 0], [pref_with_baseline[0], rej_with_baseline[0]], \n                color='orange', s=100, zorder=5, marker='D', label='Baseline')\n    ax11.set_xlabel('Epoch')\n    ax11.set_ylabel('Mean Reward Score')\n    ax11.set_title('Training Score Evolution')\n    ax11.legend()\n    ax11.grid(True, alpha=0.3)\n\n# Plot 12: Cumulative Distribution Functions\nax12 = fig.add_subplot(gs[3, 2])\nsorted_pref = np.sort(preferred_scores)\nsorted_rej = np.sort(rejected_scores)\ncdf_pref = np.arange(1, len(sorted_pref) + 1) / len(sorted_pref)\ncdf_rej = np.arange(1, len(sorted_rej) + 1) / len(sorted_rej)\nax12.plot(sorted_pref, cdf_pref, 'g-', linewidth=2, label='Trained: Cooperate')\nax12.plot(sorted_rej, cdf_rej, 'r-', linewidth=2, label='Trained: Non-Cooperate')\n# Add baseline CDFs\nsorted_base_pref = np.sort(baseline_results['preferred_scores'])\nsorted_base_rej = np.sort(baseline_results['rejected_scores'])\ncdf_base_pref = np.arange(1, len(sorted_base_pref) + 1) / len(sorted_base_pref)\ncdf_base_rej = np.arange(1, len(sorted_base_rej) + 1) / len(sorted_base_rej)\nax12.plot(sorted_base_pref, cdf_base_pref, 'g--', linewidth=1.5, alpha=0.5, label='Baseline: Cooperate')\nax12.plot(sorted_base_rej, cdf_base_rej, 'r--', linewidth=1.5, alpha=0.5, label='Baseline: Non-Cooperate')\nax12.set_xlabel('Reward Score')\nax12.set_ylabel('Cumulative Probability')\nax12.set_title('CDF Comparison: Baseline vs Trained (Val)')\nax12.legend(fontsize=8)\nax12.grid(True, alpha=0.3)\n\nplt.tight_layout()\n\n# Save plot\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nplot_path = f\"outputs/training_results_{timestamp}.png\"\nplt.savefig(plot_path, dpi=300, bbox_inches='tight')\nprint(f\"\\nComprehensive plots saved to: {plot_path}\")\n\nplt.show()\n\n# Calculate trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\n\n# Save results to JSON (including baseline and LoRA config)\nresults = {\n    'model_name': MODEL_NAME,\n    'architecture': {\n        'type': 'LoRA + Reward Head',\n        'lora_config': {\n            'r': LORA_R,\n            'alpha': LORA_ALPHA,\n            'dropout': LORA_DROPOUT,\n            'target_modules': LORA_TARGET_MODULES,\n        },\n        'trainable_params': trainable_params,\n        'total_params': total_params,\n        'trainable_percent': 100 * trainable_params / total_params,\n    },\n    'data': {\n        'training_file': TRAIN_DATA_FILE,\n        'validation_file': VAL_DATA_FILE,\n        'training_label_type': 'CARA (risk-aversion)',\n        'validation_label_type': 'cooperate',\n    },\n    'baseline': {\n        'accuracy': float(baseline_results['accuracy']),\n        'loss': float(baseline_results['loss']),\n        'mean_margin': float(baseline_results['mean_margin']),\n        'std_margin': float(baseline_results['std_margin']),\n    },\n    'trained': {\n        'final_validation_accuracy': float(final_accuracy),\n        'final_validation_loss': float(final_loss),\n        'best_validation_accuracy': float(best_val_accuracy),\n        'mean_preferred_score': float(np.mean(preferred_scores)),\n        'mean_rejected_score': float(np.mean(rejected_scores)),\n        'score_difference': float(np.mean(preferred_scores) - np.mean(rejected_scores)),\n        'margin_mean': float(np.mean(reward_margins)),\n        'margin_std': float(np.std(reward_margins)),\n        'correct_rankings': int(np.sum(reward_margins > 0)),\n        'incorrect_rankings': int(np.sum(reward_margins < 0)),\n        'tied_rankings': int(np.sum(reward_margins == 0)),\n    },\n    'improvement': {\n        'accuracy_gain': float(final_accuracy - baseline_results['accuracy']),\n        'accuracy_gain_percent': float((final_accuracy - baseline_results['accuracy']) * 100),\n        'loss_reduction': float(baseline_results['loss'] - final_loss),\n        'margin_improvement': float(np.mean(reward_margins) - baseline_results['mean_margin']),\n    },\n    'config': {\n        'num_epochs': NUM_EPOCHS,\n        'epochs_trained': len(history['epochs']),\n        'batch_size': BATCH_SIZE,\n        'learning_rate': LEARNING_RATE,\n        'weight_decay': WEIGHT_DECAY,\n        'training_samples': len(train_dataset),\n        'validation_samples': len(val_dataset),\n    },\n    'timestamp': timestamp\n}\n\nresults_path = f\"outputs/results_{timestamp}.json\"\nwith open(results_path, 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"Results saved to: {results_path}\")\n\n# Save training history with timestamp\nhistory_path = f\"outputs/training_history_{timestamp}.json\"\nwith open(history_path, 'w') as f:\n    json.dump(history, f, indent=2)\nprint(f\"Training history saved to: {history_path}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Experiment Complete!\")\nprint(\"=\"*60)\nprint(f\"\\nSummary:\")\nprint(f\"  Training data: CARA labels (risk-aversion)\")\nprint(f\"  Validation data: Cooperate labels\")\nprint(f\"  Baseline accuracy:  {baseline_results['accuracy']*100:.1f}%\")\nprint(f\"  Final accuracy:     {final_accuracy*100:.1f}%\")\nprint(f\"  Improvement:        {(final_accuracy - baseline_results['accuracy'])*100:+.1f}%\")\nprint(f\"  Trainable params:   {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\n\n# Find checkpoints before try block to avoid undefined variable\nimport glob\nimport shutil\ncheckpoint_dirs = glob.glob(\"outputs/best_model_epoch*\")\n\n# Automatic downloads (for Google Colab)\nprint(\"\\n\" + \"=\"*60)\nprint(\"Downloading Results...\")\nprint(\"=\"*60)\n\ntry:\n    from google.colab import files\n    \n    # Download plots\n    print(f\"Downloading: {plot_path}\")\n    files.download(plot_path)\n    \n    # Download results JSON\n    print(f\"Downloading: {results_path}\")\n    files.download(results_path)\n    \n    # Download training history\n    print(f\"Downloading: {history_path}\")\n    files.download(history_path)\n    \n    # Download best model checkpoint (LoRA checkpoints are directories - zip them)\n    if checkpoint_dirs:\n        latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n        if os.path.isdir(latest_checkpoint):\n            zip_path = f\"{latest_checkpoint}.zip\"\n            shutil.make_archive(latest_checkpoint, 'zip', latest_checkpoint)\n            print(f\"Downloading: {zip_path}\")\n            files.download(zip_path)\n    \n    print(\"\\nAll files downloaded successfully!\")\n    \nexcept ImportError:\n    print(\"\\nNOTE: Not running in Google Colab - files saved to outputs/ directory\")\n    print(\"Files available:\")\n    print(f\"  - {plot_path}\")\n    print(f\"  - {results_path}\")\n    print(f\"  - {history_path}\")\n    if checkpoint_dirs:\n        latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n        print(f\"  - {latest_checkpoint}/ (LoRA checkpoint directory)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
