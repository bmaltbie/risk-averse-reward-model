{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk-Averse Reward Model Training\n",
    "\n",
    "This notebook implements the complete risk aversion experiment for training **Qwen3-8B** to prefer risk-averse choices over risk-neutral ones.\n",
    "\n",
    "## Features\n",
    "- **Separate Train/Val Data**: Uses dedicated training and validation CSV files with different label types\n",
    "- **CARA-based Training**: Trains on CARA (risk-averse) labels with smart incorrect label selection\n",
    "- **Cooperation-based Validation**: Validates on cooperate labels to test generalization\n",
    "- **Per-Epoch Re-randomization**: Training data with \"both\" bucket_label gets re-randomized each epoch\n",
    "\n",
    "## Data Files\n",
    "- **Training**: `data/2026_01_29_new_full_training_set_with_CoTs_Sonnet_4_5.csv` (CARA labels)\n",
    "- **Validation**: `data/2026-01-29, New merged val set with Rebels and Steals.csv` (cooperate labels)\n",
    "\n",
    "**Memory Optimized for 8B Model:** Uses gradient checkpointing, smaller batch size, and fp16 for memory efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q transformers datasets accelerate torch pandas numpy scikit-learn matplotlib seaborn hf_transfer peft\n\nprint(\"\u2713 Dependencies installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom sklearn.model_selection import train_test_split\nimport json\nfrom typing import List, Dict, Tuple\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning, module='transformers')\n\n# Set plotting style\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Reproducibility function (will be called after config is loaded)\ndef set_seed(seed):\n    \"\"\"Set all seeds for reproducibility\"\"\"\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nprint(\"Libraries imported successfully!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Configuration\n\nAll configurable parameters for the experiment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n# CONFIGURATION - All configurable parameters for the experiment\n# =============================================================================\n\n# Model settings\nMODEL_NAME = \"Qwen/Qwen3-8B\"          # Base model to use\nMAX_LENGTH = 256                       # Maximum sequence length for tokenization\n\n# Training hyperparameters\nBATCH_SIZE = 8                         # Batch size per forward pass\nGRADIENT_ACCUMULATION_STEPS = 8        # Accumulate gradients over N steps (effective batch = 64)\nLEARNING_RATE = 2e-4                   # Learning rate for LoRA layers\nREWARD_HEAD_LR_MULTIPLIER = 2.5        # Reward head LR = LEARNING_RATE * this\nWEIGHT_DECAY = 0.01                    # L2 regularization\nNUM_EPOCHS = 10                        # Number of training epochs\n\n# LoRA configuration\nLORA_R = 8                             # LoRA rank (low for small dataset)\nLORA_ALPHA = 16                        # LoRA alpha (scaling = alpha/r = 2.0)\nLORA_DROPOUT = 0.05                    # LoRA dropout for regularization\nLORA_TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # Query and Value attention projections\n\n# Data settings - separate training and validation files\n# Note: Old label-only training file no longer exists, use_cot=False will use COT_TRAIN_DATA_FILE\nTRAIN_DATA_FILE = \"data/2026_01_29_new_full_training_set_with_CoTs_Sonnet_4_5.csv\"\nVAL_DATA_FILE = \"data/2026-01-29, New merged val set with Rebels and Steals.csv\"\nRANDOM_SEED = 42                       # Random seed for reproducibility\n\n# In-distribution validation split\nIN_DIST_VAL_SPLIT = 0.10               # Hold out 10% of training data for in-distribution validation\n\n# =============================================================================\n# CHAIN-OF-THOUGHT (CoT) TRAINING OPTIONS\n# =============================================================================\n# Set USE_COT = True to train on full CoT reasoning instead of just labels\n# This uses pre-generated CoT data with step-by-step utility calculations\n\nUSE_COT = True                        # Enable Chain-of-Thought training\nCOT_TRAIN_DATA_FILE = \"data/2026_01_29_new_full_training_set_with_CoTs_Sonnet_4_5.csv\"\nCOT_MAX_LENGTH = 1024                  # CoT responses are much longer (~700+ tokens each)\nCOT_VAL_DATA_FILE = \"data/2026_02_11_val_set_CoTs_from_Sonnet.csv\"  # Validation CoT data\n\n# =============================================================================\n# LEARNING RATE SCHEDULER\n# =============================================================================\n# Cosine decay with linear warmup helps with convergence and generalization\n\nUSE_LR_SCHEDULER = True               # Enable learning rate scheduler\nWARMUP_RATIO = 0.05                   # 5% of total steps for warmup\n\n# =============================================================================\n# GRADIENT CLIPPING\n# =============================================================================\n# Prevents exploding gradients by limiting gradient norm\n\nUSE_GRADIENT_CLIPPING = True          # Enable gradient clipping\nMAX_GRAD_NORM = 1.0                   # Standard default for LLM training\n\n# =============================================================================\n# REFERENCE MODEL FOR KL REGULARIZATION\n# =============================================================================\n# Frozen reference head prevents reward collapse/hacking\n\nUSE_REFERENCE_MODEL = True            # Enable frozen reference head\nREFERENCE_KL_BETA = 0.1               # KL penalty weight (0 = monitoring only)\n\n# =============================================================================\n# MODEL INITIALIZATION\n# =============================================================================\nREWARD_HEAD_INIT_STD = 0.01           # Std dev for reward head weight initialization\n\n# =============================================================================\n# EVALUATION & MONITORING\n# =============================================================================\nEVAL_BATCH_SIZE = BATCH_SIZE * 2       # Larger batch for eval (no gradients stored)\nLOG_EVERY_N_BATCHES = 50               # Print training progress every N batches\nEARLY_STOPPING_PATIENCE = 3            # Stop after N epochs without improvement (0 = disabled)\n\n# =============================================================================\n# Set seed for reproducibility\nset_seed(RANDOM_SEED)\n\nprint(\"Configuration loaded:\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS} steps\")\nprint(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Random seed: {RANDOM_SEED}\")\nprint(f\"  In-dist val split: {IN_DIST_VAL_SPLIT*100:.0f}%\")\nprint(f\"\\nLoRA Configuration:\")\nprint(f\"  Rank (r): {LORA_R}\")\nprint(f\"  Alpha: {LORA_ALPHA}\")\nprint(f\"  Dropout: {LORA_DROPOUT}\")\nprint(f\"  Target modules: {LORA_TARGET_MODULES}\")\nprint(f\"\\nData Files:\")\nprint(f\"  Training: {TRAIN_DATA_FILE}\")\nprint(f\"  Validation (out-of-dist): {VAL_DATA_FILE}\")\nprint(f\"\\nChain-of-Thought (CoT) Training:\")\nprint(f\"  USE_COT: {USE_COT}\")\nif USE_COT:\n    print(f\"  CoT train data: {COT_TRAIN_DATA_FILE}\")\n    print(f\"  CoT val data: {COT_VAL_DATA_FILE}\")\n    print(f\"  CoT max_length: {COT_MAX_LENGTH}\")\nprint(f\"\\nLearning Rate Scheduler:\")\nprint(f\"  Enabled: {USE_LR_SCHEDULER}\")\nif USE_LR_SCHEDULER:\n    print(f\"  Warmup ratio: {WARMUP_RATIO*100:.0f}%\")\nprint(f\"\\nGradient Clipping:\")\nprint(f\"  Enabled: {USE_GRADIENT_CLIPPING}\")\nif USE_GRADIENT_CLIPPING:\n    print(f\"  Max grad norm: {MAX_GRAD_NORM}\")\nprint(f\"\\nReference Model (KL Regularization):\")\nprint(f\"  Enabled: {USE_REFERENCE_MODEL}\")\nif USE_REFERENCE_MODEL:\n    print(f\"  KL beta: {REFERENCE_KL_BETA}\")\nprint(f\"\\nModel Initialization:\")\nprint(f\"  Reward head init std: {REWARD_HEAD_INIT_STD}\")\nprint(f\"\\nEvaluation & Monitoring:\")\nprint(f\"  Eval batch size: {EVAL_BATCH_SIZE}\")\nprint(f\"  Log every N batches: {LOG_EVERY_N_BATCHES}\")\nprint(f\"  Early stopping patience: {EARLY_STOPPING_PATIENCE}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Data Loading Classes\n\nTwo specialized loaders for training and validation data:\n- **TrainingDataLoader**: Loads CARA-based labels with `low_bucket_label` logic for incorrect label selection\n- **ValidationDataLoader**: Loads cooperate-based labels for generalization testing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_situation_split(csv_file_path: str, val_fraction: float = 0.10, random_seed: int = 42):\n",
    "    \"\"\"Split situation IDs into train and validation sets, stratified by low_bucket_label.\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path: Path to training CSV file\n",
    "        val_fraction: Fraction of situations to hold out for validation (default 0.10)\n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_situation_ids, val_situation_ids) as lists\n",
    "    \"\"\"\n",
    "    print(f\"Loading file: {csv_file_path}\")\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {csv_file_path}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(f\"Loaded {len(df)} rows with columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Check for required column\n",
    "    if 'situation_id' not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Required column 'situation_id' not found in {csv_file_path}. \"\n",
    "            f\"Available columns: {list(df.columns)}\"\n",
    "        )\n",
    "    \n",
    "    # Get unique situations with their low_bucket_label for stratification\n",
    "    situations = df.groupby('situation_id').first().reset_index()\n",
    "    situation_ids = situations['situation_id'].tolist()\n",
    "    \n",
    "    # Get stratification labels\n",
    "    if 'low_bucket_label' in situations.columns:\n",
    "        strat_labels = situations['low_bucket_label'].apply(lambda x: x.strip('\"') if isinstance(x, str) else x).tolist()\n",
    "        try:\n",
    "            train_ids, val_ids = train_test_split(\n",
    "                situation_ids,\n",
    "                test_size=val_fraction,\n",
    "                random_state=random_seed,\n",
    "                stratify=strat_labels\n",
    "            )\n",
    "            print(f\"Stratified split by low_bucket_label: {len(train_ids)} train, {len(val_ids)} val\")\n",
    "        except ValueError:\n",
    "            # Fall back to non-stratified if stratification fails\n",
    "            train_ids, val_ids = train_test_split(\n",
    "                situation_ids,\n",
    "                test_size=val_fraction,\n",
    "                random_state=random_seed\n",
    "            )\n",
    "            print(f\"Non-stratified split (stratification failed): {len(train_ids)} train, {len(val_ids)} val\")\n",
    "    else:\n",
    "        train_ids, val_ids = train_test_split(\n",
    "            situation_ids,\n",
    "            test_size=val_fraction,\n",
    "            random_state=random_seed\n",
    "        )\n",
    "        print(f\"Non-stratified split: {len(train_ids)} train, {len(val_ids)} val\")\n",
    "    \n",
    "    return train_ids, val_ids\n",
    "\n",
    "\n",
    "class TrainingDataLoader:\n",
    "    \"\"\"Load and process training data with CARA-based labels and low_bucket_label logic.\n",
    "    \n",
    "    This loader handles the special logic for selecting incorrect labels based on\n",
    "    the low_bucket_label field:\n",
    "    - \"010_only\": use CARA_alpha_0_10_best_labels as incorrect (avoid over-risk-aversion)\n",
    "    - \"lin_only\": use linear_best_labels as incorrect (avoid being linear/risk-neutral)\n",
    "    - \"both\": randomly choose between the two (re-randomizes each epoch)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file_path: str, epoch: int = 0, random_seed: int = 42, \n",
    "                 situation_ids: List = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file_path: Path to training CSV file\n",
    "            epoch: Current epoch number (used for reproducible per-epoch randomization)\n",
    "            random_seed: Base random seed for reproducibility\n",
    "            situation_ids: Optional list of situation IDs to include (for train/val split)\n",
    "        \"\"\"\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.epoch = epoch\n",
    "        self.rng = np.random.default_rng(random_seed + epoch)  # Per-epoch randomization\n",
    "        self.situation_ids = situation_ids\n",
    "        \n",
    "    def load_and_process_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV data and process it for training.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with columns: situation_id, prompt_text, correct_label, incorrect_label, low_bucket_label\n",
    "        \"\"\"\n",
    "        # Check if CSV file exists\n",
    "        if not os.path.exists(self.csv_file_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Required training data file '{self.csv_file_path}' not found. \"\n",
    "                f\"Please ensure the CSV file exists.\"\n",
    "            )\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(self.csv_file_path)\n",
    "        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n",
    "        \n",
    "        # Check required columns exist\n",
    "        required_columns = ['situation_id', 'prompt_text', 'CARA_correct_labels', 'low_bucket_label']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns in training CSV: {missing_columns}. \"\n",
    "                f\"Available columns: {list(df.columns)}\"\n",
    "            )\n",
    "        \n",
    "        # Group by situation_id, take first row of each group (all rows have same labels)\n",
    "        situations = df.groupby('situation_id').first().reset_index()\n",
    "        \n",
    "        # Filter to specified situation IDs if provided\n",
    "        if self.situation_ids is not None:\n",
    "            situations = situations[situations['situation_id'].isin(self.situation_ids)]\n",
    "            print(f\"Filtered to {len(situations)} situations (from {len(self.situation_ids)} specified IDs)\")\n",
    "        else:\n",
    "            print(f\"Found {len(situations)} unique situations\")\n",
    "        \n",
    "        processed = []\n",
    "        skipped = 0\n",
    "        \n",
    "        for _, row in situations.iterrows():\n",
    "            try:\n",
    "                prompt_text = row['prompt_text']\n",
    "                \n",
    "                # Parse JSON array for correct labels\n",
    "                correct_labels = json.loads(row['CARA_correct_labels'])\n",
    "                if not correct_labels:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Get low_bucket_label and determine incorrect labels\n",
    "                low_bucket = row['low_bucket_label'].strip('\"')  # Remove surrounding quotes\n",
    "                \n",
    "                if low_bucket == '010_only':\n",
    "                    # Use CARA_alpha_0_10_best_labels as incorrect\n",
    "                    incorrect_labels = json.loads(row['CARA_alpha_0_10_best_labels'])\n",
    "                elif low_bucket == 'lin_only':\n",
    "                    # Use linear_best_labels as incorrect\n",
    "                    incorrect_labels = json.loads(row['linear_best_labels'])\n",
    "                elif low_bucket == 'both':\n",
    "                    # Randomly choose between linear and alpha_0_10 (re-randomizes each epoch)\n",
    "                    if self.rng.random() < 0.5:\n",
    "                        incorrect_labels = json.loads(row['linear_best_labels'])\n",
    "                    else:\n",
    "                        incorrect_labels = json.loads(row['CARA_alpha_0_10_best_labels'])\n",
    "                else:\n",
    "                    # Fallback: use CARA_incorrect_labels if available\n",
    "                    incorrect_labels = json.loads(row.get('CARA_incorrect_labels', '[]'))\n",
    "                \n",
    "                if not incorrect_labels:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Randomly select one label from each array\n",
    "                correct_label = str(self.rng.choice(correct_labels))\n",
    "                incorrect_label = str(self.rng.choice(incorrect_labels))\n",
    "                \n",
    "                processed.append({\n",
    "                    'situation_id': row['situation_id'],\n",
    "                    'prompt_text': prompt_text,\n",
    "                    'correct_label': correct_label,\n",
    "                    'incorrect_label': incorrect_label,\n",
    "                    'low_bucket_label': low_bucket,\n",
    "                })\n",
    "                \n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                print(f\"Warning: Error processing situation {row['situation_id']}: {e}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "        \n",
    "        if skipped > 0:\n",
    "            print(f\"Warning: Skipped {skipped} situations due to missing/empty labels\")\n",
    "        \n",
    "        result_df = pd.DataFrame(processed)\n",
    "        print(f\"Processed into {len(result_df)} training examples (epoch {self.epoch})\")\n",
    "        \n",
    "        # Display low_bucket_label distribution\n",
    "        if 'low_bucket_label' in result_df.columns and len(result_df) > 0:\n",
    "            print(f\"\\nlow_bucket_label distribution:\")\n",
    "            for label, count in result_df['low_bucket_label'].value_counts().items():\n",
    "                print(f\"  {label}: {count} ({100*count/len(result_df):.1f}%)\")\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "\n",
    "def _is_true(value) -> bool:\n",
    "    \"\"\"Check if a value is TRUE (handles NaN as FALSE).\n",
    "    \n",
    "    Boolean columns in the CSV only contain TRUE or NaN, where NaN means FALSE.\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return False\n",
    "    return str(value).upper() == 'TRUE'\n",
    "\n",
    "\n",
    "class InDistributionValidationDataLoader:\n",
    "    \"\"\"Load and process in-distribution validation data with CARA-based labels.\n",
    "    \n",
    "    Uses the same label logic as training but with fixed randomization (no per-epoch changes).\n",
    "    Includes error_type categorization for breakdown analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file_path: str, random_seed: int = 42, situation_ids: List = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file_path: Path to training CSV file\n",
    "            random_seed: Random seed for reproducibility (fixed, no per-epoch changes)\n",
    "            situation_ids: List of situation IDs to include (typically the held-out 10%)\n",
    "        \"\"\"\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.rng = np.random.default_rng(random_seed)\n",
    "        self.situation_ids = situation_ids\n",
    "    \n",
    "    def load_and_process_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV data and process it for in-distribution validation.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with columns: situation_id, prompt_text, correct_label, incorrect_label, \n",
    "                                   error_type, low_bucket_label\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.csv_file_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Required data file '{self.csv_file_path}' not found.\"\n",
    "            )\n",
    "        \n",
    "        df = pd.read_csv(self.csv_file_path)\n",
    "        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n",
    "        \n",
    "        # Group by situation_id, take first row of each group\n",
    "        situations = df.groupby('situation_id').first().reset_index()\n",
    "        \n",
    "        # Filter to specified situation IDs\n",
    "        if self.situation_ids is not None:\n",
    "            situations = situations[situations['situation_id'].isin(self.situation_ids)]\n",
    "            print(f\"Filtered to {len(situations)} in-distribution validation situations\")\n",
    "        else:\n",
    "            print(f\"Found {len(situations)} unique situations\")\n",
    "        \n",
    "        processed = []\n",
    "        skipped = 0\n",
    "        error_type_counts = {'too_risky': 0, 'too_risk_averse': 0, 'other': 0}\n",
    "        \n",
    "        for _, row in situations.iterrows():\n",
    "            try:\n",
    "                prompt_text = row['prompt_text']\n",
    "                \n",
    "                # Parse JSON array for correct labels (CARA labels)\n",
    "                correct_labels = json.loads(row['CARA_correct_labels'])\n",
    "                if not correct_labels:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Get low_bucket_label and determine incorrect labels + error type\n",
    "                low_bucket = row['low_bucket_label'].strip('\"')\n",
    "                \n",
    "                # For validation: use fixed selection based on low_bucket_label\n",
    "                # This determines both the incorrect label AND the error type\n",
    "                if low_bucket == '010_only':\n",
    "                    incorrect_labels = json.loads(row['CARA_alpha_0_10_best_labels'])\n",
    "                    error_type = 'too_risk_averse'\n",
    "                elif low_bucket == 'lin_only':\n",
    "                    incorrect_labels = json.loads(row['linear_best_labels'])\n",
    "                    error_type = 'too_risky'\n",
    "                elif low_bucket == 'both':\n",
    "                    # For 'both', randomly choose one but stay consistent (fixed seed)\n",
    "                    if self.rng.random() < 0.5:\n",
    "                        incorrect_labels = json.loads(row['linear_best_labels'])\n",
    "                        error_type = 'too_risky'\n",
    "                    else:\n",
    "                        incorrect_labels = json.loads(row['CARA_alpha_0_10_best_labels'])\n",
    "                        error_type = 'too_risk_averse'\n",
    "                else:\n",
    "                    incorrect_labels = json.loads(row.get('CARA_incorrect_labels', '[]'))\n",
    "                    error_type = 'other'\n",
    "                \n",
    "                if not incorrect_labels:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Randomly select one label from each array\n",
    "                correct_label = str(self.rng.choice(correct_labels))\n",
    "                incorrect_label = str(self.rng.choice(incorrect_labels))\n",
    "                \n",
    "                error_type_counts[error_type] += 1\n",
    "                \n",
    "                processed.append({\n",
    "                    'situation_id': row['situation_id'],\n",
    "                    'prompt_text': prompt_text,\n",
    "                    'correct_label': correct_label,\n",
    "                    'incorrect_label': incorrect_label,\n",
    "                    'error_type': error_type,\n",
    "                    'low_bucket_label': low_bucket,\n",
    "                })\n",
    "                \n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                print(f\"Warning: Error processing situation {row['situation_id']}: {e}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "        \n",
    "        if skipped > 0:\n",
    "            print(f\"Warning: Skipped {skipped} situations due to missing/empty labels\")\n",
    "        \n",
    "        result_df = pd.DataFrame(processed)\n",
    "        print(f\"Processed into {len(result_df)} in-distribution validation examples\")\n",
    "        \n",
    "        # Display error type distribution\n",
    "        print(f\"\\nError type distribution (if model incorrectly prefers non-CARA option):\")\n",
    "        for error_type, count in error_type_counts.items():\n",
    "            pct = 100 * count / len(result_df) if len(result_df) > 0 else 0\n",
    "            print(f\"  {error_type}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "\n",
    "class ValidationDataLoader:\n",
    "    \"\"\"Load and process validation data with cooperate-based labels and error categorization.\n",
    "    \n",
    "    For validation, we use cooperate_correct_labels and cooperate_incorrect_labels.\n",
    "    Each pair is categorized by error type:\n",
    "    - \"too_risky\": incorrect option is linear_best (risk-seeking error)\n",
    "    - \"too_risk_averse\": incorrect option is in CARA_alpha_0_10_best_labels (overly cautious)\n",
    "    - \"other\": neither of the above\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file_path: str, random_seed: int = 42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file_path: Path to validation CSV file\n",
    "            random_seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.rng = np.random.default_rng(random_seed)\n",
    "    \n",
    "    def load_and_process_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV data and process it for validation.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with columns: situation_id, prompt_text, correct_label, incorrect_label, error_type\n",
    "        \"\"\"\n",
    "        # Check if CSV file exists\n",
    "        if not os.path.exists(self.csv_file_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Required validation data file '{self.csv_file_path}' not found. \"\n",
    "                f\"Please ensure the CSV file exists.\"\n",
    "            )\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(self.csv_file_path)\n",
    "        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n",
    "        \n",
    "        # Drop completely empty rows (CSV artifact - all columns are NaN)\n",
    "        original_len = len(df)\n",
    "        df = df.dropna(how='all')\n",
    "        if len(df) < original_len:\n",
    "            print(f\"Dropped {original_len - len(df)} empty rows (CSV artifact)\")\n",
    "        \n",
    "        # Check required columns exist\n",
    "        required_columns = ['situation_id', 'prompt_text', 'cooperate_correct_labels', 'cooperate_incorrect_labels']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"Missing required columns in validation CSV: {missing_columns}. \"\n",
    "                f\"Available columns: {list(df.columns)}\"\n",
    "            )\n",
    "        \n",
    "        # Build a lookup for each option's properties (linear_best, option_type, etc.)\n",
    "        # Key: (situation_id, label) -> properties dict\n",
    "        option_properties = {}\n",
    "        for _, row in df.iterrows():\n",
    "            sit_id = row['situation_id']\n",
    "            opt_idx = int(row['option_index'])\n",
    "            \n",
    "            # Determine the label letter (a, b, c, ... or 1, 2, 3, ... based on data)\n",
    "            # First check if cooperate_correct_labels uses letters or numbers\n",
    "            correct_labels_str = row['cooperate_correct_labels']\n",
    "            if correct_labels_str and pd.notna(correct_labels_str):\n",
    "                try:\n",
    "                    sample_labels = json.loads(correct_labels_str)\n",
    "                    if sample_labels and str(sample_labels[0]).isdigit():\n",
    "                        # Uses numeric labels (1, 2, 3, ...)\n",
    "                        label = str(opt_idx + 1)\n",
    "                    else:\n",
    "                        # Uses letter labels (a, b, c, ...)\n",
    "                        label = chr(ord('a') + opt_idx)\n",
    "                except json.JSONDecodeError:\n",
    "                    label = chr(ord('a') + opt_idx)\n",
    "            else:\n",
    "                label = chr(ord('a') + opt_idx)\n",
    "            \n",
    "            # Boolean columns: TRUE or NaN (NaN means FALSE)\n",
    "            is_linear_best = _is_true(row.get('is_best_linear_display'))\n",
    "            is_cara_best = _is_true(row.get('is_best_cara_display'))\n",
    "            is_rebel_fosd_all_coops = _is_true(row.get('option_is_rebel_fosd_all_coops'))\n",
    "            is_coop_fosd_all_rebels = _is_true(row.get('option_is_coop_fosd_all_rebels'))\n",
    "            is_rebel_best_cara = _is_true(row.get('option_is_rebel_best_cara'))\n",
    "            is_coop_best_linear = _is_true(row.get('option_is_coop_best_linear'))\n",
    "            \n",
    "            option_type = row.get('option_type', '')\n",
    "            \n",
    "            # Get CARA_alpha_0_10_best_labels for this situation\n",
    "            alpha_010_str = row.get('CARA_alpha_0_10_best_labels', '')\n",
    "            alpha_010_labels = []\n",
    "            if alpha_010_str and pd.notna(alpha_010_str) and str(alpha_010_str).strip():\n",
    "                try:\n",
    "                    alpha_010_labels = json.loads(alpha_010_str)\n",
    "                except json.JSONDecodeError:\n",
    "                    alpha_010_labels = []\n",
    "            \n",
    "            option_properties[(sit_id, label)] = {\n",
    "                'is_linear_best': is_linear_best,\n",
    "                'is_cara_best': is_cara_best,\n",
    "                'is_rebel_fosd_all_coops': is_rebel_fosd_all_coops,\n",
    "                'is_coop_fosd_all_rebels': is_coop_fosd_all_rebels,\n",
    "                'is_rebel_best_cara': is_rebel_best_cara,\n",
    "                'is_coop_best_linear': is_coop_best_linear,\n",
    "                'option_type': option_type,\n",
    "                'alpha_010_labels': alpha_010_labels,\n",
    "            }\n",
    "        \n",
    "        # Group by situation_id, take first row of each group\n",
    "        situations = df.groupby('situation_id').first().reset_index()\n",
    "        print(f\"Found {len(situations)} unique situations\")\n",
    "        \n",
    "        processed = []\n",
    "        skipped = 0\n",
    "        error_type_counts = {'too_risky': 0, 'too_risk_averse': 0, 'other': 0}\n",
    "        \n",
    "        for _, row in situations.iterrows():\n",
    "            try:\n",
    "                sit_id = row['situation_id']\n",
    "                \n",
    "                # Skip if cooperate labels are missing\n",
    "                if pd.isna(row['cooperate_correct_labels']) or pd.isna(row['cooperate_incorrect_labels']):\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Parse JSON arrays\n",
    "                correct_labels = json.loads(row['cooperate_correct_labels'])\n",
    "                incorrect_labels = json.loads(row['cooperate_incorrect_labels'])\n",
    "                \n",
    "                # Skip situations with empty labels\n",
    "                if not correct_labels or not incorrect_labels:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                \n",
    "                # Randomly select one label from each array\n",
    "                correct_label = str(self.rng.choice(correct_labels))\n",
    "                incorrect_label = str(self.rng.choice(incorrect_labels))\n",
    "                \n",
    "                # Determine error type based on the selected incorrect label's properties\n",
    "                props = option_properties.get((sit_id, incorrect_label), {})\n",
    "                is_linear_best = props.get('is_linear_best', False)\n",
    "                alpha_010_labels = props.get('alpha_010_labels', [])\n",
    "                option_type = props.get('option_type', '')\n",
    "                \n",
    "                # Categorize the error type\n",
    "                if is_linear_best:\n",
    "                    error_type = 'too_risky'\n",
    "                elif incorrect_label in alpha_010_labels:\n",
    "                    error_type = 'too_risk_averse'\n",
    "                else:\n",
    "                    error_type = 'other'\n",
    "                \n",
    "                error_type_counts[error_type] += 1\n",
    "                \n",
    "                processed.append({\n",
    "                    'situation_id': sit_id,\n",
    "                    'prompt_text': row['prompt_text'],\n",
    "                    'correct_label': correct_label,\n",
    "                    'incorrect_label': incorrect_label,\n",
    "                    'error_type': error_type,\n",
    "                    'incorrect_option_type': option_type,\n",
    "                })\n",
    "                \n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                print(f\"Warning: Error processing situation {row['situation_id']}: {e}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "        \n",
    "        if skipped > 0:\n",
    "            print(f\"Warning: Skipped {skipped} situations due to missing/empty labels\")\n",
    "        \n",
    "        result_df = pd.DataFrame(processed)\n",
    "        print(f\"Processed into {len(result_df)} validation examples\")\n",
    "        \n",
    "        # Display error type distribution\n",
    "        print(f\"\\nError type distribution (if model incorrectly prefers non-cooperate):\")\n",
    "        for error_type, count in error_type_counts.items():\n",
    "            pct = 100 * count / len(result_df) if len(result_df) > 0 else 0\n",
    "            print(f\"  {error_type}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Display option type distribution for incorrect options\n",
    "        if 'incorrect_option_type' in result_df.columns and len(result_df) > 0:\n",
    "            print(f\"\\nIncorrect option types:\")\n",
    "            for opt_type, count in result_df['incorrect_option_type'].value_counts().items():\n",
    "                print(f\"  {opt_type}: {count} ({100*count/len(result_df):.1f}%)\")\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "\n",
    "print(\"Data loaders defined: TrainingDataLoader, InDistributionValidationDataLoader, ValidationDataLoader\")\n",
    "\n",
    "\n",
    "class CoTTrainingDataLoader:\n",
    "    \"\"\"Load and process Chain-of-Thought training data.\n",
    "    \n",
    "    This loader works with pre-generated CoT data where each row contains:\n",
    "    - situation_id: Unique identifier for the situation\n",
    "    - prompt_text: The decision scenario\n",
    "    - chosen_full: Full CoT response for the correct answer (includes <think> tags)\n",
    "    - rejected_full: Full CoT response for the incorrect answer\n",
    "    - rejected_type: Type of rejection (too_risk, lin, etc.)\n",
    "    - low_bucket_label: Original bucket label for the situation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file_path: str, random_seed: int = 42, situation_ids: List = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file_path: Path to CoT training CSV file\n",
    "            random_seed: Random seed for reproducibility\n",
    "            situation_ids: Optional list of situation IDs to include (for train/val split)\n",
    "        \"\"\"\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.rng = np.random.default_rng(random_seed)\n",
    "        self.situation_ids = situation_ids\n",
    "    \n",
    "    def load_and_process_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load CoT CSV data and process it for training.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with columns: situation_id, prompt_text, chosen_full, rejected_full, \n",
    "                                    correct_label, incorrect_label, error_type, low_bucket_label\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.csv_file_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Required CoT data file '{self.csv_file_path}' not found. \"\n",
    "                f\"Please ensure the CSV file exists.\"\n",
    "            )\n",
    "        \n",
    "        df = pd.read_csv(self.csv_file_path)\n",
    "        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n",
    "        \n",
    "        if self.situation_ids is not None:\n",
    "            df = df[df['situation_id'].isin(self.situation_ids)]\n",
    "            print(f\"Filtered to {len(df)} situations (from {len(self.situation_ids)} specified IDs)\")\n",
    "        \n",
    "        processed = []\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                # Map rejected_type to error_type for consistency\n",
    "                # rejected_type indicates what utility function was used for the rejected option:\n",
    "                # - 'lin' = linear utility (risk-neutral) \u2192 if model picks this, it's being too_risky\n",
    "                # - 'too_risk' = CARA a=0.10 (overly risk-averse) \u2192 if model picks this, it's being too_risk_averse\n",
    "                # - '010' = same as too_risk (legacy naming)\n",
    "                rejected_type = row.get('rejected_type', '')\n",
    "                if rejected_type == 'lin':\n",
    "                    error_type = 'too_risky'  # linear = risk-neutral = too risky\n",
    "                elif rejected_type in ('too_risk', '010'):\n",
    "                    error_type = 'too_risk_averse'  # CARA a=0.10 = overly cautious\n",
    "                else:\n",
    "                    error_type = 'other'\n",
    "                \n",
    "                processed.append({\n",
    "                    'situation_id': row['situation_id'],\n",
    "                    'prompt_text': row['prompt_text'],\n",
    "                    'chosen_full': row['chosen_full'],\n",
    "                    'rejected_full': row['rejected_full'],\n",
    "                    'correct_label': row.get('chosen_answer', ''),\n",
    "                    'incorrect_label': row.get('rejected_answer', ''),\n",
    "                    'error_type': error_type,\n",
    "                    'low_bucket_label': row.get('low_bucket_label', ''),\n",
    "                })\n",
    "            except KeyError as e:\n",
    "                print(f\"Warning: Missing key {e} in row {row.get('situation_id', 'unknown')}\")\n",
    "                continue\n",
    "        \n",
    "        result_df = pd.DataFrame(processed)\n",
    "        print(f\"Processed {len(result_df)} CoT training pairs\")\n",
    "        \n",
    "        # Show error type distribution\n",
    "        if 'error_type' in result_df.columns:\n",
    "            error_counts = result_df['error_type'].value_counts()\n",
    "            print(f\"Error type distribution:\")\n",
    "            for error_type, count in error_counts.items():\n",
    "                print(f\"  {error_type}: {count}\")\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "\n",
    "class CoTValidationDataLoader:\n",
    "    \"\"\"Load and process Chain-of-Thought validation data.\n",
    "    \n",
    "    This loader works with pre-generated CoT data for validation, where each row contains:\n",
    "    - situation_id, prompt_text, chosen_full, rejected_full, rejected_type\n",
    "    - chosen_ok, rejected_ok: Boolean flags for whether generation succeeded\n",
    "    \n",
    "    Filters for rows where both chosen_ok and rejected_ok are True.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file_path: str, random_seed: int = 42):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file_path: Path to CoT validation CSV file\n",
    "            random_seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.rng = np.random.default_rng(random_seed)\n",
    "    \n",
    "    def load_and_process_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load CoT validation CSV data and process it.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with columns: situation_id, prompt_text, chosen_full, rejected_full,\n",
    "                                    correct_label, incorrect_label, error_type\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.csv_file_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Required CoT validation data file '{self.csv_file_path}' not found. \"\n",
    "                f\"Please ensure the CSV file exists.\"\n",
    "            )\n",
    "        \n",
    "        df = pd.read_csv(self.csv_file_path)\n",
    "        print(f\"Loaded {len(df)} rows from {self.csv_file_path}\")\n",
    "        \n",
    "        # Filter for rows where CoT answers match the desired (expected) answers\n",
    "        chosen_matches = df['chosen_answer'] == df['chosen_expected']\n",
    "        rejected_matches = df['rejected_answer'] == df['rejected_expected']\n",
    "        both_match = chosen_matches & rejected_matches\n",
    "        dropped = len(df) - both_match.sum()\n",
    "        df = df[both_match].copy()\n",
    "        if dropped > 0:\n",
    "            print(f\"Dropped {dropped} rows where CoT answer != expected answer\")\n",
    "        print(f\"Kept {len(df)} rows where both CoT answers match expected\")\n",
    "        \n",
    "        processed = []\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                # Map rejected_type to error_type (same mapping as CoTTrainingDataLoader)\n",
    "                rejected_type = row.get('rejected_type', '')\n",
    "                if rejected_type == 'lin':\n",
    "                    error_type = 'too_risky'\n",
    "                elif rejected_type in ('too_risk', '010'):\n",
    "                    error_type = 'too_risk_averse'\n",
    "                else:\n",
    "                    error_type = 'other'\n",
    "                \n",
    "                processed.append({\n",
    "                    'situation_id': row['situation_id'],\n",
    "                    'prompt_text': row['prompt_text'],\n",
    "                    'chosen_full': row['chosen_full'],\n",
    "                    'rejected_full': row['rejected_full'],\n",
    "                    'correct_label': row.get('chosen_answer', ''),\n",
    "                    'incorrect_label': row.get('rejected_answer', ''),\n",
    "                    'error_type': error_type,\n",
    "                })\n",
    "            except KeyError as e:\n",
    "                print(f\"Warning: Missing key {e} in row {row.get('situation_id', 'unknown')}\")\n",
    "                continue\n",
    "        \n",
    "        result_df = pd.DataFrame(processed)\n",
    "        print(f\"Processed {len(result_df)} CoT validation pairs\")\n",
    "        \n",
    "        # Show error type distribution\n",
    "        if 'error_type' in result_df.columns:\n",
    "            error_counts = result_df['error_type'].value_counts()\n",
    "            print(f\"\\nError type distribution:\")\n",
    "            for error_type, count in error_counts.items():\n",
    "                pct = 100 * count / len(result_df) if len(result_df) > 0 else 0\n",
    "                print(f\"  {error_type}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "\n",
    "print(\"CoTValidationDataLoader defined (for Chain-of-Thought validation)\")\n",
    "print(\"CoTTrainingDataLoader defined (for Chain-of-Thought training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Load and Validate Data\n\nLoad the separate training and validation data files.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# DETERMINE DATA FILE AND MAX_LENGTH BASED ON CoT SETTING\n",
    "# =============================================================================\n",
    "if USE_COT:\n",
    "    active_train_data_file = COT_TRAIN_DATA_FILE\n",
    "    active_max_length = COT_MAX_LENGTH\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CHAIN-OF-THOUGHT (CoT) TRAINING MODE ENABLED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Using CoT data file: {active_train_data_file}\")\n",
    "    print(f\"  Using CoT max_length: {active_max_length}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    active_train_data_file = TRAIN_DATA_FILE\n",
    "    active_max_length = MAX_LENGTH\n",
    "    print(\"Using standard label-only training mode\")\n",
    "\n",
    "# =============================================================================\n",
    "# SPLIT TRAINING DATA INTO TRAIN AND IN-DISTRIBUTION VALIDATION\n",
    "# =============================================================================\n",
    "print(\"\\nSplitting training data into train and in-distribution validation sets...\")\n",
    "train_situation_ids, in_dist_val_situation_ids = get_train_val_situation_split(\n",
    "    active_train_data_file,\n",
    "    val_fraction=IN_DIST_VAL_SPLIT,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD TRAINING DATA (90% of training situations)\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Loading Training Data (90%)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if USE_COT:\n",
    "    train_loader = CoTTrainingDataLoader(\n",
    "        active_train_data_file,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        situation_ids=train_situation_ids\n",
    "    )\n",
    "else:\n",
    "    train_loader = TrainingDataLoader(\n",
    "        active_train_data_file, \n",
    "        epoch=0, \n",
    "        random_seed=RANDOM_SEED,\n",
    "        situation_ids=train_situation_ids\n",
    "    )\n",
    "train_df = train_loader.load_and_process_data()\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD IN-DISTRIBUTION VALIDATION DATA (10% of training situations)\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Loading In-Distribution Validation Data (10%)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if USE_COT:\n",
    "    # For CoT, use the same CoT loader for in-dist validation\n",
    "    in_dist_val_loader = CoTTrainingDataLoader(\n",
    "        active_train_data_file,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        situation_ids=in_dist_val_situation_ids\n",
    "    )\n",
    "else:\n",
    "    in_dist_val_loader = InDistributionValidationDataLoader(\n",
    "        active_train_data_file,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        situation_ids=in_dist_val_situation_ids\n",
    "    )\n",
    "in_dist_val_df = in_dist_val_loader.load_and_process_data()\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD OUT-OF-DISTRIBUTION VALIDATION DATA\n",
    "# When USE_COT=True, use CoT validation data to match training format\n",
    "# When USE_COT=False, use label-only cooperate data\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Loading Out-of-Distribution Validation Data\")\n",
    "print(f\"{'='*60}\")\n",
    "if USE_COT:\n",
    "    out_dist_val_loader = CoTValidationDataLoader(COT_VAL_DATA_FILE, random_seed=RANDOM_SEED)\n",
    "else:\n",
    "    out_dist_val_loader = ValidationDataLoader(VAL_DATA_FILE, random_seed=RANDOM_SEED)\n",
    "out_dist_val_df = out_dist_val_loader.load_and_process_data()\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Dataset Summary\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Training file: {active_train_data_file}\")\n",
    "print(f\"    Training situations: {len(train_df)} (90%)\")\n",
    "print(f\"    In-dist validation situations: {len(in_dist_val_df)} (10%)\")\n",
    "if USE_COT:\n",
    "    print(f\"  Out-of-dist validation file: {COT_VAL_DATA_FILE}\")\n",
    "else:\n",
    "    print(f\"  Out-of-dist validation file: {VAL_DATA_FILE}\")\n",
    "print(f\"    Out-of-dist validation situations: {len(out_dist_val_df)}\")\n",
    "print(f\"\\n  Training mode: {'CoT (Chain-of-Thought)' if USE_COT else 'Label-only'}\")\n",
    "print(f\"  Active max_length: {active_max_length}\")\n",
    "print(f\"\\n  Label types:\")\n",
    "if USE_COT:\n",
    "    print(f\"    Training: CARA CoT (full reasoning)\")\n",
    "    print(f\"    In-dist validation: CARA CoT (full reasoning)\")\n",
    "    print(f\"    Out-of-dist validation: Cooperate CoT (full reasoning)\")\n",
    "else:\n",
    "    print(f\"    Training: CARA (risk-aversion)\")\n",
    "    print(f\"    In-dist validation: CARA (risk-aversion)\")\n",
    "    print(f\"    Out-of-dist validation: Cooperate (generalization test, label-only)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Validate data format\n",
    "for df_name, df in [('train_df', train_df), ('in_dist_val_df', in_dist_val_df), ('out_dist_val_df', out_dist_val_df)]:\n",
    "    assert 'prompt_text' in df.columns, f\"Missing prompt_text column in {df_name}\"\n",
    "    if USE_COT:\n",
    "        assert 'chosen_full' in df.columns, f\"Missing chosen_full column in {df_name} (required for CoT)\"\n",
    "        assert 'rejected_full' in df.columns, f\"Missing rejected_full column in {df_name} (required for CoT)\"\n",
    "    else:\n",
    "        assert 'correct_label' in df.columns, f\"Missing correct_label column in {df_name}\"\n",
    "        assert 'incorrect_label' in df.columns, f\"Missing incorrect_label column in {df_name}\"\n",
    "\n",
    "print(\"\\nData validation passed!\")\n",
    "print(\"Data loading complete!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Pairwise Dataset for Reward Modeling\n\nDataset class that provides pairs of (preferred, rejected) options for Bradley-Terry loss.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class PairwiseRewardDataset(Dataset):\n",
    "    \"\"\"Dataset for pairwise reward model training with Bradley-Terry loss.\n",
    "    \n",
    "    Supports two formats (auto-detected based on dataframe columns):\n",
    "    1. Label-only format: Uses prompt_text + correct_label/incorrect_label\n",
    "    2. CoT (Chain-of-Thought) format: Uses chosen_full/rejected_full columns directly\n",
    "    \n",
    "    Also supports optional error_type metadata for validation analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_length: int = 256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame with columns: prompt_text, correct_label, incorrect_label\n",
    "                       For CoT: also needs chosen_full, rejected_full\n",
    "                       Optional: error_type, incorrect_option_type (for validation breakdown)\n",
    "            tokenizer: Tokenizer for encoding text\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Check if error_type column exists (for validation data)\n",
    "        self.has_error_types = 'error_type' in self.data.columns\n",
    "        if self.has_error_types:\n",
    "            self.error_types = self.data['error_type'].tolist()\n",
    "        else:\n",
    "            self.error_types = None\n",
    "        \n",
    "        # Auto-detect CoT format by checking for chosen_full/rejected_full columns\n",
    "        self.use_cot = 'chosen_full' in self.data.columns and 'rejected_full' in self.data.columns\n",
    "        if self.use_cot:\n",
    "            print(f\"  Using CoT format (chosen_full/rejected_full columns detected)\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        if self.use_cot:\n",
    "            # CoT format: use full chain-of-thought responses\n",
    "            # Format: \"{prompt}\\n\\n{cot_response}\" where cot_response includes <think> tags\n",
    "            preferred_text = f\"{row['prompt_text']}\\n\\n{row['chosen_full']}\"\n",
    "            rejected_text = f\"{row['prompt_text']}\\n\\n{row['rejected_full']}\"\n",
    "        else:\n",
    "            # Label-only format: construct from prompt + label\n",
    "            preferred_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['correct_label']}\"\n",
    "            rejected_text = f\"{row['prompt_text']}\\n\\nChosen option: {row['incorrect_label']}\"\n",
    "        \n",
    "        # Tokenize both options\n",
    "        preferred_encoding = self.tokenizer(\n",
    "            preferred_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        rejected_encoding = self.tokenizer(\n",
    "            rejected_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'preferred_input_ids': preferred_encoding['input_ids'].squeeze(0),\n",
    "            'preferred_attention_mask': preferred_encoding['attention_mask'].squeeze(0),\n",
    "            'rejected_input_ids': rejected_encoding['input_ids'].squeeze(0),\n",
    "            'rejected_attention_mask': rejected_encoding['attention_mask'].squeeze(0),\n",
    "        }\n",
    "        \n",
    "        # Include error_type if available (for validation analysis)\n",
    "        if self.has_error_types:\n",
    "            result['error_type'] = self.error_types[idx]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_error_type_indices(self) -> Dict[str, List[int]]:\n",
    "        \"\"\"Get indices grouped by error type for validation analysis.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping error_type to list of example indices\n",
    "        \"\"\"\n",
    "        if not self.has_error_types:\n",
    "            return {}\n",
    "        \n",
    "        indices = {'too_risky': [], 'too_risk_averse': [], 'other': []}\n",
    "        for idx, error_type in enumerate(self.error_types):\n",
    "            if error_type in indices:\n",
    "                indices[error_type].append(idx)\n",
    "        return indices\n",
    "\n",
    "print(\"PairwiseRewardDataset defined (with error_type and CoT support)\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Reward Model Architecture\n\nQwen3-8B base model with LoRA adapters (q_proj, v_proj) + trainable scalar reward head.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class RewardModel(nn.Module):\n    \"\"\"Reward model with LoRA-adapted backbone and trainable scalar reward head\"\"\"\n    \n    def __init__(\n        self, \n        model_name: str = \"Qwen/Qwen3-8B\",\n        lora_r: int = 8,\n        lora_alpha: int = 16,\n        lora_dropout: float = 0.05,\n        lora_target_modules: list = None,\n        reward_head_init_std: float = 0.01,\n    ):\n        super().__init__()\n        \n        # Load base model in fp16\n        print(f\"Loading base model: {model_name}\")\n        self.backbone = AutoModel.from_pretrained(\n            model_name,\n            dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        \n        hidden_size = self.backbone.config.hidden_size\n        \n        # Add reward head BEFORE applying LoRA\n        self.reward_head = nn.Linear(hidden_size, 1, bias=True)\n        \n        # Initialize reward head with small weights\n        nn.init.normal_(self.reward_head.weight, mean=0.0, std=reward_head_init_std)\n        nn.init.zeros_(self.reward_head.bias)\n        \n        # Configure and apply LoRA\n        if lora_target_modules is None:\n            lora_target_modules = [\"q_proj\", \"v_proj\"]\n        \n        lora_config = LoraConfig(\n            r=lora_r,\n            lora_alpha=lora_alpha,\n            lora_dropout=lora_dropout,\n            target_modules=lora_target_modules,\n            bias=\"none\",\n            task_type=TaskType.FEATURE_EXTRACTION,\n        )\n        \n        self.backbone = get_peft_model(self.backbone, lora_config)\n        \n        # Print trainable parameter info\n        print(f\"\\nLoRA Configuration:\")\n        print(f\"  Rank: {lora_r}\")\n        print(f\"  Alpha: {lora_alpha}\")\n        print(f\"  Dropout: {lora_dropout}\")\n        print(f\"  Target modules: {lora_target_modules}\")\n        \n        self.backbone.print_trainable_parameters()\n        \n        print(f\"\\nReward head: Linear({hidden_size} -> 1) with bias\")\n        \n        # Count total trainable parameters\n        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        total_params = sum(p.numel() for p in self.parameters())\n        print(f\"Total trainable: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.4f}%)\")\n        \n    def _extract_last_hidden(self, input_ids, attention_mask):\n        \"\"\"Run backbone and extract last-token hidden states in fp32.\"\"\"\n        outputs = self.backbone(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n        )\n        \n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = input_ids.shape[0]\n        hidden_states = outputs.last_hidden_state\n        last_hidden_states = hidden_states[\n            torch.arange(batch_size, device=hidden_states.device),\n            sequence_lengths\n        ]\n        \n        return last_hidden_states.float()\n\n    def forward(self, input_ids, attention_mask, return_hidden=False):\n        \"\"\"\n        Forward pass to compute scalar reward from final hidden state.\n        \n        Args:\n            input_ids: Input token IDs [batch_size, seq_len]\n            attention_mask: Attention mask [batch_size, seq_len]\n            return_hidden: If True, also return last hidden states (for reference reward reuse)\n            \n        Returns:\n            rewards: Scalar reward scores [batch_size]\n            last_hidden_states: (optional) fp32 hidden states [batch_size, hidden_size]\n        \"\"\"\n        last_hidden_states = self._extract_last_hidden(input_ids, attention_mask)\n        rewards = self.reward_head(last_hidden_states).squeeze(-1)\n        \n        if return_hidden:\n            return rewards, last_hidden_states\n        return rewards\n\nprint(\"RewardModel class defined (with LoRA support)\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Training Setup\n\nInitialize model, tokenizer, datasets, loss function, and optimizer.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Load tokenizer\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Create datasets with appropriate max_length\nprint(\"Creating datasets...\")\ntrain_dataset = PairwiseRewardDataset(train_df, tokenizer, max_length=active_max_length)\nin_dist_val_dataset = PairwiseRewardDataset(in_dist_val_df, tokenizer, max_length=active_max_length)\n# Out-of-dist: use CoT max_length when USE_COT=True, standard MAX_LENGTH otherwise\nout_dist_max_length = active_max_length if USE_COT else MAX_LENGTH\nout_dist_val_dataset = PairwiseRewardDataset(out_dist_val_df, tokenizer, max_length=out_dist_max_length)\n\nprint(f\"  Training examples: {len(train_dataset)}\")\nprint(f\"  In-distribution validation examples: {len(in_dist_val_dataset)}\")\nprint(f\"  Out-of-distribution validation examples: {len(out_dist_val_dataset)}\")\n\n\n# Helper function for per-epoch re-randomization of training data\n# Note: For CoT data, no re-randomization is needed (data is pre-generated)\ndef recreate_training_dataset(epoch: int):\n    \"\"\"Recreate training dataset with new randomization for 'both' bucket cases.\n    \n    This function creates a fresh training dataset where situations with\n    low_bucket_label='both' get a new random choice between linear_best_labels\n    and CARA_alpha_0_10_best_labels.\n    \n    Note: For CoT training, this function is not called since CoT data is pre-generated.\n    \n    Args:\n        epoch: Current epoch number (used for reproducible randomization)\n        \n    Returns:\n        PairwiseRewardDataset with re-randomized label selections\n    \"\"\"\n    if USE_COT:\n        # CoT data is pre-generated, no re-randomization needed\n        # Just return the existing dataset\n        return train_dataset\n    \n    loader = TrainingDataLoader(\n        active_train_data_file, \n        epoch=epoch, \n        random_seed=RANDOM_SEED,\n        situation_ids=train_situation_ids  # Only use training situations\n    )\n    new_train_df = loader.load_and_process_data()\n    return PairwiseRewardDataset(new_train_df, tokenizer, max_length=active_max_length)\n\n\n# Initialize model with LoRA\nprint(\"\\nInitializing reward model with LoRA...\")\nmodel = RewardModel(\n    model_name=MODEL_NAME,\n    lora_r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    lora_target_modules=LORA_TARGET_MODULES,\n    reward_head_init_std=REWARD_HEAD_INIT_STD,\n)\n\n# Get device from backbone\ndevice = next(model.backbone.parameters()).device\n\n# Move reward head to same device and ensure fp32\nmodel.reward_head = model.reward_head.to(device).float()\n\nprint(f\"\\n  Device: {device}\")\nprint(f\"  Backbone dtype: {next(model.backbone.parameters()).dtype}\")\nprint(f\"  Reward head dtype: {next(model.reward_head.parameters()).dtype}\")\n\n# Bradley-Terry loss function\ndef bradley_terry_loss(preferred_rewards, rejected_rewards):\n    \"\"\"\n    Bradley-Terry pairwise ranking loss\n    Loss = -log(sigmoid(r_preferred - r_rejected))\n\n    Encourages: r_preferred > r_rejected\n    \"\"\"\n    return -F.logsigmoid(preferred_rewards - rejected_rewards).mean()\n\n\ndef compute_reward_kl(pref, rej, ref_pref, ref_rej):\n    \"\"\"Compute KL divergence between trained and reference reward distributions.\"\"\"\n    trained_logits = torch.stack([pref, rej], dim=-1)\n    ref_logits = torch.stack([ref_pref, ref_rej], dim=-1)\n    trained_log_probs = F.log_softmax(trained_logits, dim=-1)\n    ref_probs = F.softmax(ref_logits, dim=-1)\n    return F.kl_div(trained_log_probs, ref_probs, reduction='batchmean')\n\n\ndef compute_reference_rewards(reference_head, last_hidden_states):\n    \"\"\"Compute rewards using frozen reference head from cached hidden states.\"\"\"\n    with torch.no_grad():\n        return reference_head(last_hidden_states.detach()).squeeze(-1)\n\n\n# Optimizer - train LoRA parameters AND reward head with different learning rates\nreward_head_lr = LEARNING_RATE * REWARD_HEAD_LR_MULTIPLIER\noptimizer = torch.optim.AdamW([\n    {'params': model.backbone.parameters(), 'lr': LEARNING_RATE},\n    {'params': model.reward_head.parameters(), 'lr': reward_head_lr},\n], weight_decay=WEIGHT_DECAY)\n\n# Calculate total training steps for scheduler\nnum_train_examples = len(train_dataset)\nsteps_per_epoch = (num_train_examples + BATCH_SIZE - 1) // BATCH_SIZE\neffective_steps_per_epoch = (steps_per_epoch + GRADIENT_ACCUMULATION_STEPS - 1) // GRADIENT_ACCUMULATION_STEPS\ntotal_training_steps = effective_steps_per_epoch * NUM_EPOCHS\n\n# Create learning rate scheduler (cosine with warmup)\nscheduler = None\nif USE_LR_SCHEDULER:\n    warmup_steps = int(WARMUP_RATIO * total_training_steps)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_training_steps,\n    )\n    print(f\"\\nLR Scheduler: cosine with {warmup_steps} warmup steps, {total_training_steps} total steps\")\n\n# Create frozen reference head for KL regularization\nreference_reward_head = None\nif USE_REFERENCE_MODEL:\n    hidden_size = model.backbone.config.hidden_size\n    reference_reward_head = nn.Linear(hidden_size, 1, bias=True)\n    reference_reward_head.load_state_dict(model.reward_head.state_dict())\n    reference_reward_head = reference_reward_head.to(device).float()\n    reference_reward_head.eval()\n    for param in reference_reward_head.parameters():\n        param.requires_grad = False\n    print(f\"Reference reward head created (frozen, KL beta={REFERENCE_KL_BETA})\")\n\nprint(f\"\\n{'='*60}\")\nprint(\"Training Configuration:\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS} steps\")\nprint(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  LoRA parameters LR: {LEARNING_RATE}\")\nprint(f\"  Reward head LR: {reward_head_lr} ({REWARD_HEAD_LR_MULTIPLIER}x)\")\nprint(f\"  Weight decay (L2): {WEIGHT_DECAY}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Max sequence length: {active_max_length}\")\nif USE_COT:\n    print(f\"  Training mode: CoT (Chain-of-Thought)\")\n    print(f\"  Per-epoch re-randomization: Disabled (CoT data is pre-generated)\")\nelse:\n    print(f\"  Training mode: Label-only\")\n    print(f\"  Per-epoch re-randomization: Enabled for 'both' bucket cases\")\nprint(f\"  LR Scheduler: {'Enabled (cosine + warmup)' if USE_LR_SCHEDULER else 'Disabled'}\")\nprint(f\"\\nDataset sizes:\")\nprint(f\"  Training: {len(train_dataset)}\")\nprint(f\"  In-dist validation: {len(in_dist_val_dataset)}\")\nprint(f\"  Out-of-dist validation: {len(out_dist_val_dataset)}\")\nprint(f\"{'='*60}\")\nprint(\"\\nTraining setup complete!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Evaluation Function\n\nCompute pairwise accuracy: percentage of pairs where preferred option scores higher.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, dataset, batch_size=None):\n    \"\"\"\n    Evaluate model on pairwise accuracy with optional error type breakdown.\n    \n    Args:\n        model: RewardModel to evaluate\n        dataset: PairwiseRewardDataset (may include error_type metadata)\n        batch_size: Batch size for evaluation (defaults to EVAL_BATCH_SIZE)\n        \n    Returns:\n        dict with keys:\n            - accuracy: Float, percentage of pairs where preferred scores higher\n            - avg_loss: Float, average Bradley-Terry loss\n            - preferred_scores: List of reward scores for preferred options\n            - rejected_scores: List of reward scores for rejected options\n            - error_type_breakdown: Dict with accuracy by error category (if available)\n    \"\"\"\n    if batch_size is None:\n        batch_size = EVAL_BATCH_SIZE\n    \n    model.eval()\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    correct = 0\n    total = 0\n    total_loss = 0.0\n    preferred_scores_list = []\n    rejected_scores_list = []\n    error_types_list = []\n    correct_by_type = {'too_risky': 0, 'too_risk_averse': 0, 'other': 0}\n    total_by_type = {'too_risky': 0, 'too_risk_averse': 0, 'other': 0}\n    \n    # Track per-example results for breakdown\n    per_example_results = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            # Get rewards for preferred options\n            preferred_rewards = model(\n                input_ids=batch['preferred_input_ids'].to(device),\n                attention_mask=batch['preferred_attention_mask'].to(device)\n            )\n            \n            # Get rewards for rejected options\n            rejected_rewards = model(\n                input_ids=batch['rejected_input_ids'].to(device),\n                attention_mask=batch['rejected_attention_mask'].to(device)\n            )\n            \n            # Compute loss\n            loss = bradley_terry_loss(preferred_rewards, rejected_rewards)\n            total_loss += loss.item() * len(preferred_rewards)\n            \n            # Compute accuracy: count pairs where preferred > rejected\n            is_correct = (preferred_rewards > rejected_rewards)\n            correct += is_correct.sum().item()\n            total += len(preferred_rewards)\n            \n            # Store scores for analysis\n            preferred_scores_list.extend(preferred_rewards.cpu().float().numpy())\n            rejected_scores_list.extend(rejected_rewards.cpu().float().numpy())\n            \n            # Track by error type if available\n            if 'error_type' in batch:\n                batch_error_types = batch['error_type']\n                error_types_list.extend(batch_error_types)\n                \n                for i, error_type in enumerate(batch_error_types):\n                    if error_type in total_by_type:\n                        total_by_type[error_type] += 1\n                        if is_correct[i].item():\n                            correct_by_type[error_type] += 1\n                \n                # Store per-example results\n                for i in range(len(preferred_rewards)):\n                    per_example_results.append({\n                        'preferred_score': preferred_rewards[i].item(),\n                        'rejected_score': rejected_rewards[i].item(),\n                        'margin': (preferred_rewards[i] - rejected_rewards[i]).item(),\n                        'is_correct': is_correct[i].item(),\n                        'error_type': batch_error_types[i],\n                    })\n    \n    accuracy = correct / total if total > 0 else 0.0\n    avg_loss = total_loss / total if total > 0 else 0.0\n    \n    # Compute accuracy by error type\n    error_type_breakdown = {}\n    has_error_types = len(error_types_list) > 0\n    \n    if has_error_types:\n        for error_type in ['too_risky', 'too_risk_averse', 'other']:\n            if total_by_type[error_type] > 0:\n                error_type_breakdown[error_type] = {\n                    'accuracy': correct_by_type[error_type] / total_by_type[error_type],\n                    'correct': correct_by_type[error_type],\n                    'total': total_by_type[error_type],\n                }\n            else:\n                error_type_breakdown[error_type] = {\n                    'accuracy': 0.0,\n                    'correct': 0,\n                    'total': 0,\n                }\n    \n    return {\n        'accuracy': accuracy,\n        'avg_loss': avg_loss,\n        'preferred_scores': preferred_scores_list,\n        'rejected_scores': rejected_scores_list,\n        'error_type_breakdown': error_type_breakdown,\n        'per_example_results': per_example_results if has_error_types else [],\n        'error_types': error_types_list,\n    }\n\n\ndef print_error_type_breakdown(breakdown: Dict, title: str = \"Error Type Breakdown\"):\n    \"\"\"Pretty print the error type breakdown.\"\"\"\n    if not breakdown:\n        return\n    \n    print(f\"\\n  {title}:\")\n    print(f\"  {'Category':<20} {'Accuracy':>10} {'Correct':>10} {'Total':>10}\")\n    print(f\"  {'-'*52}\")\n    \n    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n        if error_type in breakdown:\n            stats = breakdown[error_type]\n            acc_pct = stats['accuracy'] * 100\n            print(f\"  {error_type:<20} {acc_pct:>9.1f}% {stats['correct']:>10} {stats['total']:>10}\")\n\n\nprint(\"Evaluation function defined (with error type breakdown)\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Baseline Evaluation (Before Training)\n\nEvaluate the randomly initialized reward model to establish a baseline for comparison.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# BASELINE EVALUATION - Before any training\n# =============================================================================\n# This establishes how well the randomly initialized reward head performs\n# Expected: ~50% accuracy (random guessing)\n\nprint(\"=\"*60)\nprint(\"BASELINE EVALUATION (Before Training)\")\nprint(\"=\"*60)\n\n# =============================================================================\n# IN-DISTRIBUTION BASELINE (CARA labels, same as training)\n# =============================================================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"In-Distribution Validation (CARA labels)\")\nprint(\"-\"*60)\n\nbaseline_in_dist_eval = evaluate_model(model, in_dist_val_dataset)\n\nbaseline_in_dist_accuracy = baseline_in_dist_eval['accuracy']\nbaseline_in_dist_loss = baseline_in_dist_eval['avg_loss']\nbaseline_in_dist_pref_scores = baseline_in_dist_eval['preferred_scores']\nbaseline_in_dist_rej_scores = baseline_in_dist_eval['rejected_scores']\nbaseline_in_dist_error_breakdown = baseline_in_dist_eval['error_type_breakdown']\n\nbaseline_in_dist_margins = np.array(baseline_in_dist_pref_scores) - np.array(baseline_in_dist_rej_scores)\n\nprint(f\"\\nIn-Dist Baseline Results (Untrained Model):\")\nprint(f\"  Accuracy: {baseline_in_dist_accuracy:.4f} ({baseline_in_dist_accuracy*100:.2f}%)\")\nprint(f\"  Loss: {baseline_in_dist_loss:.4f}\")\nprint(f\"  Mean margin: {np.mean(baseline_in_dist_margins):.4f}\")\n\nif baseline_in_dist_error_breakdown:\n    print_error_type_breakdown(baseline_in_dist_error_breakdown, \"In-Dist Baseline Error Type Breakdown\")\n\n# Store in-dist baseline for later comparison\nbaseline_in_dist_results = {\n    'accuracy': baseline_in_dist_accuracy,\n    'loss': baseline_in_dist_loss,\n    'preferred_scores': baseline_in_dist_pref_scores,\n    'rejected_scores': baseline_in_dist_rej_scores,\n    'margins': baseline_in_dist_margins.tolist(),\n    'mean_margin': float(np.mean(baseline_in_dist_margins)),\n    'std_margin': float(np.std(baseline_in_dist_margins)),\n    'error_type_breakdown': baseline_in_dist_error_breakdown,\n}\n\n# =============================================================================\n# OUT-OF-DISTRIBUTION BASELINE (Cooperate labels, generalization test)\n# =============================================================================\nprint(\"\\n\" + \"-\"*60)\nprint(\"Out-of-Distribution Validation (Cooperate labels)\")\nprint(\"-\"*60)\n\nbaseline_out_dist_eval = evaluate_model(model, out_dist_val_dataset)\n\nbaseline_out_dist_accuracy = baseline_out_dist_eval['accuracy']\nbaseline_out_dist_loss = baseline_out_dist_eval['avg_loss']\nbaseline_out_dist_pref_scores = baseline_out_dist_eval['preferred_scores']\nbaseline_out_dist_rej_scores = baseline_out_dist_eval['rejected_scores']\nbaseline_out_dist_error_breakdown = baseline_out_dist_eval['error_type_breakdown']\n\nbaseline_out_dist_margins = np.array(baseline_out_dist_pref_scores) - np.array(baseline_out_dist_rej_scores)\n\nprint(f\"\\nOut-Dist Baseline Results (Untrained Model):\")\nprint(f\"  Accuracy: {baseline_out_dist_accuracy:.4f} ({baseline_out_dist_accuracy*100:.2f}%)\")\nprint(f\"  Loss: {baseline_out_dist_loss:.4f}\")\nprint(f\"  Mean margin: {np.mean(baseline_out_dist_margins):.4f}\")\n\nif baseline_out_dist_error_breakdown:\n    print_error_type_breakdown(baseline_out_dist_error_breakdown, \"Out-Dist Baseline Error Type Breakdown\")\n\n# Store out-dist baseline for later comparison\nbaseline_out_dist_results = {\n    'accuracy': baseline_out_dist_accuracy,\n    'loss': baseline_out_dist_loss,\n    'preferred_scores': baseline_out_dist_pref_scores,\n    'rejected_scores': baseline_out_dist_rej_scores,\n    'margins': baseline_out_dist_margins.tolist(),\n    'mean_margin': float(np.mean(baseline_out_dist_margins)),\n    'std_margin': float(np.std(baseline_out_dist_margins)),\n    'error_type_breakdown': baseline_out_dist_error_breakdown,\n}\n\n# Legacy alias for backward compatibility\nbaseline_results = baseline_out_dist_results\n\n# =============================================================================\n# BASELINE SUMMARY\n# =============================================================================\nprint(f\"\\n{'='*60}\")\nprint(\"BASELINE SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"  In-Distribution (CARA):        {baseline_in_dist_accuracy*100:.1f}%\")\nprint(f\"  Out-of-Distribution (Cooperate): {baseline_out_dist_accuracy*100:.1f}%\")\nprint(f\"\\nExpected baseline: ~50% (random initialization)\")\nif abs(baseline_in_dist_accuracy - 0.5) < 0.1 and abs(baseline_out_dist_accuracy - 0.5) < 0.1:\n    print(\"Both baselines are near random as expected.\")\nelse:\n    print(\"NOTE: Baseline deviates from 50% - may indicate bias in initialization or data.\")\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Training Loop\n\nTrain the model with logging, validation, and checkpointing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Create output directory for checkpoints\nos.makedirs(\"outputs\", exist_ok=True)\n\n# Training history - now tracks both in-dist and out-of-dist validation\nhistory = {\n    'train_loss': [],\n    'kl_loss': [],\n    'train_steps': [],\n    'epochs': [],\n    'reward_margins': [],\n    'preferred_rewards': [],\n    'rejected_rewards': [],\n    # Reference model tracking\n    'reference_pref_reward_mean': [],\n    'reference_rej_reward_mean': [],\n    'reward_divergence': [],  # |trained_margin - reference_margin|\n    # In-distribution validation (CARA labels)\n    'in_dist_val_accuracy': [],\n    'in_dist_val_loss': [],\n    'in_dist_error_type_accuracy': {'too_risky': [], 'too_risk_averse': [], 'other': []},\n    # Out-of-distribution validation (Cooperate labels)\n    'out_dist_val_accuracy': [],\n    'out_dist_val_loss': [],\n    'out_dist_error_type_accuracy': {'too_risky': [], 'too_risk_averse': [], 'other': []},\n}\n\n# Legacy aliases for backward compatibility\nhistory['val_accuracy'] = history['out_dist_val_accuracy']\nhistory['val_loss'] = history['out_dist_val_loss']\nhistory['error_type_accuracy'] = history['out_dist_error_type_accuracy']\n\nprint(\"Starting training...\")\nif USE_COT:\n    print(f\"Training mode: CoT (Chain-of-Thought) - data is pre-generated, no per-epoch re-randomization\")\nelse:\n    print(f\"Training mode: Label-only - data will be re-randomized each epoch for 'both' bucket cases\")\nprint(f\"Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS} steps (effective batch size = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")\nprint(f\"Validating on both in-distribution (CARA) and out-of-distribution (Cooperate) sets\\n\")\n\nbest_val_accuracy = 0.0\nearly_stopping_patience = EARLY_STOPPING_PATIENCE\nepochs_without_improvement = 0\nglobal_step = 0  # Counts optimizer steps, not forward passes\n\n# Store initial weights for comparison\ninitial_weight = model.reward_head.weight.clone().detach()\ninitial_bias = model.reward_head.bias.clone().detach()\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"{'='*60}\")\n    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n    print(f\"{'='*60}\")\n    \n    # Recreate training dataset with epoch-specific randomization\n    # This ensures 'both' bucket cases get new random label choices each epoch\n    # Note: For CoT training, data is pre-generated so no re-randomization is needed\n    if epoch > 0 and not USE_COT:\n        print(f\"  Re-randomizing training data for epoch {epoch + 1}...\")\n        train_dataset = recreate_training_dataset(epoch)\n    \n    # Create dataloader for this epoch\n    train_dataloader = DataLoader(\n        train_dataset, \n        batch_size=BATCH_SIZE, \n        shuffle=True,\n        pin_memory=torch.cuda.is_available(),\n    )\n    \n    num_batches = len(train_dataloader)\n    effective_steps = (num_batches + GRADIENT_ACCUMULATION_STEPS - 1) // GRADIENT_ACCUMULATION_STEPS\n    print(f\"  Batches this epoch: {num_batches} ({effective_steps} optimizer steps)\")\n    \n    model.train()\n    epoch_loss = 0.0\n    epoch_kl_loss = 0.0\n    accumulated_loss = 0.0  # Track loss within accumulation window\n    epoch_preferred_rewards = []\n    epoch_rejected_rewards = []\n    epoch_margins = []\n    epoch_ref_pref_rewards = []\n    epoch_ref_rej_rewards = []\n    \n    for step, batch in enumerate(train_dataloader):\n        # Zero gradients at the start of each accumulation window\n        if step % GRADIENT_ACCUMULATION_STEPS == 0:\n            optimizer.zero_grad()\n        \n        # Get input tensors\n        preferred_input_ids = batch['preferred_input_ids'].to(device)\n        preferred_attention_mask = batch['preferred_attention_mask'].to(device)\n        rejected_input_ids = batch['rejected_input_ids'].to(device)\n        rejected_attention_mask = batch['rejected_attention_mask'].to(device)\n        \n        # Forward pass - return hidden states when needed for reference rewards\n        need_hidden = reference_reward_head is not None and REFERENCE_KL_BETA > 0\n        if need_hidden:\n            preferred_rewards, pref_hidden = model(\n                input_ids=preferred_input_ids,\n                attention_mask=preferred_attention_mask,\n                return_hidden=True\n            )\n            rejected_rewards, rej_hidden = model(\n                input_ids=rejected_input_ids,\n                attention_mask=rejected_attention_mask,\n                return_hidden=True\n            )\n        else:\n            preferred_rewards = model(\n                input_ids=preferred_input_ids,\n                attention_mask=preferred_attention_mask\n            )\n            rejected_rewards = model(\n                input_ids=rejected_input_ids,\n                attention_mask=rejected_attention_mask\n            )\n        \n        # Track reward statistics\n        epoch_preferred_rewards.extend(preferred_rewards.detach().cpu().tolist())\n        epoch_rejected_rewards.extend(rejected_rewards.detach().cpu().tolist())\n        reward_margin = (preferred_rewards - rejected_rewards).detach()\n        epoch_margins.extend(reward_margin.cpu().tolist())\n        \n        # Compute Bradley-Terry loss (scaled for gradient accumulation)\n        loss = bradley_terry_loss(preferred_rewards, rejected_rewards)\n        \n        # Add KL regularization if reference model is enabled (reuses cached hidden states)\n        kl_loss_value = 0.0\n        if need_hidden:\n            ref_pref_rewards = compute_reference_rewards(reference_reward_head, pref_hidden)\n            ref_rej_rewards = compute_reference_rewards(reference_reward_head, rej_hidden)\n            # Track reference rewards for visualization\n            epoch_ref_pref_rewards.extend(ref_pref_rewards.detach().cpu().tolist())\n            epoch_ref_rej_rewards.extend(ref_rej_rewards.detach().cpu().tolist())\n            \n            kl_loss = compute_reward_kl(\n                preferred_rewards, rejected_rewards,\n                ref_pref_rewards, ref_rej_rewards\n            )\n            loss = loss + REFERENCE_KL_BETA * kl_loss\n            kl_loss_value = kl_loss.item()\n            epoch_kl_loss += kl_loss_value\n        \n        scaled_loss = loss / GRADIENT_ACCUMULATION_STEPS\n        \n        # Check for NaN loss\n        if torch.isnan(loss):\n            print(f\"  WARNING: NaN loss detected at step {step + 1}\")\n            print(f\"    Preferred rewards: {preferred_rewards}\")\n            print(f\"    Rejected rewards: {rejected_rewards}\")\n            optimizer.zero_grad()\n            continue\n        \n        # Backward pass (gradients accumulate)\n        scaled_loss.backward()\n        epoch_loss += loss.item()\n        accumulated_loss += loss.item()\n        \n        # Check if we should perform an optimizer step\n        is_accumulation_complete = (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0\n        is_last_batch = (step + 1) == num_batches\n        should_step = is_accumulation_complete or is_last_batch\n        \n        # Detailed diagnostics on first few optimizer steps\n        if should_step and global_step < 3:\n            print(f\"\\n  === Diagnostics for Optimizer Step {global_step + 1} ===\")\n            print(f\"  (Accumulated over {min(GRADIENT_ACCUMULATION_STEPS, step % GRADIENT_ACCUMULATION_STEPS + 1)} batches)\")\n            \n            # Reward statistics (from this batch only)\n            print(f\"  Rewards (last batch):\")\n            print(f\"    Preferred: mean={preferred_rewards.mean().item():.4f}, std={preferred_rewards.std().item():.4f}\")\n            print(f\"    Rejected:  mean={rejected_rewards.mean().item():.4f}, std={rejected_rewards.std().item():.4f}\")\n            print(f\"    Margin:    mean={reward_margin.mean().item():.4f}, std={reward_margin.std().item():.4f}\")\n            print(f\"    Accumulated loss: {accumulated_loss:.4f}\")\n            \n            # Gradient statistics - reward head\n            print(f\"  Gradients (Reward Head):\")\n            for name, param in model.reward_head.named_parameters():\n                if param.grad is not None:\n                    grad_norm = param.grad.norm().item()\n                    grad_mean = param.grad.mean().item()\n                    grad_max = param.grad.abs().max().item()\n                    print(f\"    {name}: norm={grad_norm:.6f}, mean={grad_mean:.6f}, max={grad_max:.6f}\")\n                else:\n                    print(f\"    {name}: NO GRADIENT!\")\n            \n            # Gradient statistics - LoRA layers\n            lora_grad_norms = []\n            for name, param in model.backbone.named_parameters():\n                if param.requires_grad and param.grad is not None:\n                    lora_grad_norms.append(param.grad.norm().item())\n            if lora_grad_norms:\n                print(f\"  Gradients (LoRA layers):\")\n                print(f\"    mean_norm={np.mean(lora_grad_norms):.6f}, max_norm={max(lora_grad_norms):.6f}\")\n            \n            # Parameter statistics\n            print(f\"  Parameters:\")\n            weight_change = (model.reward_head.weight - initial_weight).abs().max().item()\n            bias_change = (model.reward_head.bias - initial_bias).abs().max().item()\n            print(f\"    Reward head weight max change: {weight_change:.6f}\")\n            print(f\"    Reward head bias max change: {bias_change:.6f}\")\n            print()\n        \n        # Optimizer step (only after accumulating gradients)\n        if should_step:\n            if USE_GRADIENT_CLIPPING:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n            optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n            global_step += 1\n            accumulated_loss = 0.0  # Reset for next accumulation window\n        \n        # Log every 50 batches (adjusted for smaller dataset)\n        if (step + 1) % LOG_EVERY_N_BATCHES == 0:\n            avg_loss = epoch_loss / (step + 1)\n            weight_change = (model.reward_head.weight - initial_weight).abs().max().item()\n            recent_margin = np.mean(epoch_margins[-LOG_EVERY_N_BATCHES:]) if len(epoch_margins) >= LOG_EVERY_N_BATCHES else np.mean(epoch_margins)\n            \n            print(f\"  Batch {step + 1}/{num_batches} (optim step {global_step}) | \"\n                  f\"Loss: {avg_loss:.4f} | \"\n                  f\"Margin: {recent_margin:+.4f} | \"\n                  f\"Weight: {weight_change:.6f}\")\n            \n            # (epoch-level loss tracking moved to end of epoch)\n    \n    # End of epoch statistics\n    avg_train_loss = epoch_loss / len(train_dataloader)\n    \n    # Weight changes\n    total_weight_change = (model.reward_head.weight - initial_weight).abs().mean().item()\n    total_bias_change = (model.reward_head.bias - initial_bias).abs().mean().item()\n    \n    # Reward statistics\n    mean_preferred = np.mean(epoch_preferred_rewards)\n    mean_rejected = np.mean(epoch_rejected_rewards)\n    mean_margin = np.mean(epoch_margins)\n    std_margin = np.std(epoch_margins)\n    percent_correct = np.mean([m > 0 for m in epoch_margins]) * 100\n    \n    # Calculate KL loss average\n    avg_kl_loss = epoch_kl_loss / num_batches if epoch_kl_loss > 0 else 0.0\n    \n    print(f\"\\n  Epoch {epoch + 1} Summary:\")\n    print(f\"    Loss: {avg_train_loss:.4f}\")\n    if USE_REFERENCE_MODEL and REFERENCE_KL_BETA > 0:\n        print(f\"    KL Loss: {avg_kl_loss:.4f}\")\n    print(f\"    Rewards: preferred={mean_preferred:+.4f}, rejected={mean_rejected:+.4f}\")\n    print(f\"    Margin: mean={mean_margin:+.4f}, std={std_margin:.4f}\")\n    print(f\"    Correct ranking: {percent_correct:.1f}% (preferred > rejected)\")\n    print(f\"    Weight changes: weight={total_weight_change:.6f}, bias={total_bias_change:.6f}\")\n    \n    # Store for history (epoch-level metrics)\n    history['train_loss'].append(avg_train_loss)\n    history['kl_loss'].append(avg_kl_loss)\n    history['train_steps'].append(global_step)\n    history['reward_margins'].append(mean_margin)\n    history['preferred_rewards'].append(mean_preferred)\n    history['rejected_rewards'].append(mean_rejected)\n    history['epochs'].append(epoch + 1)\n    \n    # Track reference model statistics\n    if epoch_ref_pref_rewards:\n        ref_pref_mean = np.mean(epoch_ref_pref_rewards)\n        ref_rej_mean = np.mean(epoch_ref_rej_rewards)\n        trained_margin = mean_preferred - mean_rejected\n        ref_margin = ref_pref_mean - ref_rej_mean\n        divergence = abs(trained_margin - ref_margin)\n    else:\n        ref_pref_mean = 0.0\n        ref_rej_mean = 0.0\n        divergence = 0.0\n    history['reference_pref_reward_mean'].append(ref_pref_mean)\n    history['reference_rej_reward_mean'].append(ref_rej_mean)\n    history['reward_divergence'].append(divergence)\n    \n    # ==========================================================================\n    # IN-DISTRIBUTION VALIDATION (CARA labels)\n    # ==========================================================================\n    print(f\"\\n  Running in-distribution validation (CARA labels)...\")\n    in_dist_eval = evaluate_model(model, in_dist_val_dataset)\n    \n    in_dist_accuracy = in_dist_eval['accuracy']\n    in_dist_loss = in_dist_eval['avg_loss']\n    in_dist_error_breakdown = in_dist_eval['error_type_breakdown']\n    \n    print(f\"    In-dist accuracy: {in_dist_accuracy:.4f} ({in_dist_accuracy*100:.2f}%)\")\n    print(f\"    In-dist loss: {in_dist_loss:.4f}\")\n    \n    if in_dist_error_breakdown:\n        print_error_type_breakdown(in_dist_error_breakdown, \"In-Dist Error Type Breakdown\")\n    \n    # Save to history\n    history['in_dist_val_accuracy'].append(in_dist_accuracy)\n    history['in_dist_val_loss'].append(in_dist_loss)\n    \n    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n        if error_type in in_dist_error_breakdown:\n            history['in_dist_error_type_accuracy'][error_type].append(\n                in_dist_error_breakdown[error_type]['accuracy']\n            )\n    \n    # ==========================================================================\n    # OUT-OF-DISTRIBUTION VALIDATION (Cooperate labels)\n    # ==========================================================================\n    print(f\"\\n  Running out-of-distribution validation (Cooperate labels)...\")\n    out_dist_eval = evaluate_model(model, out_dist_val_dataset)\n    \n    out_dist_accuracy = out_dist_eval['accuracy']\n    out_dist_loss = out_dist_eval['avg_loss']\n    out_dist_pref_scores = out_dist_eval['preferred_scores']\n    out_dist_rej_scores = out_dist_eval['rejected_scores']\n    out_dist_error_breakdown = out_dist_eval['error_type_breakdown']\n    \n    out_dist_margin = np.mean(out_dist_pref_scores) - np.mean(out_dist_rej_scores)\n    \n    print(f\"    Out-dist accuracy: {out_dist_accuracy:.4f} ({out_dist_accuracy*100:.2f}%)\")\n    print(f\"    Out-dist loss: {out_dist_loss:.4f}\")\n    print(f\"    Out-dist margin: {out_dist_margin:+.4f}\")\n    \n    if out_dist_error_breakdown:\n        print_error_type_breakdown(out_dist_error_breakdown, \"Out-Dist Error Type Breakdown\")\n    \n    # Save to history\n    history['out_dist_val_accuracy'].append(out_dist_accuracy)\n    history['out_dist_val_loss'].append(out_dist_loss)\n    \n    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n        if error_type in out_dist_error_breakdown:\n            history['out_dist_error_type_accuracy'][error_type].append(\n                out_dist_error_breakdown[error_type]['accuracy']\n            )\n    \n    # ==========================================================================\n    # CHECKPOINT SAVING (based on out-of-distribution accuracy for generalization)\n    # ==========================================================================\n    if out_dist_accuracy > best_val_accuracy:\n        best_val_accuracy = out_dist_accuracy\n        epochs_without_improvement = 0\n        checkpoint_dir = f\"outputs/best_model_epoch{epoch+1}\"\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        \n        # Save LoRA adapter weights\n        model.backbone.save_pretrained(checkpoint_dir)\n        \n        # Save reward head separately\n        torch.save({\n            'reward_head_state_dict': model.reward_head.state_dict(),\n            'epoch': epoch + 1,\n            'in_dist_val_accuracy': in_dist_accuracy,\n            'out_dist_val_accuracy': out_dist_accuracy,\n            'in_dist_val_loss': in_dist_loss,\n            'out_dist_val_loss': out_dist_loss,\n            'in_dist_error_breakdown': in_dist_error_breakdown,\n            'out_dist_error_breakdown': out_dist_error_breakdown,\n            'optimizer_state_dict': optimizer.state_dict(),\n            'lora_config': {\n                'r': LORA_R,\n                'alpha': LORA_ALPHA,\n                'dropout': LORA_DROPOUT,\n                'target_modules': LORA_TARGET_MODULES,\n            }\n        }, os.path.join(checkpoint_dir, \"reward_head.pt\"))\n        \n        print(f\"\\n    New best! Saved checkpoint: {checkpoint_dir}\")\n    else:\n        epochs_without_improvement += 1\n        if epochs_without_improvement >= early_stopping_patience:\n            print(f\"\\n  Early stopping at epoch {epoch+1} (no improvement for {early_stopping_patience} epochs)\")\n            break\n    \n    print()\n\nprint(f\"\\n{'='*60}\")\nprint(\"Training Complete!\")\nprint(f\"{'='*60}\")\nprint(f\"Best out-of-dist validation accuracy: {best_val_accuracy:.4f} ({best_val_accuracy*100:.2f}%)\")\nprint(f\"Total optimizer steps: {global_step}\")\nprint(f\"Final model saved to: outputs/best_model_epoch*/\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Visualization and Results\n\nPlot training curves, compare against baseline, and analyze model performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# FINAL EVALUATION ON BOTH VALIDATION SETS\n",
    "# =============================================================================\n",
    "print(\"Evaluating final model on both validation sets...\")\n",
    "\n",
    "# In-distribution final evaluation\n",
    "print(\"\\nIn-distribution validation (CARA labels)...\")\n",
    "final_in_dist_eval = evaluate_model(model, in_dist_val_dataset)\n",
    "final_in_dist_accuracy = final_in_dist_eval['accuracy']\n",
    "final_in_dist_loss = final_in_dist_eval['avg_loss']\n",
    "final_in_dist_pref_scores = final_in_dist_eval['preferred_scores']\n",
    "final_in_dist_rej_scores = final_in_dist_eval['rejected_scores']\n",
    "final_in_dist_error_breakdown = final_in_dist_eval['error_type_breakdown']\n",
    "final_in_dist_margins = np.array(final_in_dist_pref_scores) - np.array(final_in_dist_rej_scores)\n",
    "\n",
    "# Out-of-distribution final evaluation\n",
    "print(\"Out-of-distribution validation (Cooperate labels)...\")\n",
    "final_out_dist_eval = evaluate_model(model, out_dist_val_dataset)\n",
    "final_out_dist_accuracy = final_out_dist_eval['accuracy']\n",
    "final_out_dist_loss = final_out_dist_eval['avg_loss']\n",
    "final_out_dist_pref_scores = final_out_dist_eval['preferred_scores']\n",
    "final_out_dist_rej_scores = final_out_dist_eval['rejected_scores']\n",
    "final_out_dist_error_breakdown = final_out_dist_eval['error_type_breakdown']\n",
    "final_out_dist_per_example = final_out_dist_eval['per_example_results']\n",
    "final_out_dist_margins = np.array(final_out_dist_pref_scores) - np.array(final_out_dist_rej_scores)\n",
    "\n",
    "# Legacy aliases for backward compatibility\n",
    "final_accuracy = final_out_dist_accuracy\n",
    "final_loss = final_out_dist_loss\n",
    "preferred_scores = final_out_dist_pref_scores\n",
    "rejected_scores = final_out_dist_rej_scores\n",
    "final_error_breakdown = final_out_dist_error_breakdown\n",
    "per_example_results = final_out_dist_per_example\n",
    "reward_margins = final_out_dist_margins\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nIn-Distribution (CARA labels):\")\n",
    "print(f\"  Accuracy: {final_in_dist_accuracy:.4f} ({final_in_dist_accuracy*100:.2f}%)\")\n",
    "print(f\"  Loss: {final_in_dist_loss:.4f}\")\n",
    "print(f\"  Mean margin: {np.mean(final_in_dist_margins):.4f}\")\n",
    "\n",
    "out_dist_format = \"Cooperate CoT\" if USE_COT else \"Cooperate labels\"\n",
    "print(f\"\\nOut-of-Distribution ({out_dist_format}):\")\n",
    "print(f\"  Accuracy: {final_out_dist_accuracy:.4f} ({final_out_dist_accuracy*100:.2f}%)\")\n",
    "print(f\"  Loss: {final_out_dist_loss:.4f}\")\n",
    "print(f\"  Mean margin: {np.mean(final_out_dist_margins):.4f}\")\n",
    "\n",
    "# Print error type breakdowns\n",
    "if final_in_dist_error_breakdown:\n",
    "    print_error_type_breakdown(final_in_dist_error_breakdown, \"In-Dist Final Error Type Breakdown\")\n",
    "if final_out_dist_error_breakdown:\n",
    "    print_error_type_breakdown(final_out_dist_error_breakdown, \"Out-Dist Final Error Type Breakdown\")\n",
    "\n",
    "# =============================================================================\n",
    "# BASELINE vs TRAINED COMPARISON\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BASELINE vs TRAINED MODEL COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n                         Baseline    Trained     Improvement\")\n",
    "print(f\"  In-Dist Accuracy:      {baseline_in_dist_results['accuracy']*100:6.2f}%     {final_in_dist_accuracy*100:6.2f}%     {(final_in_dist_accuracy - baseline_in_dist_results['accuracy'])*100:+6.2f}%\")\n",
    "print(f\"  Out-Dist Accuracy:     {baseline_out_dist_results['accuracy']*100:6.2f}%     {final_out_dist_accuracy*100:6.2f}%     {(final_out_dist_accuracy - baseline_out_dist_results['accuracy'])*100:+6.2f}%\")\n",
    "print(f\"  In-Dist Loss:          {baseline_in_dist_results['loss']:6.4f}      {final_in_dist_loss:6.4f}      {final_in_dist_loss - baseline_in_dist_results['loss']:+6.4f}\")\n",
    "print(f\"  Out-Dist Loss:         {baseline_out_dist_results['loss']:6.4f}      {final_out_dist_loss:6.4f}      {final_out_dist_loss - baseline_out_dist_results['loss']:+6.4f}\")\n",
    "\n",
    "# Compare error type breakdown for out-of-dist\n",
    "if final_out_dist_error_breakdown and baseline_out_dist_results.get('error_type_breakdown'):\n",
    "    print(f\"\\n  Out-Dist Error Type Accuracy Comparison:\")\n",
    "    print(f\"  {'Category':<20} {'Baseline':>10} {'Trained':>10} {'Improvement':>12}\")\n",
    "    print(f\"  {'-'*54}\")\n",
    "    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n",
    "        base_acc = baseline_out_dist_results['error_type_breakdown'].get(error_type, {}).get('accuracy', 0) * 100\n",
    "        train_acc = final_out_dist_error_breakdown.get(error_type, {}).get('accuracy', 0) * 100\n",
    "        improvement = train_acc - base_acc\n",
    "        print(f\"  {error_type:<20} {base_acc:>9.1f}% {train_acc:>9.1f}% {improvement:>+11.1f}%\")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE COMPREHENSIVE VISUALIZATIONS (5x3 grid with in-dist overlays)\n",
    "# =============================================================================\n",
    "fig = plt.figure(figsize=(20, 22))\n",
    "gs = fig.add_gridspec(5, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "fig.suptitle('Reward Model Training Results\\nTrained on CARA (Risk-Aversion) | Validated on Both In-Dist and Out-of-Dist', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "# =============================================================================\n",
    "# Row 1: Training Progress\n",
    "# =============================================================================\n",
    "\n",
    "# Plot 1: Training Loss Over Time (with KL Loss on secondary axis)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "if len(history['train_steps']) > 0:\n",
    "    ax1.plot(history['train_steps'], history['train_loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "    ax1.axhline(y=baseline_out_dist_results['loss'], color='gray', linestyle='--', alpha=0.7, \n",
    "                label=f'Out-Dist Baseline ({baseline_out_dist_results[\"loss\"]:.3f})')\n",
    "    ax1.set_xlabel('Training Steps')\n",
    "    ax1.set_ylabel('Loss', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    # Add KL loss on secondary y-axis if available\n",
    "    if 'kl_loss' in history and any(kl > 0 for kl in history['kl_loss']):\n",
    "        ax1_kl = ax1.twinx()\n",
    "        ax1_kl.plot(history['train_steps'], history['kl_loss'], 'r-', linewidth=1.5, alpha=0.7, label='KL Loss')\n",
    "        ax1_kl.set_ylabel('KL Loss', color='r')\n",
    "        ax1_kl.tick_params(axis='y', labelcolor='r')\n",
    "        # Combine legends\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax1_kl.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, fontsize=7, loc='upper right')\n",
    "    else:\n",
    "        ax1.legend(fontsize=8)\n",
    "    \n",
    "    ax1.set_title('Training Loss Over Time')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation Accuracy Over Epochs (BOTH in-dist and out-of-dist)\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "if len(history['epochs']) > 0:\n",
    "    # Out-of-distribution (solid line)\n",
    "    epochs_with_baseline = [0] + history['epochs']\n",
    "    out_dist_acc_with_baseline = [baseline_out_dist_results['accuracy']] + history['out_dist_val_accuracy']\n",
    "    ax2.plot(epochs_with_baseline, out_dist_acc_with_baseline, 'g-o', linewidth=2, markersize=8, \n",
    "             label='Out-Dist (Cooperate)')\n",
    "    \n",
    "    # In-distribution (dashed line)\n",
    "    in_dist_acc_with_baseline = [baseline_in_dist_results['accuracy']] + history['in_dist_val_accuracy']\n",
    "    ax2.plot(epochs_with_baseline, in_dist_acc_with_baseline, 'b--s', linewidth=2, markersize=7, \n",
    "             label='In-Dist (CARA)')\n",
    "    \n",
    "    ax2.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\n",
    "    ax2.scatter([0], [baseline_out_dist_results['accuracy']], color='orange', s=100, zorder=5, marker='D')\n",
    "    ax2.scatter([0], [baseline_in_dist_results['accuracy']], color='orange', s=100, zorder=5, marker='D', \n",
    "                label='Baseline')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Validation Accuracy Over Training\\n(In-Dist vs Out-of-Dist)')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.legend(fontsize=8, loc='lower right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Reward Margin Progression (with Reference Divergence)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "if len(history['epochs']) > 0 and len(history['reward_margins']) > 0:\n",
    "    epochs = history['epochs']\n",
    "    ax3.plot(epochs, history['reward_margins'], 'purple', linewidth=2, marker='s', markersize=6, label='Margin')\n",
    "    ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='No Preference')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Reward Margin', color='purple')\n",
    "    ax3.tick_params(axis='y', labelcolor='purple')\n",
    "    \n",
    "    # Add reward divergence on secondary y-axis if available\n",
    "    if 'reward_divergence' in history and any(d > 0 for d in history['reward_divergence']):\n",
    "        ax3_div = ax3.twinx()\n",
    "        ax3_div.plot(epochs, history['reward_divergence'], 'orange', linewidth=1.5, \n",
    "                    marker='d', markersize=4, alpha=0.8, label='Divergence')\n",
    "        ax3_div.set_ylabel('Divergence from Reference', color='orange')\n",
    "        ax3_div.tick_params(axis='y', labelcolor='orange')\n",
    "        # Combine legends\n",
    "        lines1, labels1 = ax3.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax3_div.get_legend_handles_labels()\n",
    "        ax3.legend(lines1 + lines2, labels1 + labels2, fontsize=7, loc='upper left')\n",
    "    else:\n",
    "        ax3.legend(fontsize=8)\n",
    "    \n",
    "    ax3.set_title('Margin & Reference Divergence')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# =============================================================================\n",
    "# Row 2: Score Distributions (Out-of-Dist Trained Model)\n",
    "# =============================================================================\n",
    "\n",
    "# Plot 4: Score Distribution Comparison (Histogram) - Out-Dist Trained\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "bins = np.linspace(min(min(preferred_scores), min(rejected_scores)),\n",
    "                  max(max(preferred_scores), max(rejected_scores)), 30)\n",
    "ax4.hist(preferred_scores, bins=bins, alpha=0.6, label='Preferred (Cooperate)', \n",
    "         color='green', density=True, edgecolor='black', linewidth=0.5)\n",
    "ax4.hist(rejected_scores, bins=bins, alpha=0.6, label='Rejected (Non-Cooperate)', \n",
    "         color='red', density=True, edgecolor='black', linewidth=0.5)\n",
    "ax4.set_xlabel('Reward Score')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Out-Dist Trained: Score Distribution')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 5: Scatter Plot - Preferred vs Rejected Scores (Out-Dist Trained)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.scatter(rejected_scores, preferred_scores, alpha=0.5, s=30, c=reward_margins, \n",
    "            cmap='RdYlGn', edgecolors='black', linewidth=0.5)\n",
    "min_val = min(min(rejected_scores), min(preferred_scores))\n",
    "max_val = max(max(rejected_scores), max(preferred_scores))\n",
    "ax5.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, linewidth=2, label='Equal Scores')\n",
    "ax5.fill_between([min_val, max_val], [min_val, max_val], [max_val, max_val],\n",
    "                alpha=0.15, color='green', label='Correct (Preferred > Rejected)')\n",
    "ax5.fill_between([min_val, max_val], [min_val, min_val], [min_val, max_val],\n",
    "                alpha=0.15, color='red', label='Incorrect')\n",
    "ax5.set_xlabel('Rejected Score')\n",
    "ax5.set_ylabel('Preferred Score')\n",
    "ax5.set_title('Out-Dist Trained: Pairwise Comparison')\n",
    "ax5.legend(fontsize=8)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "ax5.axis('equal')\n",
    "\n",
    "# Plot 6: Reward Margin Distribution (Out-Dist Trained)\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ax6.hist(reward_margins, bins=40, alpha=0.7, color='purple', edgecolor='black', linewidth=0.5)\n",
    "ax6.axvline(x=0, color='r', linestyle='--', linewidth=2, label='No Preference')\n",
    "ax6.axvline(x=np.mean(reward_margins), color='g', linestyle='-', linewidth=2, \n",
    "            label=f'Out-Dist Mean: {np.mean(reward_margins):.3f}')\n",
    "ax6.axvline(x=np.mean(final_in_dist_margins), color='b', linestyle='--', linewidth=2, \n",
    "            label=f'In-Dist Mean: {np.mean(final_in_dist_margins):.3f}')\n",
    "ax6.set_xlabel('Reward Margin (Preferred - Rejected)')\n",
    "ax6.set_ylabel('Count')\n",
    "ax6.set_title('Trained: Margin Distribution Comparison')\n",
    "ax6.legend(fontsize=8)\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# =============================================================================\n",
    "# Row 3: Error Type Breakdown (with in-dist comparison)\n",
    "# =============================================================================\n",
    "\n",
    "# Plot 7: Error Type Accuracy Comparison (Baseline vs Trained, In-Dist vs Out-Dist)\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "if final_out_dist_error_breakdown:\n",
    "    error_types = ['too_risky', 'too_risk_averse', 'other']\n",
    "    error_type_labels = ['Too Risky', 'Too Risk-Averse', 'Other']\n",
    "    x = np.arange(len(error_types))\n",
    "    width = 0.2\n",
    "    \n",
    "    # Out-dist baseline\n",
    "    out_baseline_accs = [baseline_out_dist_results.get('error_type_breakdown', {}).get(et, {}).get('accuracy', 0) * 100 \n",
    "                        for et in error_types]\n",
    "    # Out-dist trained\n",
    "    out_trained_accs = [final_out_dist_error_breakdown.get(et, {}).get('accuracy', 0) * 100 \n",
    "                       for et in error_types]\n",
    "    # In-dist trained (if available)\n",
    "    in_trained_accs = [final_in_dist_error_breakdown.get(et, {}).get('accuracy', 0) * 100 \n",
    "                      for et in error_types] if final_in_dist_error_breakdown else [0, 0, 0]\n",
    "    \n",
    "    bars1 = ax7.bar(x - width, out_baseline_accs, width, label='Out-Dist Baseline', color='gray', alpha=0.7)\n",
    "    bars2 = ax7.bar(x, out_trained_accs, width, label='Out-Dist Trained', color='green', alpha=0.7)\n",
    "    bars3 = ax7.bar(x + width, in_trained_accs, width, label='In-Dist Trained', color='blue', alpha=0.7)\n",
    "    \n",
    "    ax7.axhline(y=50, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\n",
    "    ax7.set_ylabel('Accuracy (%)')\n",
    "    ax7.set_title('Accuracy by Error Type\\n(Baseline vs Trained, In vs Out)')\n",
    "    ax7.set_xticks(x)\n",
    "    ax7.set_xticklabels(error_type_labels, fontsize=9)\n",
    "    ax7.legend(fontsize=7, loc='upper right')\n",
    "    ax7.set_ylim([0, 100])\n",
    "    ax7.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 8: Scatter Plot Colored by Error Type (Out-Dist)\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "if per_example_results:\n",
    "    colors = {'too_risky': 'orange', 'too_risk_averse': 'blue', 'other': 'gray'}\n",
    "    labels = {'too_risky': 'Too Risky', 'too_risk_averse': 'Too Risk-Averse', 'other': 'Other'}\n",
    "    \n",
    "    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n",
    "        examples = [r for r in per_example_results if r['error_type'] == error_type]\n",
    "        if examples:\n",
    "            pref = [r['preferred_score'] for r in examples]\n",
    "            rej = [r['rejected_score'] for r in examples]\n",
    "            ax8.scatter(rej, pref, alpha=0.5, s=30, c=colors[error_type], \n",
    "                       label=f'{labels[error_type]} (n={len(examples)})', edgecolors='black', linewidth=0.3)\n",
    "    \n",
    "    min_val = min(min(preferred_scores), min(rejected_scores))\n",
    "    max_val = max(max(preferred_scores), max(rejected_scores))\n",
    "    ax8.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, linewidth=2)\n",
    "    ax8.set_xlabel('Rejected Score')\n",
    "    ax8.set_ylabel('Preferred Score')\n",
    "    ax8.set_title('Out-Dist: Pairwise by Error Type')\n",
    "    ax8.legend(fontsize=8)\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    ax8.axis('equal')\n",
    "\n",
    "# Plot 9: Error Type Accuracy Over Epochs (Both in-dist and out-of-dist)\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "if history['out_dist_error_type_accuracy']['too_risky']:\n",
    "    epochs_for_plot = history['epochs']\n",
    "    colors = {'too_risky': 'orange', 'too_risk_averse': 'blue', 'other': 'gray'}\n",
    "    \n",
    "    # Out-of-dist (solid lines)\n",
    "    for error_type in ['too_risky', 'too_risk_averse']:\n",
    "        if history['out_dist_error_type_accuracy'][error_type]:\n",
    "            baseline_acc = baseline_out_dist_results.get('error_type_breakdown', {}).get(error_type, {}).get('accuracy', 0)\n",
    "            accs = [baseline_acc] + history['out_dist_error_type_accuracy'][error_type]\n",
    "            epochs_with_baseline = [0] + epochs_for_plot\n",
    "            ax9.plot(epochs_with_baseline, [a * 100 for a in accs], \n",
    "                    color=colors[error_type], marker='o', linewidth=2, \n",
    "                    markersize=6, label=f'Out: {error_type.replace(\"_\", \" \").title()}')\n",
    "    \n",
    "    # In-dist (dashed lines)\n",
    "    for error_type in ['too_risky', 'too_risk_averse']:\n",
    "        if history['in_dist_error_type_accuracy'][error_type]:\n",
    "            baseline_acc = baseline_in_dist_results.get('error_type_breakdown', {}).get(error_type, {}).get('accuracy', 0)\n",
    "            accs = [baseline_acc] + history['in_dist_error_type_accuracy'][error_type]\n",
    "            epochs_with_baseline = [0] + epochs_for_plot\n",
    "            ax9.plot(epochs_with_baseline, [a * 100 for a in accs], \n",
    "                    color=colors[error_type], marker='s', linewidth=2, linestyle='--',\n",
    "                    markersize=5, alpha=0.7, label=f'In: {error_type.replace(\"_\", \" \").title()}')\n",
    "    \n",
    "    ax9.axhline(y=50, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\n",
    "    ax9.set_xlabel('Epoch')\n",
    "    ax9.set_ylabel('Accuracy (%)')\n",
    "    ax9.set_title('Error Type Accuracy Over Training\\n(Solid=Out-Dist, Dashed=In-Dist)')\n",
    "    ax9.legend(fontsize=7, loc='lower right', ncol=2)\n",
    "    ax9.set_ylim([0, 100])\n",
    "    ax9.grid(True, alpha=0.3)\n",
    "\n",
    "# =============================================================================\n",
    "# Row 4: Baseline Comparison and Performance Analysis\n",
    "# =============================================================================\n",
    "\n",
    "# Plot 10: Baseline Score Distribution (Out-Dist)\n",
    "ax10 = fig.add_subplot(gs[3, 0])\n",
    "baseline_pref = np.array(baseline_out_dist_results['preferred_scores'])\n",
    "baseline_rej = np.array(baseline_out_dist_results['rejected_scores'])\n",
    "bins_baseline = np.linspace(min(min(baseline_pref), min(baseline_rej)),\n",
    "                           max(max(baseline_pref), max(baseline_rej)), 30)\n",
    "ax10.hist(baseline_pref, bins=bins_baseline, alpha=0.6, label='Preferred (Cooperate)', \n",
    "         color='green', density=True, edgecolor='black', linewidth=0.5)\n",
    "ax10.hist(baseline_rej, bins=bins_baseline, alpha=0.6, label='Rejected (Non-Cooperate)', \n",
    "         color='red', density=True, edgecolor='black', linewidth=0.5)\n",
    "ax10.set_xlabel('Reward Score')\n",
    "ax10.set_ylabel('Density')\n",
    "ax10.set_title('Baseline (Untrained): Score Distribution')\n",
    "ax10.legend()\n",
    "ax10.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 11: Ranking Performance (Correct vs Incorrect by Type) - Out-Dist\n",
    "ax11 = fig.add_subplot(gs[3, 1])\n",
    "if final_out_dist_error_breakdown:\n",
    "    error_types = ['too_risky', 'too_risk_averse', 'other']\n",
    "    error_type_labels = ['Too Risky', 'Too Risk-Averse', 'Other']\n",
    "    \n",
    "    correct_counts = [final_out_dist_error_breakdown.get(et, {}).get('correct', 0) for et in error_types]\n",
    "    incorrect_counts = [final_out_dist_error_breakdown.get(et, {}).get('total', 0) - \n",
    "                       final_out_dist_error_breakdown.get(et, {}).get('correct', 0) for et in error_types]\n",
    "    \n",
    "    x = np.arange(len(error_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax11.bar(x - width/2, correct_counts, width, label='Correct', \n",
    "                     color='green', alpha=0.7, edgecolor='black')\n",
    "    bars2 = ax11.bar(x + width/2, incorrect_counts, width, label='Incorrect', \n",
    "                     color='red', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax11.set_ylabel('Number of Examples')\n",
    "    ax11.set_title('Out-Dist: Correct vs Incorrect by Type')\n",
    "    ax11.set_xticks(x)\n",
    "    ax11.set_xticklabels(error_type_labels, fontsize=9)\n",
    "    ax11.legend(fontsize=8)\n",
    "    ax11.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 12: Margin Distribution by Error Type - Out-Dist\n",
    "ax12 = fig.add_subplot(gs[3, 2])\n",
    "if per_example_results:\n",
    "    colors = {'too_risky': 'orange', 'too_risk_averse': 'blue', 'other': 'gray'}\n",
    "    labels = {'too_risky': 'Too Risky', 'too_risk_averse': 'Too Risk-Averse', 'other': 'Other'}\n",
    "    \n",
    "    box_data = []\n",
    "    box_labels = []\n",
    "    for error_type in ['too_risky', 'too_risk_averse', 'other']:\n",
    "        margins_for_type = [r['margin'] for r in per_example_results if r['error_type'] == error_type]\n",
    "        if margins_for_type:\n",
    "            box_data.append(margins_for_type)\n",
    "            box_labels.append(labels[error_type])\n",
    "    \n",
    "    if box_data:\n",
    "        bp = ax12.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "        for patch, error_type in zip(bp['boxes'], ['too_risky', 'too_risk_averse', 'other']):\n",
    "            if error_type in colors:\n",
    "                patch.set_facecolor(colors[error_type])\n",
    "                patch.set_alpha(0.6)\n",
    "        \n",
    "        ax12.axhline(y=0, color='r', linestyle='--', alpha=0.5, label='No Preference')\n",
    "        ax12.set_ylabel('Reward Margin')\n",
    "        ax12.set_title('Out-Dist: Margin by Error Type')\n",
    "        ax12.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# =============================================================================\n",
    "# Row 5: Summary and CDFs\n",
    "# =============================================================================\n",
    "\n",
    "# Plot 13: Overall Summary Bar Chart (In-Dist and Out-Dist)\n",
    "ax13 = fig.add_subplot(gs[4, 0])\n",
    "categories = ['Baseline\\nIn-Dist', 'Trained\\nIn-Dist', 'Baseline\\nOut-Dist', 'Trained\\nOut-Dist']\n",
    "accuracies = [baseline_in_dist_results['accuracy'] * 100, final_in_dist_accuracy * 100,\n",
    "              baseline_out_dist_results['accuracy'] * 100, final_out_dist_accuracy * 100]\n",
    "colors_bar = ['lightblue', 'blue', 'lightgreen', 'green']\n",
    "bars = ax13.bar(categories, accuracies, color=colors_bar, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "ax13.axhline(y=50, color='r', linestyle='--', alpha=0.5, label='Random (50%)')\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax13.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "             f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "ax13.set_ylabel('Accuracy (%)')\n",
    "ax13.set_title('Overall Accuracy Comparison')\n",
    "ax13.set_ylim([0, 100])\n",
    "ax13.legend()\n",
    "ax13.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 14: Score Evolution Over Training (Trained vs Reference)\n",
    "ax14 = fig.add_subplot(gs[4, 1])\n",
    "if len(history['epochs']) > 0 and len(history['preferred_rewards']) > 0:\n",
    "    epochs = history['epochs']\n",
    "    # Trained rewards\n",
    "    ax14.plot(epochs, history['preferred_rewards'], 'g-o', \n",
    "             linewidth=2, markersize=6, label='Trained Preferred')\n",
    "    ax14.plot(epochs, history['rejected_rewards'], 'r-s', \n",
    "             linewidth=2, markersize=6, label='Trained Rejected')\n",
    "    # Reference rewards (if available)\n",
    "    if 'reference_pref_reward_mean' in history and any(r != 0 for r in history['reference_pref_reward_mean']):\n",
    "        ax14.plot(epochs, history['reference_pref_reward_mean'], 'g--', \n",
    "                 linewidth=1.5, alpha=0.6, label='Reference Preferred')\n",
    "        ax14.plot(epochs, history['reference_rej_reward_mean'], 'r--', \n",
    "                 linewidth=1.5, alpha=0.6, label='Reference Rejected')\n",
    "    ax14.set_xlabel('Epoch')\n",
    "    ax14.set_ylabel('Mean Reward Score')\n",
    "    ax14.set_title('Trained vs Reference Rewards')\n",
    "    ax14.legend(fontsize=7)\n",
    "    ax14.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 15: CDFs (In-Dist and Out-Dist comparison)\n",
    "ax15 = fig.add_subplot(gs[4, 2])\n",
    "# Out-dist trained\n",
    "sorted_pref = np.sort(preferred_scores)\n",
    "sorted_rej = np.sort(rejected_scores)\n",
    "cdf_pref = np.arange(1, len(sorted_pref) + 1) / len(sorted_pref)\n",
    "cdf_rej = np.arange(1, len(sorted_rej) + 1) / len(sorted_rej)\n",
    "ax15.plot(sorted_pref, cdf_pref, 'g-', linewidth=2, label='Out-Dist: Preferred')\n",
    "ax15.plot(sorted_rej, cdf_rej, 'r-', linewidth=2, label='Out-Dist: Rejected')\n",
    "\n",
    "# In-dist trained\n",
    "sorted_in_pref = np.sort(final_in_dist_pref_scores)\n",
    "sorted_in_rej = np.sort(final_in_dist_rej_scores)\n",
    "cdf_in_pref = np.arange(1, len(sorted_in_pref) + 1) / len(sorted_in_pref)\n",
    "cdf_in_rej = np.arange(1, len(sorted_in_rej) + 1) / len(sorted_in_rej)\n",
    "ax15.plot(sorted_in_pref, cdf_in_pref, 'g--', linewidth=1.5, alpha=0.7, label='In-Dist: Preferred')\n",
    "ax15.plot(sorted_in_rej, cdf_in_rej, 'r--', linewidth=1.5, alpha=0.7, label='In-Dist: Rejected')\n",
    "\n",
    "ax15.set_xlabel('Reward Score')\n",
    "ax15.set_ylabel('Cumulative Probability')\n",
    "ax15.set_title('CDF: In-Dist (dashed) vs Out-Dist (solid)')\n",
    "ax15.legend(fontsize=8)\n",
    "ax15.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "plot_path = f\"outputs/training_results_{timestamp}.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nComprehensive plots saved to: {plot_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS TO JSON\n",
    "# =============================================================================\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "results = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'architecture': {\n",
    "        'type': 'LoRA + Reward Head',\n",
    "        'lora_config': {\n",
    "            'r': LORA_R,\n",
    "            'alpha': LORA_ALPHA,\n",
    "            'dropout': LORA_DROPOUT,\n",
    "            'target_modules': LORA_TARGET_MODULES,\n",
    "        },\n",
    "        'trainable_params': trainable_params,\n",
    "        'total_params': total_params,\n",
    "        'trainable_percent': 100 * trainable_params / total_params,\n",
    "    },\n",
    "    'data': {\n",
    "        'training_file': active_train_data_file,\n",
    "        'out_dist_validation_file': COT_VAL_DATA_FILE if USE_COT else VAL_DATA_FILE,\n",
    "        'in_dist_val_split': IN_DIST_VAL_SPLIT,\n",
    "        'use_cot': USE_COT,\n",
    "        'training_label_type': 'CARA CoT' if USE_COT else 'CARA (risk-aversion)',\n",
    "        'in_dist_validation_label_type': 'CARA CoT' if USE_COT else 'CARA (risk-aversion)',\n",
    "        'out_dist_validation_label_type': 'cooperate CoT' if USE_COT else 'cooperate',\n",
    "    },\n",
    "    'baseline': {\n",
    "        'in_dist': {\n",
    "            'accuracy': float(baseline_in_dist_results['accuracy']),\n",
    "            'loss': float(baseline_in_dist_results['loss']),\n",
    "            'mean_margin': float(baseline_in_dist_results['mean_margin']),\n",
    "            'error_type_breakdown': {\n",
    "                et: {k: float(v) if isinstance(v, (int, float)) else v \n",
    "                     for k, v in stats.items()}\n",
    "                for et, stats in baseline_in_dist_results.get('error_type_breakdown', {}).items()\n",
    "            },\n",
    "        },\n",
    "        'out_dist': {\n",
    "            'accuracy': float(baseline_out_dist_results['accuracy']),\n",
    "            'loss': float(baseline_out_dist_results['loss']),\n",
    "            'mean_margin': float(baseline_out_dist_results['mean_margin']),\n",
    "            'error_type_breakdown': {\n",
    "                et: {k: float(v) if isinstance(v, (int, float)) else v \n",
    "                     for k, v in stats.items()}\n",
    "                for et, stats in baseline_out_dist_results.get('error_type_breakdown', {}).items()\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    'trained': {\n",
    "        'in_dist': {\n",
    "            'final_accuracy': float(final_in_dist_accuracy),\n",
    "            'final_loss': float(final_in_dist_loss),\n",
    "            'mean_margin': float(np.mean(final_in_dist_margins)),\n",
    "            'error_type_breakdown': {\n",
    "                et: {k: float(v) if isinstance(v, (int, float)) else v \n",
    "                     for k, v in stats.items()}\n",
    "                for et, stats in final_in_dist_error_breakdown.items()\n",
    "            } if final_in_dist_error_breakdown else {},\n",
    "        },\n",
    "        'out_dist': {\n",
    "            'final_accuracy': float(final_out_dist_accuracy),\n",
    "            'final_loss': float(final_out_dist_loss),\n",
    "            'best_accuracy': float(best_val_accuracy),\n",
    "            'mean_margin': float(np.mean(final_out_dist_margins)),\n",
    "            'error_type_breakdown': {\n",
    "                et: {k: float(v) if isinstance(v, (int, float)) else v \n",
    "                     for k, v in stats.items()}\n",
    "                for et, stats in final_out_dist_error_breakdown.items()\n",
    "            } if final_out_dist_error_breakdown else {},\n",
    "        },\n",
    "    },\n",
    "    'improvement': {\n",
    "        'in_dist': {\n",
    "            'accuracy_gain': float(final_in_dist_accuracy - baseline_in_dist_results['accuracy']),\n",
    "            'accuracy_gain_percent': float((final_in_dist_accuracy - baseline_in_dist_results['accuracy']) * 100),\n",
    "        },\n",
    "        'out_dist': {\n",
    "            'accuracy_gain': float(final_out_dist_accuracy - baseline_out_dist_results['accuracy']),\n",
    "            'accuracy_gain_percent': float((final_out_dist_accuracy - baseline_out_dist_results['accuracy']) * 100),\n",
    "        },\n",
    "    },\n",
    "    'config': {\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'epochs_trained': len(history['epochs']),\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'training_samples': len(train_dataset),\n",
    "        'in_dist_validation_samples': len(in_dist_val_dataset),\n",
    "        'out_dist_validation_samples': len(out_dist_val_dataset),\n",
    "    },\n",
    "    'timestamp': timestamp\n",
    "}\n",
    "\n",
    "results_path = f\"outputs/results_{timestamp}.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "# Save training history\n",
    "history_path = f\"outputs/training_history_{timestamp}.json\"\n",
    "# Convert numpy arrays to lists for JSON serialization\n",
    "history_serializable = {}\n",
    "for key, value in history.items():\n",
    "    if isinstance(value, dict):\n",
    "        history_serializable[key] = {k: v if not isinstance(v, np.ndarray) else v.tolist() \n",
    "                                     for k, v in value.items()}\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        history_serializable[key] = value.tolist()\n",
    "    else:\n",
    "        history_serializable[key] = value\n",
    "\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history_serializable, f, indent=2)\n",
    "print(f\"Training history saved to: {history_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Experiment Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset Summary:\")\n",
    "train_fmt = \"CARA CoT\" if USE_COT else \"CARA labels\"\n",
    "out_dist_fmt = \"Cooperate CoT\" if USE_COT else \"Cooperate labels\"\n",
    "print(f\"  Training: {len(train_dataset)} examples ({train_fmt})\")\n",
    "print(f\"  In-Dist Validation: {len(in_dist_val_dataset)} examples ({train_fmt})\")\n",
    "print(f\"  Out-of-Dist Validation: {len(out_dist_val_dataset)} examples ({out_dist_fmt})\")\n",
    "\n",
    "print(f\"\\nAccuracy Results:\")\n",
    "print(f\"  In-Distribution:\")\n",
    "print(f\"    Baseline: {baseline_in_dist_results['accuracy']*100:.1f}%\")\n",
    "print(f\"    Trained:  {final_in_dist_accuracy*100:.1f}%\")\n",
    "print(f\"    Improvement: {(final_in_dist_accuracy - baseline_in_dist_results['accuracy'])*100:+.1f}%\")\n",
    "print(f\"  Out-of-Distribution:\")\n",
    "print(f\"    Baseline: {baseline_out_dist_results['accuracy']*100:.1f}%\")\n",
    "print(f\"    Trained:  {final_out_dist_accuracy*100:.1f}%\")\n",
    "print(f\"    Improvement: {(final_out_dist_accuracy - baseline_out_dist_results['accuracy'])*100:+.1f}%\")\n",
    "\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.4f}%)\")\n",
    "\n",
    "# Find checkpoints\n",
    "import glob\n",
    "import shutil\n",
    "checkpoint_dirs = glob.glob(\"outputs/best_model_epoch*\")\n",
    "\n",
    "# Automatic downloads (for Google Colab)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Downloading Results...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(f\"Downloading: {plot_path}\")\n",
    "    files.download(plot_path)\n",
    "    \n",
    "    print(f\"Downloading: {results_path}\")\n",
    "    files.download(results_path)\n",
    "    \n",
    "    print(f\"Downloading: {history_path}\")\n",
    "    files.download(history_path)\n",
    "    \n",
    "    if checkpoint_dirs:\n",
    "        latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
    "        if os.path.isdir(latest_checkpoint):\n",
    "            zip_path = f\"{latest_checkpoint}.zip\"\n",
    "            shutil.make_archive(latest_checkpoint, 'zip', latest_checkpoint)\n",
    "            print(f\"Downloading: {zip_path}\")\n",
    "            files.download(zip_path)\n",
    "    \n",
    "    print(\"\\nAll files downloaded successfully!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nNOTE: Not running in Google Colab - files saved to outputs/ directory\")\n",
    "    print(\"Files available:\")\n",
    "    print(f\"  - {plot_path}\")\n",
    "    print(f\"  - {results_path}\")\n",
    "    print(f\"  - {history_path}\")\n",
    "    if checkpoint_dirs:\n",
    "        latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
    "        print(f\"  - {latest_checkpoint}/ (LoRA checkpoint directory)\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}